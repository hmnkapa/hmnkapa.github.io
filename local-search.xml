<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>神经网络 Neural Networks</title>
    <link href="/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络-neural-networks">4. 神经网络 Neural Networks</h1><h2 id="神经网络-neural-network">4.1 神经网络 Neural Network</h2><p>由于两个线性分类器的直接叠加等效于一个线性分类器（忽略bias）： <spanclass="math display">\[f=W_2W_1x=W^*x\]</span>我们在两个线性分类器中间添加一个非线性函数，就构成了一个两层的<strong>神经网络（NeuralNetwork）</strong>： <span class="math display">\[f=W_2\max(0, W_1x)\]</span></p><h2 id="激活函数-activation-function">4.2 激活函数 Activationfunction</h2><p>神经网络中的非线性函数被称为<strong>激活函数（Activationfunction）</strong></p><p>以下是一些常见的激活函数</p><p>ReLU(Rectified Linear Unit): <span class="math display">\[\mathrm{ReLU}(z) = \max (0,z)\]</span> Leaky ReLU: <span class="math display">\[\max(0.1x,x)\]</span></p><p>Sigmoid: <span class="math display">\[\sigma(x) = \frac{1}{1+e^{-x}}\]</span> tanh: <span class="math display">\[\tanh(x)\]</span> Maxout: <span class="math display">\[max(w_1^{\mathsf T}x+b_1, w_2^{\mathsf T}x+b_2)\]</span> ELU: <span class="math display">\[\begin{cases}x&amp; x\geq0\\\alpha (e^x-1)&amp; x&lt;0\end{cases}\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>优化理论 Optimization</title>
    <link href="/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    <url>/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="优化理论-optimization">3. 优化理论 Optimization</h1><h2 id="优化理论-optimization-1">3.1 优化理论 Optimization</h2><p>一个线性分类器的loss可以表示为 <span class="math display">\[L(W) = \frac{1}{N}\sum_{i=1}^N L_i(x_i, y_i,W) + \lambda R(W)\]</span> <strong>优化理论（optimization）</strong>就是求使<spanclass="math inline">\(L\)</span>最小时<spanclass="math inline">\(W\)</span>的值，即求 <span class="math display">\[w^*=\arg \min_w L(w)\]</span></p><h2 id="梯度下降法-gradient-descent">3.2 梯度下降法 GradientDescent</h2><p>通过迭代的方式，沿函数梯度的负方向下降以寻找函数最小值的方法为<strong>梯度下降法（gradientdescent）</strong>： <span class="math display">\[x_{t+1} = x_t - \alpha \nabla f(x_t)\]</span> 代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Vanilla gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(loss_fn, data, w)<br>    w -= lenrning_rate * dw<br></code></pre></td></tr></table></figure><p>其中hyperparameters有<code>initailize_weights()</code>、<code>num_steps</code>和<code>learning_rate</code></p><p>在计算梯度时，通常使用<spanclass="math inline">\(\nabla_WL\)</span>的解析式计算，并用数值计算的方式检验</p><h2 id="随机梯度下降法-stochastic-gradient-descent-sgd">3.3随机梯度下降法 Stochastic Gradient Descent (SGD)</h2><p>loss的梯度计算表达式为 <span class="math display">\[\nabla_WL(W) = \frac{1}{N}\sum_{i=1}^N \nabla _W L_i(x_i, y_i,W) +\lambda \nabla_WR(W)\]</span> 当<spanclass="math inline">\(N\)</span>的数值很大时，计算梯度的开销会很大</p><p>为了避免巨大的开销，我们可以从概率的角度考虑loss function</p><p>对于一个数据集： <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> 式中<span class="math inline">\(x_i\)</span>是图像，<spanclass="math inline">\(y_i\)</span>是该图像对应的正确标签，我们将<spanclass="math inline">\(L\)</span>视作关于<spanclass="math inline">\(x\)</span>、<spanclass="math inline">\(y\)</span>的joint probability distribution</p><p>那么loss就可以看做该分布的期望，即 <span class="math display">\[L(W) = \mathbb E(L) +\lambda R(W)\\\nabla _W L(W) = \nabla_W\mathbb E(L) + \lambda \nabla_WR(W)\]</span> 为了方便计算<span class="math inline">\(\mathbbE(L)\)</span>，可以采用蒙特卡洛方法进行采样估计： <spanclass="math display">\[L(W) \approx \frac1n \sum_{i=1}^n L_i(x_i, y_i, W) + \lambda R(W)\\\nabla_WL(W) \approx \frac1n \sum _{i=1}^n \nabla_WL_i(x_i, y_i, W) +\lambda \nabla _WR(W)\]</span>所以我们会从整个数据集中采样出minibatch来估计梯度，称为<strong>随机梯度下降法（stochasticgradient descent）</strong>，minibatch的大小通常为32、64或128</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    minibatch = sample_data(data, batch_size)<br>    dw = compute_gradient(loss_fn, minibatch, w)<br>    w -= learning_rate * dw<br></code></pre></td></tr></table></figure><p>其中hyperparameters有<code>initialize_weights()</code>，<code>num_steps</code>、<code>learning_rate</code>、<code>batch_size</code>和<code>sample_data()</code></p><h2 id="sgd-momentum-sgdm">3.4 SGD + Momentum (SGDM)</h2><p>当loss function有局部最小值（local minimum）或鞍点（saddlepoint）时，SGD方法会立即停止，我们引入”速度向量“来模拟一个小球沿斜坡下滚的过程：<span class="math display">\[v_{t+1} = \rho v_t + \nabla f(x_t)\\x_{t+1} = x_t - \alpha v_{t+1}\]</span> 速度向量的实质就是梯度的累计，函数将沿累计的梯度方向下降</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent with momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    v = rho * v + dw<br>    w -= learning_rate * v<br></code></pre></td></tr></table></figure><p>同时由于速度向量的引入，我们引入了一个新的hyperparameter<code>rho</code>模拟摩擦系数以保证小球最终会停止，通常<code>rho</code>的值为0.9或0.99</p><h2 id="nesterov-momentum">3.5 Nesterov Momentum</h2><p>在累计梯度时，我们选择不累计当前的梯度，而是假设小球以当前的速度运动一小段距离后，累计运动后小球处的梯度，这种方法被称为<strong>Nesterovmomentum</strong>： <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t)\\x_{t+1} = x_t + v_{t+1}\]</span> 通常为了计算方便，我们令<span class="math inline">\(\tilde x =x_t + \rho v_t\)</span>，于是有： <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(\tilde x_t)\\\tilde x_{t+1} = \tilde x_t + v_{t+1} +\rho ( v_{t+1} - v_t)\]</span> 代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Nesterov momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    old_v = v<br>    v = rho * v - learning_rate * dw<br>    w -= rho * old_v - (<span class="hljs-number">1</span> + rho) * v<br></code></pre></td></tr></table></figure><h2 id="adagrad">3.6 AdaGrad</h2><p>在梯度下降法中，由于learningrate是固定的，梯度大小不同的方向下降的速度是相同的</p><p>但是我们希望在梯度较大的方向减缓下降速度以防震荡，梯度较小的方向加快下降速度，于是我们用<strong>AdaGrad</strong>使learningrate自适应大小： <span class="math display">\[S_{t+1} = S_t + \nabla_W^2 \\x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span> 式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># AdaGrad</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared += dw * dw<br>    w -= learning_rate * dw / (grad_sqruared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><h2 id="rmsprop">3.7 RMSProp</h2><p>在AdaGrad中，若迭代的次数过多，可能会出现梯度平方累积过大的情况，导致learningrate过小而不能产生有效迭代</p><p><strong>RMSProp</strong>效仿SGD+Momentum引入摩擦系数<spanclass="math inline">\(\rho\)</span>保证小球最终停止的想法，引入“摩擦“以防止梯度平方的累积过大：<span class="math display">\[S_{t+1} = \rho S_t + (1-\rho)\nabla_W^2 \\x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span> 式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># RMSProp</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared = decay_rate * grad_squared + (<span class="hljs-number">1</span> - decay_rate) * dw * dw<br>    w -= learning_rate * dw / (grad_squared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>decay_rate</code>为新的hyperparameter</p><h2 id="adam">3.8 Adam</h2><p>将RMSProp和momentum的思想结合起来，我们就得到了<strong>Adam</strong>：<span class="math display">\[v_{t+1} = \rho_1 v_t + (1 - \rho_1)\nabla _W\\S_{t+1} = \rho_2 S_t + (1-\rho_2)\nabla_W^2\\x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span> 式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>和<code>beta2</code>为新的hyperparameters</p><p>但是在第一次迭代时，由于<spanclass="math inline">\(\rho_2\)</span>通常趋近于1，<spanclass="math inline">\(S_1\)</span>和<spanclass="math inline">\(S_2\)</span>通常会趋近于0，导致第一次迭代的跨度会非常大，容易产生不好的结果，所以我们在Adam中会加入<strong>biascorrection</strong>来修正： <span class="math display">\[v_{t+1} = \frac{\rho_1 v_t + (1 - \rho_1)\nabla_W}{\sqrt{1-\rho_1^{t+1}}}\\S_{t+1} = \frac{\rho_2 S_t +(1-\rho_2)\nabla_W^2}{\sqrt{1-\rho_2^{t+1}}}\\x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span> 修正后的代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam with bias correction</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    moment1_unbias = moment1 / (<span class="hljs-number">1</span> - beta1 ** t)<br>    moment2_unbias = moment2 / (<span class="hljs-number">1</span> - beta2 ** t)<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>通常取0.9，<code>beta2</code>通常取0.999，<code>learning_rate</code>通常取1e-3、5e-4、1e-4</p><h2 id="二阶优化-second-order-optimization">3.9 二阶优化 Second-OrderOptimization</h2><p>以上我们提到的优化方式均为<strong>一阶优化</strong>，它们通过梯度来线性拟合函数并迭代以得到函数最小值</p><p>那么，我们可以考虑通过梯度和黑塞矩阵来二次拟合函数，不妨设二次拟合的迭代式为<span class="math display">\[x_{t+1} = x_t+d\]</span> 我们希望<spanclass="math inline">\(x_{t+1}\)</span>尽可能小，即求 <spanclass="math display">\[d = \arg \min_d f(x_t+d)\]</span> <span class="math inline">\(f(x)\)</span>在<spanclass="math inline">\(x_t\)</span>处的二阶泰勒展开式为 <spanclass="math display">\[f(x) = f(x_t) + (x - x_t)^{\mathsf{T}}\nabla f(x_t) +\frac12(x-x_t)^{\mathsf{T}}\mathbf{H}f(x_t)(x-x_t)\]</span> 代入<span class="math inline">\(x_{t+1}\)</span>的值得 <spanclass="math display">\[f(x_{t+1}) = f(x_t+d)=f(x_t)+d^{\mathsf{T}}\nablaf(x_t)+\frac12d^{\mathsf{T}}\mathbf{H}f(x_t)d\]</span> 当<span class="math inline">\(d=-[\mathbf{H}f(x_t)]^{-1}\nablaf(x_t)\)</span>时取最小值</p><p>则二阶优化的迭代式为： <span class="math display">\[x_{t+1} = x_t-\left[\mathbf{H}f(x_t)\right]^{-1}\nabla f(x_t)\]</span>然而由于黑塞矩阵元素数量过多且矩阵求逆复杂度过高，实践中很少使用二阶优化</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>线性分类器 Linear Classifier</title>
    <link href="/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="线性分类器-linear-classifier">2. 线性分类器 LinearClassifier</h1><h2 id="评分函数-score-function">2.1 评分函数 Score Function</h2><p><strong>评分函数（score function）</strong> 是<spanclass="math inline">\(\mathbb{R}^D \to\mathbb{R}^C\)</span>的线性映射，即一张图片在每个标签上所得到的评分：<span class="math display">\[f(x, W) = Wx+b,x\in \mathbb{R}^D, W\in \mathbb{R}^{C\times D}, b\in \mathbb{R}^D\]</span></p><p>式中<spanclass="math inline">\(x\)</span>是将图像数据拉长（flatten）得到的<spanclass="math inline">\(D\)</span>维列向量，<spanclass="math inline">\(W\)</span>是<strong>参数（parameter）</strong>或称<strong>权重（weight）</strong> ，<spanclass="math inline">\(b\)</span>为<strong>偏差向量（biasvector）</strong></p><p><span class="math inline">\(C\)</span>为待分类的标签个数，<spanclass="math inline">\(f(x,W)\)</span>即为该图像对于每个标签的评分</p><h2 id="损失函数-loss-function">2.2 损失函数 Loss Function</h2><p><strong>损失函数（loss funtion）</strong>量化了线性分类器的效果，其值越高，则线性分类器效果越差，又被称为目标函数（objectivefunction）、代价函数（cost function）</p><p>对于一个数据集： <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> 式中<span class="math inline">\(x_i\)</span>是图像，<spanclass="math inline">\(y_i\)</span>是该图像对应的正确标签</p><p>则对于单个的一张图像的损失为： <span class="math display">\[L_i(f(x_i, W), y_i)\]</span> 对于数据集来说，损失是每张图像损失的平均值： <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)\]</span> <span class="math inline">\(L_i\)</span>即为损失函数</p><h2 id="多类支持向量机损失-multiclass-svm-loss">2.3 多类支持向量机损失Multiclass SVM Loss</h2><p>朴素的想法是，正确的标签的评分应当比其他标签的评分要高</p><p>所以，对于给定的一张图像<span class="math inline">\(x_i,y_i\)</span>，其评分<span class="math inline">\(s=f(x_i,W)\)</span>，则SVM损失有如下形式： <span class="math display">\[L_i = \sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)\]</span> 当评分均为很小的随机值时，损失应当接近<spanclass="math inline">\(C-1\)</span>，<spanclass="math inline">\(C\)</span>为待分类的总标签数，此性质可作为debug的依据</p><h2 id="正则化-regularization">2.4 正则化 Regularization</h2><p>使上述的损失<span class="math inline">\(L\)</span>最小的<spanclass="math inline">\(W\)</span>并不唯一</p><p>若<span class="math inline">\(W\)</span>可使损失<spanclass="math inline">\(L\)</span>最小，则<spanclass="math inline">\(\lambda W\)</span>也可使<spanclass="math inline">\(L\)</span>最小</p><p>于是，我们在损失函数的表达式中引入一项<spanclass="math inline">\(\lambda R(W)\)</span> : <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)+\lambda R(W)\]</span> 式中<spanclass="math inline">\(\lambda\)</span>为<strong>正则化强度（regularizationstrength）</strong> ，为超参数</p><p>正则化的好处：</p><ol type="1"><li><p>进一步地筛选<spanclass="math inline">\(W\)</span>，使所选定的<spanclass="math inline">\(W\)</span>拥有除最小化损失以外的其他功能</p></li><li><p>避免<strong>过拟合（overfitting）</strong></p></li><li><p>通过增加曲率以提高<strong>优化（optimization）</strong>效果</p></li></ol><p>常用的正则化有：</p><p>L2正则化 <span class="math display">\[R(W) = \sum_k \sum_l W_{k,l}^2\]</span> L1正则化 <span class="math display">\[R(W)= \sum_k \sum_l \left| W_{k,l}^2 \right |\]</span> 或者将二者联系起来： <span class="math display">\[R(W) = \sum_k \sum_l \beta W_{k,l}^2 + \left |W_{k,l}^2 \right |\]</span> 其他还有dropout、Batchnormalization、Cutout、Mixup、Stochastic depth等等</p><h2 id="交叉熵损失-cross-entropy-loss">2.5 交叉熵损失 Cross-EntropyLoss</h2><p>另一种损失函函数使用<strong>归一化指数函数（softmaxfunction）</strong> 将评分用概率来描述，被称为<strong>交叉熵损失（Cross- Entropy Loss）</strong>或者<strong>多元逻辑回归（Multinomial Logistic Regression）</strong></p><p>对于一个评分函数<spanclass="math inline">\(s=f(x_i,W)\)</span>，其softmax function的形式为：<span class="math display">\[P(Y = k|X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}\]</span> 我们假设真实的概率分布为<spanclass="math inline">\(P\)</span>，训练得到的概率分布为<spanclass="math inline">\(Q\)</span>，我们使用<spanclass="math inline">\(Q\)</span>来拟合<spanclass="math inline">\(P\)</span>，则交叉熵为： <spanclass="math display">\[H(P, Q) = H(P) + D_{KL}(P||Q)\]</span> 式中<spanclass="math inline">\(D_{KL}(P||Q)\)</span>为<strong>相对熵（Kullback-LeiblerDivergence）</strong> ： <span class="math display">\[D_{KL}(P||Q) = \sum_{i=1}^n P(x_i)\log \frac{P(x_i)}{Q(x_i)}\]</span> 由于真实的概率分布不变，即<spanclass="math inline">\(H(P)\)</span>不变，则若交叉熵<spanclass="math inline">\(H(P,Q)\)</span>最小，只需相对熵<spanclass="math inline">\(D_{KL}(P||Q)\)</span>最小即可</p><p>当单张图片的损失具有如下形式时，交叉熵最小： <spanclass="math display">\[L_i = - \log P(Y=y_i|X = x_i)\]</span> 所以交叉熵损失的具体形式为： <span class="math display">\[L = \frac1N \sum_i\left(-\log\left(\frac{e^{s_{y_i}}}{\sum_je^{s_j}}\right)\right)+\lambda R(W)\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>图像分类 Image Classification</title>
    <link href="/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    <url>/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="图像分类-image-classification">1. 图像分类 ImageClassification</h1><h2 id="图像分类器">1.1 图像分类器</h2><p>图像分类的算法难以用如下的函数进行硬编码(hard-code)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify_image</span>(<span class="hljs-params">image</span>):<br><span class="hljs-comment"># Some magic here?</span><br><span class="hljs-keyword">return</span> class_label<br></code></pre></td></tr></table></figure><p>所以通常采用机器学习，即Data-Driven的方式</p><p>先用包含图像与标签的数据集训练分类器，再评估分类器在分类新图片的表现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">images, labels</span>):<br><span class="hljs-comment"># Machine learning!</span><br><span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">model, test_images</span>):<br><span class="hljs-comment"># Use model to predict labels</span><br><span class="hljs-keyword">return</span> test_labels<br></code></pre></td></tr></table></figure><h2 id="常见数据集">1.2 常见数据集</h2><p>MNIST</p><p>CIFAR10</p><p>CIFAR100</p><p>ImageNet</p><p>MIT Places</p><h2 id="邻近算法-nearest-neighbor">1.3 邻近算法 Nearest Neighbor</h2><ol type="1"><li>记忆数据集中所有的图像和对应的标签</li><li>与数据集中最相似的图像的标签即为新图像的标签</li></ol><p>使用<strong>距离度量</strong>比较图像，以下是常见的两种距离度量：</p><p>L1 距离（曼哈顿距离 Manhattan distance）: <spanclass="math display">\[d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|\]</span> L2 距离（欧拉距离 Euclidean distance）： <spanclass="math display">\[d_2(I_1, I_2) = \sqrt{\sum_p (I_1^p - I_2^p)^2}\]</span></p><h2 id="k近邻算法-k-nearest-neighbors">1.4 K近邻算法 K-NearestNeighbors</h2><p>若 𝑘 个最相似的图像中的大多数为某一个标签，则该图像也属于这个标签</p><p>当训练的样本足够多时，K近邻算法可以表示任何函数</p><h2 id="超参数-hyperparameter">1.5 超参数 Hyperparameter</h2><p>从数据集中无法通过训练得到的参数为<strong>超参数</strong>，超参数需要在学习之前设定</p><p>例如K邻近算法中的K就是超参数</p><p>超参数的设定方法：</p><p>将数据集分为train、validate和test三部分，选择超参数的值在train上训练，并在validate中验证，只在最后使用test上查看效果</p><p>如果条件允许，可以将数据集分割成许多部分，每次选择不同的部分作为validate，剩下的部分作为train，同样的只在最后在test上检验</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习与计算机视觉</title>
    <link href="/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    <url>/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h2 id="写在前面">写在前面</h2><p>这是一篇Umich的EECS498-007课程的学习笔记，开始阅读之前您需要学习过这门课程或已经掌握了深度学习与计算机视觉的基本知识。本篇旨在复习课程要点，不适合初学者作为tutorial学习。</p><h2 id="关于umich-eecs-498-007">关于Umich EECS 498-007</h2><p>来自<ahref="https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/EECS498-007/">csdiy</a>的课程简介</p><blockquote><p>UMich 的 Computer Vision课，课程视频和作业质量极高，涵盖的主题非常全，同时 Assignments的难度由浅及深，覆盖了 CV 主流模型发展的全阶段，是一门非常好的 ComputerVision 入门课。</p></blockquote><p>本篇笔记基于Fall2019的课程，课程链接：https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r</p><p>与此同时，该课程的assignments（含答案）已上传至<ahref="https://github.com/hmnkapa/eecs598">github</a>，答案是我自己做的，仅供参考</p><p>Assignments来自Fall 2020的<ahref="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">课程主页</a></p><h2 id="目录">目录</h2><ol type="1"><li>图像分类 Image Classification</li><li>线性分类器 Linear Classifier</li><li>优化理论 Optimization</li><li>神经网络 Neural Networks</li><li>反向传播算法 Backpropagation</li><li>卷积网络 Convolutional Networks</li><li>CNN架构 CNN Architectures</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>柯西-施瓦茨（Cauchy-Schwarz）不等式的积分形式</title>
    <link href="/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/"/>
    <url>/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="柯西-施瓦茨不等式的积分形式">柯西-施瓦茨不等式的积分形式</h2><p>假设 <span class="math inline">\(𝑓(𝑥)\)</span> 和 <spanclass="math inline">\(𝑔(𝑥)\)</span> 在区间 <spanclass="math inline">\([𝑎,𝑏]\)</span> 上黎曼可积，那么</p><p><span class="math display">\[\int_𝑎^𝑏𝑓^2(𝑥)\mathrm{d}𝑥⋅\int_𝑎^𝑏𝑔^2(𝑥)\mathrm{d}𝑥≥(\int_𝑎^𝑏𝑓(𝑥)𝑔(𝑥)\mathrm{d}𝑥)^2\]</span></p><h2 id="前置知识">前置知识</h2><p>柯西不等式 <span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 及其一般形式 <span class="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 极限、微积分基本知识</p><h2id="柯西-施瓦茨不等式的积分形式的证明">柯西-施瓦茨不等式的积分形式的证明</h2><p>事实上，说起柯西，在高中课本（习题）上我们已经学习过柯西不等式的二维形式</p><p><span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 进一步地，柯西不等式的一般形式如下 <spanclass="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 既然都叫柯西（</p><p>我们考虑使用柯西不等式的一般形式来证明柯西-施瓦茨不等式的积分形式</p><p>由定积分的定义，柯西-施瓦茨不等式的积分形式等价于 <spanclass="math display">\[\lim_{n\to\infty}\sum_{i=1}^nf^2(x_i)\Delta{x} \cdot\lim_{n\to\infty}\sum_{i=1}^ng^2(x_i)\Delta{x} \geq \left(\lim_{n\to\infty}\sum_{i=1}^nf(x_i)g(x_i)\Delta{x} \right)^2\\\]</span> 此处<spanclass="math inline">\(\Delta{x}=\frac{b-a}{n}\)</span>，<spanclass="math inline">\(x_i=a+i\Delta{x}\)</span></p><p>由极限的运算法则，上式等价于 <span class="math display">\[\lim_{n\to\infty} \left[ \sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i)- \left( \sum_{i=1}^nf(x_i)g(x_i) \right)^2 \right] \Delta^2{x} \geq0\\\]</span> 由极限的保号性并约去<spanclass="math inline">\(\Delta^2{x}\)</span>，有 <spanclass="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) - \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \geq0\\\]</span> 即 <span class="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) \geq \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \\\]</span> 此即柯西不等式的一般形式.</p><p>柯西不等式的一般形式的取等条件为<spanclass="math inline">\(\frac{a_1}{b_1}=\frac{a_2}{b_2}=...=\frac{a_n}{b_n}\)</span>，因此对于柯西-施瓦茨不等式的一般形式，</p><p>当且仅当 <span class="math display">\[f(x)=\lambda g(x)\]</span> 时，等号成立，式中<spanclass="math inline">\(\lambda\)</span>为一实数.</p><h2 id="一道例题">一道例题</h2><p>这个不等式是最近在准备微积分的第一次期中考试时遇到的</p><p>（其实我开始写文章的时候距离考试还有24分钟</p><p>想找一些卷子做一做</p><p>于是发现了一份浙江某211大学号称<ahref="https://www.zhihu.com/question/265940112?utm_medium=social&amp;utm_oi=1650268599276343296&amp;utm_psn=1703111356650299392&amp;utm_source=qq">120年来最难的微积分试卷</a></p><p>其最后一题原题如下：</p><blockquote><ol start="12" type="1"><li>设 <span class="math inline">\(𝑓(𝑥)\)</span> 在 <spanclass="math inline">\([0,1]\)</span> 上连续且可导，当 <spanclass="math inline">\(𝑥∈[0,1]\)</span> 时， <spanclass="math inline">\(\displaystyle\int_𝑥^1𝑓(𝑡)\mathrm{d}𝑡≥\frac{1-x^3}{2}\)</span> ，证明： <spanclass="math display">\[\displaystyle \int_0^1\,[f(x)]^2\mathrm{d}x&gt;\frac{5}{12}\\\]</span></li></ol></blockquote><p>令<span class="math inline">\(\displaystyleF(x)=\int_x^1\,f(t)\mathrm{d}t=-\int_1^x\,f(t)\mathrm{d}t\)</span> ，则<span class="math inline">\(\displaystyle F^\prime(x)=-f(x)\)</span></p><p>由 <span class="math inline">\(\displaystyleF(x)\geq\frac{1-x^3}{2}\)</span> 可得 <span class="math display">\[\int_0^1\,F(x)\mathrm{d}x \geq\int_0^1\frac{1-x^3}{2}\mathrm{d}x=\frac{3}{8}\\\]</span> 下面运用分部积分法来计算 <spanclass="math inline">\(\displaystyle \int_0^1\,F(x)\mathrm{d}x\)</span><span class="math display">\[\begin{align} \int_0^1\,F(x)\mathrm{d}x&amp;=xF(x)\Big]_0^1-\int_0^1x\mathrm{d}F(x)\\&amp;=F(1)+\int_0^1xf(x)\mathrm{d}x\\ &amp;=\int_0^1xf(x)\mathrm{d}x\\\end{align} \\\]</span></p><p>于是，我们有 <span class="math display">\[\int_0^1xf(x)\mathrm{d}x \geq \frac{3}{8} \\\]</span> 由柯西-施瓦茨不等式 <span class="math display">\[\int_0^1\,f^2(x)\mathrm{d}x\cdot\int_0^1\,x^2\mathrm{d}x \geq \left(\int_0^1\,xf(x)\mathrm{d}x \right)^2 \geq \frac{9}{64} \\\]</span> <span class="math inline">\(\displaystyle\int_0^1\,x^2\mathrm{d}x\)</span>是好积的，其结果为 <spanclass="math inline">\(\frac{1}{3}\)</span> ，故 <spanclass="math display">\[\int_0^1\,f^2(x)\mathrm{d}x \geq \frac{27}{64}&gt;\frac{5}{12}\\\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
