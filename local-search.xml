<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>深度学习与计算机视觉</title>
    <link href="/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    <url>/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h2 id="写在前面">写在前面</h2><p>这是一篇Umich的EECS498-007课程的学习笔记，开始阅读之前您需要学习过这门课程或已经掌握了深度学习与计算机视觉的基本知识。本篇旨在复习课程要点，不适合初学者作为tutorial学习。</p><h2 id="关于umich-eecs-498-007">关于Umich EECS 498-007</h2><p>来自<ahref="https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/EECS498-007/">csdiy</a>的课程简介</p><blockquote><p>UMich 的 Computer Vision课，课程视频和作业质量极高，涵盖的主题非常全，同时 Assignments的难度由浅及深，覆盖了 CV 主流模型发展的全阶段，是一门非常好的 ComputerVision 入门课。</p></blockquote><p>本篇笔记基于Fall2019的课程，课程链接：https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r</p><p>与此同时，该课程的assignments（含答案）已上传至<ahref="https://github.com/hmnkapa/eecs598">github</a>，答案是我自己做的，仅供参考</p><p>Assignments来自Fall 2020的<ahref="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">课程主页</a></p><h2 id="目录">目录</h2><ol type="1"><li>图像分类 Image Classification</li><li>线性分类器 Linear Classifier</li><li>优化理论 Optimization</li><li>神经网络 Neural Networks</li><li>反向传播算法 Backpropagation</li><li>卷积网络 Convolutional Networks</li><li>CNN架构 CNN Architectures</li></ol><h2 id="图像分类-image-classification">1. 图像分类 ImageClassification</h2><h3 id="图像分类器">1.1 图像分类器</h3><p>图像分类的算法难以用如下的函数进行硬编码(hard-code)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify_image</span>(<span class="hljs-params">image</span>):<br><span class="hljs-comment"># Some magic here?</span><br><span class="hljs-keyword">return</span> class_label<br></code></pre></td></tr></table></figure><p>所以通常采用机器学习，一种Data-Driven的方式先用包含图像与标签的数据集训练分类器，再评估分类器在分类新图片的表现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">images, labels</span>):<br><span class="hljs-comment"># Machine learning!</span><br><span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">model, test_images</span>):<br><span class="hljs-comment"># Use model to predict labels</span><br><span class="hljs-keyword">return</span> test_labels<br></code></pre></td></tr></table></figure><h3 id="常见数据集">1.2 常见数据集</h3><p>MNIST CIFAR10 CIFAR100 ImageNet MIT Places</p><h3 id="邻近算法-nearest-neighbor">1.3 邻近算法 Nearest Neighbor</h3><ol type="1"><li>记忆数据集中所有的图像和对应的标签</li><li>与数据集中最相似的图像的标签即为新图像的标签使用<strong>距离度量</strong>比较图像，以下是常见的两种距离度量： L1距离（曼哈顿距离 Manhattan distance）：</li></ol><p><span class="math display">\[d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|\]</span></p><p>L2 距离（欧拉距离 Euclidean distance）： <spanclass="math display">\[d_2(I_1, I_2) = \sqrt{\sum_p (I_1^p - I_2^p)^2}\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>柯西-施瓦茨（Cauchy-Schwarz）不等式的积分形式</title>
    <link href="/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/"/>
    <url>/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="柯西-施瓦茨不等式的积分形式">柯西-施瓦茨不等式的积分形式</h2><p>假设 <span class="math inline">\(𝑓(𝑥)\)</span> 和 <spanclass="math inline">\(𝑔(𝑥)\)</span> 在区间 <spanclass="math inline">\([𝑎,𝑏]\)</span> 上黎曼可积，那么</p><p><span class="math display">\[\int_𝑎^𝑏𝑓^2(𝑥)\mathrm{d}𝑥⋅\int_𝑎^𝑏𝑔^2(𝑥)\mathrm{d}𝑥≥(\int_𝑎^𝑏𝑓(𝑥)𝑔(𝑥)\mathrm{d}𝑥)^2\]</span></p><h2 id="前置知识">前置知识</h2><p>柯西不等式 <span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 及其一般形式 <span class="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 极限、微积分基本知识</p><h2id="柯西-施瓦茨不等式的积分形式的证明">柯西-施瓦茨不等式的积分形式的证明</h2><p>事实上，说起柯西，在高中课本（习题）上我们已经学习过柯西不等式的二维形式</p><p><span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 进一步地，柯西不等式的一般形式如下 <spanclass="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 既然都叫柯西（</p><p>我们考虑使用柯西不等式的一般形式来证明柯西-施瓦茨不等式的积分形式</p><p>由定积分的定义，柯西-施瓦茨不等式的积分形式等价于 <spanclass="math display">\[\lim_{n\to\infty}\sum_{i=1}^nf^2(x_i)\Delta{x} \cdot\lim_{n\to\infty}\sum_{i=1}^ng^2(x_i)\Delta{x} \geq \left(\lim_{n\to\infty}\sum_{i=1}^nf(x_i)g(x_i)\Delta{x} \right)^2\\\]</span> 此处<spanclass="math inline">\(\Delta{x}=\frac{b-a}{n}\)</span>，<spanclass="math inline">\(x_i=a+i\Delta{x}\)</span></p><p>由极限的运算法则，上式等价于 <span class="math display">\[\lim_{n\to\infty} \left[ \sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i)- \left( \sum_{i=1}^nf(x_i)g(x_i) \right)^2 \right] \Delta^2{x} \geq0\\\]</span> 由极限的保号性并约去<spanclass="math inline">\(\Delta^2{x}\)</span>，有 <spanclass="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) - \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \geq0\\\]</span> 即 <span class="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) \geq \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \\\]</span> 此即柯西不等式的一般形式.</p><p>柯西不等式的一般形式的取等条件为<spanclass="math inline">\(\frac{a_1}{b_1}=\frac{a_2}{b_2}=...=\frac{a_n}{b_n}\)</span>，因此对于柯西-施瓦茨不等式的一般形式，</p><p>当且仅当 <span class="math display">\[f(x)=\lambda g(x)\]</span> 时，等号成立，式中<spanclass="math inline">\(\lambda\)</span>为一实数.</p><h2 id="一道例题">一道例题</h2><p>这个不等式是最近在准备微积分的第一次期中考试时遇到的</p><p>（其实我开始写文章的时候距离考试还有24分钟</p><p>想找一些卷子做一做</p><p>于是发现了一份浙江某211大学号称<ahref="https://www.zhihu.com/question/265940112?utm_medium=social&amp;utm_oi=1650268599276343296&amp;utm_psn=1703111356650299392&amp;utm_source=qq">120年来最难的微积分试卷</a></p><p>其最后一题原题如下：</p><blockquote><ol start="12" type="1"><li>设 <span class="math inline">\(𝑓(𝑥)\)</span> 在 <spanclass="math inline">\([0,1]\)</span> 上连续且可导，当 <spanclass="math inline">\(𝑥∈[0,1]\)</span> 时， <spanclass="math inline">\(\displaystyle\int_𝑥^1𝑓(𝑡)\mathrm{d}𝑡≥\frac{1-x^3}{2}\)</span> ，证明： <spanclass="math display">\[\displaystyle \int_0^1\,[f(x)]^2\mathrm{d}x&gt;\frac{5}{12}\\\]</span></li></ol></blockquote><p>令<span class="math inline">\(\displaystyleF(x)=\int_x^1\,f(t)\mathrm{d}t=-\int_1^x\,f(t)\mathrm{d}t\)</span> ，则<span class="math inline">\(\displaystyle F^\prime(x)=-f(x)\)</span></p><p>由 <span class="math inline">\(\displaystyleF(x)\geq\frac{1-x^3}{2}\)</span> 可得 <span class="math display">\[\int_0^1\,F(x)\mathrm{d}x \geq\int_0^1\frac{1-x^3}{2}\mathrm{d}x=\frac{3}{8}\\\]</span> 下面运用分部积分法来计算 <spanclass="math inline">\(\displaystyle \int_0^1\,F(x)\mathrm{d}x\)</span><span class="math display">\[\begin{align} \int_0^1\,F(x)\mathrm{d}x&amp;=xF(x)\Big]_0^1-\int_0^1x\mathrm{d}F(x)\\&amp;=F(1)+\int_0^1xf(x)\mathrm{d}x\\ &amp;=\int_0^1xf(x)\mathrm{d}x\\\end{align} \\\]</span></p><p>于是，我们有 <span class="math display">\[\int_0^1xf(x)\mathrm{d}x \geq \frac{3}{8} \\\]</span> 由柯西-施瓦茨不等式 <span class="math display">\[\int_0^1\,f^2(x)\mathrm{d}x\cdot\int_0^1\,x^2\mathrm{d}x \geq \left(\int_0^1\,xf(x)\mathrm{d}x \right)^2 \geq \frac{9}{64} \\\]</span> <span class="math inline">\(\displaystyle\int_0^1\,x^2\mathrm{d}x\)</span>是好积的，其结果为 <spanclass="math inline">\(\frac{1}{3}\)</span> ，故 <spanclass="math display">\[\int_0^1\,f^2(x)\mathrm{d}x \geq \frac{27}{64}&gt;\frac{5}{12}\\\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
