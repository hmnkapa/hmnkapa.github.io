<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ç¥ç»ç½‘ç»œ Neural Networks</title>
    <link href="/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="ç¥ç»ç½‘ç»œ-neural-networks">4. ç¥ç»ç½‘ç»œ Neural Networks</h1><h2 id="ç¥ç»ç½‘ç»œ-neural-network">4.1 ç¥ç»ç½‘ç»œ Neural Network</h2><p>ç”±äºä¸¤ä¸ªçº¿æ€§åˆ†ç±»å™¨çš„ç›´æ¥å åŠ ç­‰æ•ˆäºä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ï¼ˆå¿½ç•¥biasï¼‰ï¼š <spanclass="math display">\[f=W_2W_1x=W^*x\]</span>æˆ‘ä»¬åœ¨ä¸¤ä¸ªçº¿æ€§åˆ†ç±»å™¨ä¸­é—´æ·»åŠ ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œå°±æ„æˆäº†ä¸€ä¸ªä¸¤å±‚çš„<strong>ç¥ç»ç½‘ç»œï¼ˆNeuralNetworkï¼‰</strong>ï¼š <span class="math display">\[f=W_2\max(0, W_1x)\]</span></p><h2 id="æ¿€æ´»å‡½æ•°-activation-function">4.2 æ¿€æ´»å‡½æ•° Activationfunction</h2><p>ç¥ç»ç½‘ç»œä¸­çš„éçº¿æ€§å‡½æ•°è¢«ç§°ä¸º<strong>æ¿€æ´»å‡½æ•°ï¼ˆActivationfunctionï¼‰</strong></p><p>ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„æ¿€æ´»å‡½æ•°</p><p>ReLU(Rectified Linear Unit): <span class="math display">\[\mathrm{ReLU}(z) = \max (0,z)\]</span> Leaky ReLU: <span class="math display">\[\max(0.1x,x)\]</span></p><p>Sigmoid: <span class="math display">\[\sigma(x) = \frac{1}{1+e^{-x}}\]</span> tanh: <span class="math display">\[\tanh(x)\]</span> Maxout: <span class="math display">\[max(w_1^{\mathsf T}x+b_1, w_2^{\mathsf T}x+b_2)\]</span> ELU: <span class="math display">\[\begin{cases}x&amp; x\geq0\\\alpha (e^x-1)&amp; x&lt;0\end{cases}\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ä¼˜åŒ–ç†è®º Optimization</title>
    <link href="/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    <url>/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="ä¼˜åŒ–ç†è®º-optimization">3. ä¼˜åŒ–ç†è®º Optimization</h1><h2 id="ä¼˜åŒ–ç†è®º-optimization-1">3.1 ä¼˜åŒ–ç†è®º Optimization</h2><p>ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨çš„losså¯ä»¥è¡¨ç¤ºä¸º <span class="math display">\[L(W) = \frac{1}{N}\sum_{i=1}^N L_i(x_i, y_i,W) + \lambda R(W)\]</span> <strong>ä¼˜åŒ–ç†è®ºï¼ˆoptimizationï¼‰</strong>å°±æ˜¯æ±‚ä½¿<spanclass="math inline">\(L\)</span>æœ€å°æ—¶<spanclass="math inline">\(W\)</span>çš„å€¼ï¼Œå³æ±‚ <span class="math display">\[w^*=\arg \min_w L(w)\]</span></p><h2 id="æ¢¯åº¦ä¸‹é™æ³•-gradient-descent">3.2 æ¢¯åº¦ä¸‹é™æ³• GradientDescent</h2><p>é€šè¿‡è¿­ä»£çš„æ–¹å¼ï¼Œæ²¿å‡½æ•°æ¢¯åº¦çš„è´Ÿæ–¹å‘ä¸‹é™ä»¥å¯»æ‰¾å‡½æ•°æœ€å°å€¼çš„æ–¹æ³•ä¸º<strong>æ¢¯åº¦ä¸‹é™æ³•ï¼ˆgradientdescentï¼‰</strong>ï¼š <span class="math display">\[x_{t+1} = x_t - \alpha \nabla f(x_t)\]</span> ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Vanilla gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(loss_fn, data, w)<br>    w -= lenrning_rate * dw<br></code></pre></td></tr></table></figure><p>å…¶ä¸­hyperparametersæœ‰<code>initailize_weights()</code>ã€<code>num_steps</code>å’Œ<code>learning_rate</code></p><p>åœ¨è®¡ç®—æ¢¯åº¦æ—¶ï¼Œé€šå¸¸ä½¿ç”¨<spanclass="math inline">\(\nabla_WL\)</span>çš„è§£æå¼è®¡ç®—ï¼Œå¹¶ç”¨æ•°å€¼è®¡ç®—çš„æ–¹å¼æ£€éªŒ</p><h2 id="éšæœºæ¢¯åº¦ä¸‹é™æ³•-stochastic-gradient-descent-sgd">3.3éšæœºæ¢¯åº¦ä¸‹é™æ³• Stochastic Gradient Descent (SGD)</h2><p>lossçš„æ¢¯åº¦è®¡ç®—è¡¨è¾¾å¼ä¸º <span class="math display">\[\nabla_WL(W) = \frac{1}{N}\sum_{i=1}^N \nabla _W L_i(x_i, y_i,W) +\lambda \nabla_WR(W)\]</span> å½“<spanclass="math inline">\(N\)</span>çš„æ•°å€¼å¾ˆå¤§æ—¶ï¼Œè®¡ç®—æ¢¯åº¦çš„å¼€é”€ä¼šå¾ˆå¤§</p><p>ä¸ºäº†é¿å…å·¨å¤§çš„å¼€é”€ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ¦‚ç‡çš„è§’åº¦è€ƒè™‘loss function</p><p>å¯¹äºä¸€ä¸ªæ•°æ®é›†ï¼š <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> å¼ä¸­<span class="math inline">\(x_i\)</span>æ˜¯å›¾åƒï¼Œ<spanclass="math inline">\(y_i\)</span>æ˜¯è¯¥å›¾åƒå¯¹åº”çš„æ­£ç¡®æ ‡ç­¾ï¼Œæˆ‘ä»¬å°†<spanclass="math inline">\(L\)</span>è§†ä½œå…³äº<spanclass="math inline">\(x\)</span>ã€<spanclass="math inline">\(y\)</span>çš„joint probability distribution</p><p>é‚£ä¹ˆlosså°±å¯ä»¥çœ‹åšè¯¥åˆ†å¸ƒçš„æœŸæœ›ï¼Œå³ <span class="math display">\[L(W) = \mathbb E(L) +\lambda R(W)\\\nabla _W L(W) = \nabla_W\mathbb E(L) + \lambda \nabla_WR(W)\]</span> ä¸ºäº†æ–¹ä¾¿è®¡ç®—<span class="math inline">\(\mathbbE(L)\)</span>ï¼Œå¯ä»¥é‡‡ç”¨è’™ç‰¹å¡æ´›æ–¹æ³•è¿›è¡Œé‡‡æ ·ä¼°è®¡ï¼š <spanclass="math display">\[L(W) \approx \frac1n \sum_{i=1}^n L_i(x_i, y_i, W) + \lambda R(W)\\\nabla_WL(W) \approx \frac1n \sum _{i=1}^n \nabla_WL_i(x_i, y_i, W) +\lambda \nabla _WR(W)\]</span>æ‰€ä»¥æˆ‘ä»¬ä¼šä»æ•´ä¸ªæ•°æ®é›†ä¸­é‡‡æ ·å‡ºminibatchæ¥ä¼°è®¡æ¢¯åº¦ï¼Œç§°ä¸º<strong>éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆstochasticgradient descentï¼‰</strong>ï¼Œminibatchçš„å¤§å°é€šå¸¸ä¸º32ã€64æˆ–128</p><p>ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    minibatch = sample_data(data, batch_size)<br>    dw = compute_gradient(loss_fn, minibatch, w)<br>    w -= learning_rate * dw<br></code></pre></td></tr></table></figure><p>å…¶ä¸­hyperparametersæœ‰<code>initialize_weights()</code>ï¼Œ<code>num_steps</code>ã€<code>learning_rate</code>ã€<code>batch_size</code>å’Œ<code>sample_data()</code></p><h2 id="sgd-momentum-sgdm">3.4 SGD + Momentum (SGDM)</h2><p>å½“loss functionæœ‰å±€éƒ¨æœ€å°å€¼ï¼ˆlocal minimumï¼‰æˆ–éç‚¹ï¼ˆsaddlepointï¼‰æ—¶ï¼ŒSGDæ–¹æ³•ä¼šç«‹å³åœæ­¢ï¼Œæˆ‘ä»¬å¼•å…¥â€é€Ÿåº¦å‘é‡â€œæ¥æ¨¡æ‹Ÿä¸€ä¸ªå°çƒæ²¿æ–œå¡ä¸‹æ»šçš„è¿‡ç¨‹ï¼š<span class="math display">\[v_{t+1} = \rho v_t + \nabla f(x_t)\\x_{t+1} = x_t - \alpha v_{t+1}\]</span> é€Ÿåº¦å‘é‡çš„å®è´¨å°±æ˜¯æ¢¯åº¦çš„ç´¯è®¡ï¼Œå‡½æ•°å°†æ²¿ç´¯è®¡çš„æ¢¯åº¦æ–¹å‘ä¸‹é™</p><p>ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent with momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    v = rho * v + dw<br>    w -= learning_rate * v<br></code></pre></td></tr></table></figure><p>åŒæ—¶ç”±äºé€Ÿåº¦å‘é‡çš„å¼•å…¥ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„hyperparameter<code>rho</code>æ¨¡æ‹Ÿæ‘©æ“¦ç³»æ•°ä»¥ä¿è¯å°çƒæœ€ç»ˆä¼šåœæ­¢ï¼Œé€šå¸¸<code>rho</code>çš„å€¼ä¸º0.9æˆ–0.99</p><h2 id="nesterov-momentum">3.5 Nesterov Momentum</h2><p>åœ¨ç´¯è®¡æ¢¯åº¦æ—¶ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸ç´¯è®¡å½“å‰çš„æ¢¯åº¦ï¼Œè€Œæ˜¯å‡è®¾å°çƒä»¥å½“å‰çš„é€Ÿåº¦è¿åŠ¨ä¸€å°æ®µè·ç¦»åï¼Œç´¯è®¡è¿åŠ¨åå°çƒå¤„çš„æ¢¯åº¦ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸º<strong>Nesterovmomentum</strong>ï¼š <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t)\\x_{t+1} = x_t + v_{t+1}\]</span> é€šå¸¸ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œæˆ‘ä»¬ä»¤<span class="math inline">\(\tilde x =x_t + \rho v_t\)</span>ï¼Œäºæ˜¯æœ‰ï¼š <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(\tilde x_t)\\\tilde x_{t+1} = \tilde x_t + v_{t+1} +\rho ( v_{t+1} - v_t)\]</span> ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Nesterov momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    old_v = v<br>    v = rho * v - learning_rate * dw<br>    w -= rho * old_v - (<span class="hljs-number">1</span> + rho) * v<br></code></pre></td></tr></table></figure><h2 id="adagrad">3.6 AdaGrad</h2><p>åœ¨æ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œç”±äºlearningrateæ˜¯å›ºå®šçš„ï¼Œæ¢¯åº¦å¤§å°ä¸åŒçš„æ–¹å‘ä¸‹é™çš„é€Ÿåº¦æ˜¯ç›¸åŒçš„</p><p>ä½†æ˜¯æˆ‘ä»¬å¸Œæœ›åœ¨æ¢¯åº¦è¾ƒå¤§çš„æ–¹å‘å‡ç¼“ä¸‹é™é€Ÿåº¦ä»¥é˜²éœ‡è¡ï¼Œæ¢¯åº¦è¾ƒå°çš„æ–¹å‘åŠ å¿«ä¸‹é™é€Ÿåº¦ï¼Œäºæ˜¯æˆ‘ä»¬ç”¨<strong>AdaGrad</strong>ä½¿learningrateè‡ªé€‚åº”å¤§å°ï¼š <span class="math display">\[S_{t+1} = S_t + \nabla_W^2 \\x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span> å¼ä¸­å¹³æ–¹å’Œé™¤æ³•å‡æŒ‡çŸ©é˜µæŒ‰å…ƒç´ æ“ä½œï¼Œ<spanclass="math inline">\(\varepsilon\)</span>æ˜¯ä¸€ä¸ªæå°çš„æ•°å­—ä»¥å¢å¼ºæ•°å€¼ç¨³å®šæ€§ï¼Œé˜²æ­¢å‡ºç°åˆ†æ¯ä¸º0çš„æƒ…å†µ</p><p>ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># AdaGrad</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared += dw * dw<br>    w -= learning_rate * dw / (grad_sqruared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><h2 id="rmsprop">3.7 RMSProp</h2><p>åœ¨AdaGradä¸­ï¼Œè‹¥è¿­ä»£çš„æ¬¡æ•°è¿‡å¤šï¼Œå¯èƒ½ä¼šå‡ºç°æ¢¯åº¦å¹³æ–¹ç´¯ç§¯è¿‡å¤§çš„æƒ…å†µï¼Œå¯¼è‡´learningrateè¿‡å°è€Œä¸èƒ½äº§ç”Ÿæœ‰æ•ˆè¿­ä»£</p><p><strong>RMSProp</strong>æ•ˆä»¿SGD+Momentumå¼•å…¥æ‘©æ“¦ç³»æ•°<spanclass="math inline">\(\rho\)</span>ä¿è¯å°çƒæœ€ç»ˆåœæ­¢çš„æƒ³æ³•ï¼Œå¼•å…¥â€œæ‘©æ“¦â€œä»¥é˜²æ­¢æ¢¯åº¦å¹³æ–¹çš„ç´¯ç§¯è¿‡å¤§ï¼š<span class="math display">\[S_{t+1} = \rho S_t + (1-\rho)\nabla_W^2 \\x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span> å¼ä¸­å¹³æ–¹å’Œé™¤æ³•å‡æŒ‡çŸ©é˜µæŒ‰å…ƒç´ æ“ä½œï¼Œ<spanclass="math inline">\(\varepsilon\)</span>æ˜¯ä¸€ä¸ªæå°çš„æ•°å­—ä»¥å¢å¼ºæ•°å€¼ç¨³å®šæ€§ï¼Œé˜²æ­¢å‡ºç°åˆ†æ¯ä¸º0çš„æƒ…å†µ</p><p>ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># RMSProp</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared = decay_rate * grad_squared + (<span class="hljs-number">1</span> - decay_rate) * dw * dw<br>    w -= learning_rate * dw / (grad_squared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>decay_rate</code>ä¸ºæ–°çš„hyperparameter</p><h2 id="adam">3.8 Adam</h2><p>å°†RMSPropå’Œmomentumçš„æ€æƒ³ç»“åˆèµ·æ¥ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†<strong>Adam</strong>ï¼š<span class="math display">\[v_{t+1} = \rho_1 v_t + (1 - \rho_1)\nabla _W\\S_{t+1} = \rho_2 S_t + (1-\rho_2)\nabla_W^2\\x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span> å¼ä¸­å¹³æ–¹å’Œé™¤æ³•å‡æŒ‡çŸ©é˜µæŒ‰å…ƒç´ æ“ä½œï¼Œ<spanclass="math inline">\(\varepsilon\)</span>æ˜¯ä¸€ä¸ªæå°çš„æ•°å­—ä»¥å¢å¼ºæ•°å€¼ç¨³å®šæ€§ï¼Œé˜²æ­¢å‡ºç°åˆ†æ¯ä¸º0çš„æƒ…å†µ</p><p>ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>å’Œ<code>beta2</code>ä¸ºæ–°çš„hyperparameters</p><p>ä½†æ˜¯åœ¨ç¬¬ä¸€æ¬¡è¿­ä»£æ—¶ï¼Œç”±äº<spanclass="math inline">\(\rho_2\)</span>é€šå¸¸è¶‹è¿‘äº1ï¼Œ<spanclass="math inline">\(S_1\)</span>å’Œ<spanclass="math inline">\(S_2\)</span>é€šå¸¸ä¼šè¶‹è¿‘äº0ï¼Œå¯¼è‡´ç¬¬ä¸€æ¬¡è¿­ä»£çš„è·¨åº¦ä¼šéå¸¸å¤§ï¼Œå®¹æ˜“äº§ç”Ÿä¸å¥½çš„ç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨Adamä¸­ä¼šåŠ å…¥<strong>biascorrection</strong>æ¥ä¿®æ­£ï¼š <span class="math display">\[v_{t+1} = \frac{\rho_1 v_t + (1 - \rho_1)\nabla_W}{\sqrt{1-\rho_1^{t+1}}}\\S_{t+1} = \frac{\rho_2 S_t +(1-\rho_2)\nabla_W^2}{\sqrt{1-\rho_2^{t+1}}}\\x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span> ä¿®æ­£åçš„ä»£ç æ€è·¯å¦‚ä¸‹ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam with bias correction</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    moment1_unbias = moment1 / (<span class="hljs-number">1</span> - beta1 ** t)<br>    moment2_unbias = moment2 / (<span class="hljs-number">1</span> - beta2 ** t)<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>é€šå¸¸å–0.9ï¼Œ<code>beta2</code>é€šå¸¸å–0.999ï¼Œ<code>learning_rate</code>é€šå¸¸å–1e-3ã€5e-4ã€1e-4</p><h2 id="äºŒé˜¶ä¼˜åŒ–-second-order-optimization">3.9 äºŒé˜¶ä¼˜åŒ– Second-OrderOptimization</h2><p>ä»¥ä¸Šæˆ‘ä»¬æåˆ°çš„ä¼˜åŒ–æ–¹å¼å‡ä¸º<strong>ä¸€é˜¶ä¼˜åŒ–</strong>ï¼Œå®ƒä»¬é€šè¿‡æ¢¯åº¦æ¥çº¿æ€§æ‹Ÿåˆå‡½æ•°å¹¶è¿­ä»£ä»¥å¾—åˆ°å‡½æ•°æœ€å°å€¼</p><p>é‚£ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘é€šè¿‡æ¢¯åº¦å’Œé»‘å¡çŸ©é˜µæ¥äºŒæ¬¡æ‹Ÿåˆå‡½æ•°ï¼Œä¸å¦¨è®¾äºŒæ¬¡æ‹Ÿåˆçš„è¿­ä»£å¼ä¸º<span class="math display">\[x_{t+1} = x_t+d\]</span> æˆ‘ä»¬å¸Œæœ›<spanclass="math inline">\(x_{t+1}\)</span>å°½å¯èƒ½å°ï¼Œå³æ±‚ <spanclass="math display">\[d = \arg \min_d f(x_t+d)\]</span> <span class="math inline">\(f(x)\)</span>åœ¨<spanclass="math inline">\(x_t\)</span>å¤„çš„äºŒé˜¶æ³°å‹’å±•å¼€å¼ä¸º <spanclass="math display">\[f(x) = f(x_t) + (x - x_t)^{\mathsf{T}}\nabla f(x_t) +\frac12(x-x_t)^{\mathsf{T}}\mathbf{H}f(x_t)(x-x_t)\]</span> ä»£å…¥<span class="math inline">\(x_{t+1}\)</span>çš„å€¼å¾— <spanclass="math display">\[f(x_{t+1}) = f(x_t+d)=f(x_t)+d^{\mathsf{T}}\nablaf(x_t)+\frac12d^{\mathsf{T}}\mathbf{H}f(x_t)d\]</span> å½“<span class="math inline">\(d=-[\mathbf{H}f(x_t)]^{-1}\nablaf(x_t)\)</span>æ—¶å–æœ€å°å€¼</p><p>åˆ™äºŒé˜¶ä¼˜åŒ–çš„è¿­ä»£å¼ä¸ºï¼š <span class="math display">\[x_{t+1} = x_t-\left[\mathbf{H}f(x_t)\right]^{-1}\nabla f(x_t)\]</span>ç„¶è€Œç”±äºé»‘å¡çŸ©é˜µå…ƒç´ æ•°é‡è¿‡å¤šä¸”çŸ©é˜µæ±‚é€†å¤æ‚åº¦è¿‡é«˜ï¼Œå®è·µä¸­å¾ˆå°‘ä½¿ç”¨äºŒé˜¶ä¼˜åŒ–</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>çº¿æ€§åˆ†ç±»å™¨ Linear Classifier</title>
    <link href="/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="çº¿æ€§åˆ†ç±»å™¨-linear-classifier">2. çº¿æ€§åˆ†ç±»å™¨ LinearClassifier</h1><h2 id="è¯„åˆ†å‡½æ•°-score-function">2.1 è¯„åˆ†å‡½æ•° Score Function</h2><p><strong>è¯„åˆ†å‡½æ•°ï¼ˆscore functionï¼‰</strong> æ˜¯<spanclass="math inline">\(\mathbb{R}^D \to\mathbb{R}^C\)</span>çš„çº¿æ€§æ˜ å°„ï¼Œå³ä¸€å¼ å›¾ç‰‡åœ¨æ¯ä¸ªæ ‡ç­¾ä¸Šæ‰€å¾—åˆ°çš„è¯„åˆ†ï¼š<span class="math display">\[f(x, W) = Wx+b,x\in \mathbb{R}^D, W\in \mathbb{R}^{C\times D}, b\in \mathbb{R}^D\]</span></p><p>å¼ä¸­<spanclass="math inline">\(x\)</span>æ˜¯å°†å›¾åƒæ•°æ®æ‹‰é•¿ï¼ˆflattenï¼‰å¾—åˆ°çš„<spanclass="math inline">\(D\)</span>ç»´åˆ—å‘é‡ï¼Œ<spanclass="math inline">\(W\)</span>æ˜¯<strong>å‚æ•°ï¼ˆparameterï¼‰</strong>æˆ–ç§°<strong>æƒé‡ï¼ˆweightï¼‰</strong> ï¼Œ<spanclass="math inline">\(b\)</span>ä¸º<strong>åå·®å‘é‡ï¼ˆbiasvectorï¼‰</strong></p><p><span class="math inline">\(C\)</span>ä¸ºå¾…åˆ†ç±»çš„æ ‡ç­¾ä¸ªæ•°ï¼Œ<spanclass="math inline">\(f(x,W)\)</span>å³ä¸ºè¯¥å›¾åƒå¯¹äºæ¯ä¸ªæ ‡ç­¾çš„è¯„åˆ†</p><h2 id="æŸå¤±å‡½æ•°-loss-function">2.2 æŸå¤±å‡½æ•° Loss Function</h2><p><strong>æŸå¤±å‡½æ•°ï¼ˆloss funtionï¼‰</strong>é‡åŒ–äº†çº¿æ€§åˆ†ç±»å™¨çš„æ•ˆæœï¼Œå…¶å€¼è¶Šé«˜ï¼Œåˆ™çº¿æ€§åˆ†ç±»å™¨æ•ˆæœè¶Šå·®ï¼Œåˆè¢«ç§°ä¸ºç›®æ ‡å‡½æ•°ï¼ˆobjectivefunctionï¼‰ã€ä»£ä»·å‡½æ•°ï¼ˆcost functionï¼‰</p><p>å¯¹äºä¸€ä¸ªæ•°æ®é›†ï¼š <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> å¼ä¸­<span class="math inline">\(x_i\)</span>æ˜¯å›¾åƒï¼Œ<spanclass="math inline">\(y_i\)</span>æ˜¯è¯¥å›¾åƒå¯¹åº”çš„æ­£ç¡®æ ‡ç­¾</p><p>åˆ™å¯¹äºå•ä¸ªçš„ä¸€å¼ å›¾åƒçš„æŸå¤±ä¸ºï¼š <span class="math display">\[L_i(f(x_i, W), y_i)\]</span> å¯¹äºæ•°æ®é›†æ¥è¯´ï¼ŒæŸå¤±æ˜¯æ¯å¼ å›¾åƒæŸå¤±çš„å¹³å‡å€¼ï¼š <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)\]</span> <span class="math inline">\(L_i\)</span>å³ä¸ºæŸå¤±å‡½æ•°</p><h2 id="å¤šç±»æ”¯æŒå‘é‡æœºæŸå¤±-multiclass-svm-loss">2.3 å¤šç±»æ”¯æŒå‘é‡æœºæŸå¤±Multiclass SVM Loss</h2><p>æœ´ç´ çš„æƒ³æ³•æ˜¯ï¼Œæ­£ç¡®çš„æ ‡ç­¾çš„è¯„åˆ†åº”å½“æ¯”å…¶ä»–æ ‡ç­¾çš„è¯„åˆ†è¦é«˜</p><p>æ‰€ä»¥ï¼Œå¯¹äºç»™å®šçš„ä¸€å¼ å›¾åƒ<span class="math inline">\(x_i,y_i\)</span>ï¼Œå…¶è¯„åˆ†<span class="math inline">\(s=f(x_i,W)\)</span>ï¼Œåˆ™SVMæŸå¤±æœ‰å¦‚ä¸‹å½¢å¼ï¼š <span class="math display">\[L_i = \sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)\]</span> å½“è¯„åˆ†å‡ä¸ºå¾ˆå°çš„éšæœºå€¼æ—¶ï¼ŒæŸå¤±åº”å½“æ¥è¿‘<spanclass="math inline">\(C-1\)</span>ï¼Œ<spanclass="math inline">\(C\)</span>ä¸ºå¾…åˆ†ç±»çš„æ€»æ ‡ç­¾æ•°ï¼Œæ­¤æ€§è´¨å¯ä½œä¸ºdebugçš„ä¾æ®</p><h2 id="æ­£åˆ™åŒ–-regularization">2.4 æ­£åˆ™åŒ– Regularization</h2><p>ä½¿ä¸Šè¿°çš„æŸå¤±<span class="math inline">\(L\)</span>æœ€å°çš„<spanclass="math inline">\(W\)</span>å¹¶ä¸å”¯ä¸€</p><p>è‹¥<span class="math inline">\(W\)</span>å¯ä½¿æŸå¤±<spanclass="math inline">\(L\)</span>æœ€å°ï¼Œåˆ™<spanclass="math inline">\(\lambda W\)</span>ä¹Ÿå¯ä½¿<spanclass="math inline">\(L\)</span>æœ€å°</p><p>äºæ˜¯ï¼Œæˆ‘ä»¬åœ¨æŸå¤±å‡½æ•°çš„è¡¨è¾¾å¼ä¸­å¼•å…¥ä¸€é¡¹<spanclass="math inline">\(\lambda R(W)\)</span> : <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)+\lambda R(W)\]</span> å¼ä¸­<spanclass="math inline">\(\lambda\)</span>ä¸º<strong>æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆregularizationstrengthï¼‰</strong> ï¼Œä¸ºè¶…å‚æ•°</p><p>æ­£åˆ™åŒ–çš„å¥½å¤„ï¼š</p><ol type="1"><li><p>è¿›ä¸€æ­¥åœ°ç­›é€‰<spanclass="math inline">\(W\)</span>ï¼Œä½¿æ‰€é€‰å®šçš„<spanclass="math inline">\(W\)</span>æ‹¥æœ‰é™¤æœ€å°åŒ–æŸå¤±ä»¥å¤–çš„å…¶ä»–åŠŸèƒ½</p></li><li><p>é¿å…<strong>è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰</strong></p></li><li><p>é€šè¿‡å¢åŠ æ›²ç‡ä»¥æé«˜<strong>ä¼˜åŒ–ï¼ˆoptimizationï¼‰</strong>æ•ˆæœ</p></li></ol><p>å¸¸ç”¨çš„æ­£åˆ™åŒ–æœ‰ï¼š</p><p>L2æ­£åˆ™åŒ– <span class="math display">\[R(W) = \sum_k \sum_l W_{k,l}^2\]</span> L1æ­£åˆ™åŒ– <span class="math display">\[R(W)= \sum_k \sum_l \left| W_{k,l}^2 \right |\]</span> æˆ–è€…å°†äºŒè€…è”ç³»èµ·æ¥ï¼š <span class="math display">\[R(W) = \sum_k \sum_l \beta W_{k,l}^2 + \left |W_{k,l}^2 \right |\]</span> å…¶ä»–è¿˜æœ‰dropoutã€Batchnormalizationã€Cutoutã€Mixupã€Stochastic depthç­‰ç­‰</p><h2 id="äº¤å‰ç†µæŸå¤±-cross-entropy-loss">2.5 äº¤å‰ç†µæŸå¤± Cross-EntropyLoss</h2><p>å¦ä¸€ç§æŸå¤±å‡½å‡½æ•°ä½¿ç”¨<strong>å½’ä¸€åŒ–æŒ‡æ•°å‡½æ•°ï¼ˆsoftmaxfunctionï¼‰</strong> å°†è¯„åˆ†ç”¨æ¦‚ç‡æ¥æè¿°ï¼Œè¢«ç§°ä¸º<strong>äº¤å‰ç†µæŸå¤±ï¼ˆCross- Entropy Lossï¼‰</strong>æˆ–è€…<strong>å¤šå…ƒé€»è¾‘å›å½’ï¼ˆMultinomial Logistic Regressionï¼‰</strong></p><p>å¯¹äºä¸€ä¸ªè¯„åˆ†å‡½æ•°<spanclass="math inline">\(s=f(x_i,W)\)</span>ï¼Œå…¶softmax functionçš„å½¢å¼ä¸ºï¼š<span class="math display">\[P(Y = k|X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}\]</span> æˆ‘ä»¬å‡è®¾çœŸå®çš„æ¦‚ç‡åˆ†å¸ƒä¸º<spanclass="math inline">\(P\)</span>ï¼Œè®­ç»ƒå¾—åˆ°çš„æ¦‚ç‡åˆ†å¸ƒä¸º<spanclass="math inline">\(Q\)</span>ï¼Œæˆ‘ä»¬ä½¿ç”¨<spanclass="math inline">\(Q\)</span>æ¥æ‹Ÿåˆ<spanclass="math inline">\(P\)</span>ï¼Œåˆ™äº¤å‰ç†µä¸ºï¼š <spanclass="math display">\[H(P, Q) = H(P) + D_{KL}(P||Q)\]</span> å¼ä¸­<spanclass="math inline">\(D_{KL}(P||Q)\)</span>ä¸º<strong>ç›¸å¯¹ç†µï¼ˆKullback-LeiblerDivergenceï¼‰</strong> ï¼š <span class="math display">\[D_{KL}(P||Q) = \sum_{i=1}^n P(x_i)\log \frac{P(x_i)}{Q(x_i)}\]</span> ç”±äºçœŸå®çš„æ¦‚ç‡åˆ†å¸ƒä¸å˜ï¼Œå³<spanclass="math inline">\(H(P)\)</span>ä¸å˜ï¼Œåˆ™è‹¥äº¤å‰ç†µ<spanclass="math inline">\(H(P,Q)\)</span>æœ€å°ï¼Œåªéœ€ç›¸å¯¹ç†µ<spanclass="math inline">\(D_{KL}(P||Q)\)</span>æœ€å°å³å¯</p><p>å½“å•å¼ å›¾ç‰‡çš„æŸå¤±å…·æœ‰å¦‚ä¸‹å½¢å¼æ—¶ï¼Œäº¤å‰ç†µæœ€å°ï¼š <spanclass="math display">\[L_i = - \log P(Y=y_i|X = x_i)\]</span> æ‰€ä»¥äº¤å‰ç†µæŸå¤±çš„å…·ä½“å½¢å¼ä¸ºï¼š <span class="math display">\[L = \frac1N \sum_i\left(-\log\left(\frac{e^{s_{y_i}}}{\sum_je^{s_j}}\right)\right)+\lambda R(W)\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>å›¾åƒåˆ†ç±» Image Classification</title>
    <link href="/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    <url>/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="å›¾åƒåˆ†ç±»-image-classification">1. å›¾åƒåˆ†ç±» ImageClassification</h1><h2 id="å›¾åƒåˆ†ç±»å™¨">1.1 å›¾åƒåˆ†ç±»å™¨</h2><p>å›¾åƒåˆ†ç±»çš„ç®—æ³•éš¾ä»¥ç”¨å¦‚ä¸‹çš„å‡½æ•°è¿›è¡Œç¡¬ç¼–ç (hard-code)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify_image</span>(<span class="hljs-params">image</span>):<br><span class="hljs-comment"># Some magic here?</span><br><span class="hljs-keyword">return</span> class_label<br></code></pre></td></tr></table></figure><p>æ‰€ä»¥é€šå¸¸é‡‡ç”¨æœºå™¨å­¦ä¹ ï¼Œå³Data-Drivençš„æ–¹å¼</p><p>å…ˆç”¨åŒ…å«å›¾åƒä¸æ ‡ç­¾çš„æ•°æ®é›†è®­ç»ƒåˆ†ç±»å™¨ï¼Œå†è¯„ä¼°åˆ†ç±»å™¨åœ¨åˆ†ç±»æ–°å›¾ç‰‡çš„è¡¨ç°ï¼š</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">images, labels</span>):<br><span class="hljs-comment"># Machine learning!</span><br><span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">model, test_images</span>):<br><span class="hljs-comment"># Use model to predict labels</span><br><span class="hljs-keyword">return</span> test_labels<br></code></pre></td></tr></table></figure><h2 id="å¸¸è§æ•°æ®é›†">1.2 å¸¸è§æ•°æ®é›†</h2><p>MNIST</p><p>CIFAR10</p><p>CIFAR100</p><p>ImageNet</p><p>MIT Places</p><h2 id="é‚»è¿‘ç®—æ³•-nearest-neighbor">1.3 é‚»è¿‘ç®—æ³• Nearest Neighbor</h2><ol type="1"><li>è®°å¿†æ•°æ®é›†ä¸­æ‰€æœ‰çš„å›¾åƒå’Œå¯¹åº”çš„æ ‡ç­¾</li><li>ä¸æ•°æ®é›†ä¸­æœ€ç›¸ä¼¼çš„å›¾åƒçš„æ ‡ç­¾å³ä¸ºæ–°å›¾åƒçš„æ ‡ç­¾</li></ol><p>ä½¿ç”¨<strong>è·ç¦»åº¦é‡</strong>æ¯”è¾ƒå›¾åƒï¼Œä»¥ä¸‹æ˜¯å¸¸è§çš„ä¸¤ç§è·ç¦»åº¦é‡ï¼š</p><p>L1 è·ç¦»ï¼ˆæ›¼å“ˆé¡¿è·ç¦» Manhattan distanceï¼‰: <spanclass="math display">\[d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|\]</span> L2 è·ç¦»ï¼ˆæ¬§æ‹‰è·ç¦» Euclidean distanceï¼‰ï¼š <spanclass="math display">\[d_2(I_1, I_2) = \sqrt{\sum_p (I_1^p - I_2^p)^2}\]</span></p><h2 id="kè¿‘é‚»ç®—æ³•-k-nearest-neighbors">1.4 Kè¿‘é‚»ç®—æ³• K-NearestNeighbors</h2><p>è‹¥ ğ‘˜ ä¸ªæœ€ç›¸ä¼¼çš„å›¾åƒä¸­çš„å¤§å¤šæ•°ä¸ºæŸä¸€ä¸ªæ ‡ç­¾ï¼Œåˆ™è¯¥å›¾åƒä¹Ÿå±äºè¿™ä¸ªæ ‡ç­¾</p><p>å½“è®­ç»ƒçš„æ ·æœ¬è¶³å¤Ÿå¤šæ—¶ï¼ŒKè¿‘é‚»ç®—æ³•å¯ä»¥è¡¨ç¤ºä»»ä½•å‡½æ•°</p><h2 id="è¶…å‚æ•°-hyperparameter">1.5 è¶…å‚æ•° Hyperparameter</h2><p>ä»æ•°æ®é›†ä¸­æ— æ³•é€šè¿‡è®­ç»ƒå¾—åˆ°çš„å‚æ•°ä¸º<strong>è¶…å‚æ•°</strong>ï¼Œè¶…å‚æ•°éœ€è¦åœ¨å­¦ä¹ ä¹‹å‰è®¾å®š</p><p>ä¾‹å¦‚Ké‚»è¿‘ç®—æ³•ä¸­çš„Kå°±æ˜¯è¶…å‚æ•°</p><p>è¶…å‚æ•°çš„è®¾å®šæ–¹æ³•ï¼š</p><p>å°†æ•°æ®é›†åˆ†ä¸ºtrainã€validateå’Œtestä¸‰éƒ¨åˆ†ï¼Œé€‰æ‹©è¶…å‚æ•°çš„å€¼åœ¨trainä¸Šè®­ç»ƒï¼Œå¹¶åœ¨validateä¸­éªŒè¯ï¼Œåªåœ¨æœ€åä½¿ç”¨testä¸ŠæŸ¥çœ‹æ•ˆæœ</p><p>å¦‚æœæ¡ä»¶å…è®¸ï¼Œå¯ä»¥å°†æ•°æ®é›†åˆ†å‰²æˆè®¸å¤šéƒ¨åˆ†ï¼Œæ¯æ¬¡é€‰æ‹©ä¸åŒçš„éƒ¨åˆ†ä½œä¸ºvalidateï¼Œå‰©ä¸‹çš„éƒ¨åˆ†ä½œä¸ºtrainï¼ŒåŒæ ·çš„åªåœ¨æœ€ååœ¨testä¸Šæ£€éªŒ</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>æ·±åº¦å­¦ä¹ ä¸è®¡ç®—æœºè§†è§‰</title>
    <link href="/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    <url>/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h2 id="å†™åœ¨å‰é¢">å†™åœ¨å‰é¢</h2><p>è¿™æ˜¯ä¸€ç¯‡Umichçš„EECS498-007è¯¾ç¨‹çš„å­¦ä¹ ç¬”è®°ï¼Œå¼€å§‹é˜…è¯»ä¹‹å‰æ‚¨éœ€è¦å­¦ä¹ è¿‡è¿™é—¨è¯¾ç¨‹æˆ–å·²ç»æŒæ¡äº†æ·±åº¦å­¦ä¹ ä¸è®¡ç®—æœºè§†è§‰çš„åŸºæœ¬çŸ¥è¯†ã€‚æœ¬ç¯‡æ—¨åœ¨å¤ä¹ è¯¾ç¨‹è¦ç‚¹ï¼Œä¸é€‚åˆåˆå­¦è€…ä½œä¸ºtutorialå­¦ä¹ ã€‚</p><h2 id="å…³äºumich-eecs-498-007">å…³äºUmich EECS 498-007</h2><p>æ¥è‡ª<ahref="https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/EECS498-007/">csdiy</a>çš„è¯¾ç¨‹ç®€ä»‹</p><blockquote><p>UMich çš„ Computer Visionè¯¾ï¼Œè¯¾ç¨‹è§†é¢‘å’Œä½œä¸šè´¨é‡æé«˜ï¼Œæ¶µç›–çš„ä¸»é¢˜éå¸¸å…¨ï¼ŒåŒæ—¶ Assignmentsçš„éš¾åº¦ç”±æµ…åŠæ·±ï¼Œè¦†ç›–äº† CV ä¸»æµæ¨¡å‹å‘å±•çš„å…¨é˜¶æ®µï¼Œæ˜¯ä¸€é—¨éå¸¸å¥½çš„ ComputerVision å…¥é—¨è¯¾ã€‚</p></blockquote><p>æœ¬ç¯‡ç¬”è®°åŸºäºFall2019çš„è¯¾ç¨‹ï¼Œè¯¾ç¨‹é“¾æ¥ï¼šhttps://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r</p><p>ä¸æ­¤åŒæ—¶ï¼Œè¯¥è¯¾ç¨‹çš„assignmentsï¼ˆå«ç­”æ¡ˆï¼‰å·²ä¸Šä¼ è‡³<ahref="https://github.com/hmnkapa/eecs598">github</a>ï¼Œç­”æ¡ˆæ˜¯æˆ‘è‡ªå·±åšçš„ï¼Œä»…ä¾›å‚è€ƒ</p><p>Assignmentsæ¥è‡ªFall 2020çš„<ahref="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">è¯¾ç¨‹ä¸»é¡µ</a></p><h2 id="ç›®å½•">ç›®å½•</h2><ol type="1"><li>å›¾åƒåˆ†ç±» Image Classification</li><li>çº¿æ€§åˆ†ç±»å™¨ Linear Classifier</li><li>ä¼˜åŒ–ç†è®º Optimization</li><li>ç¥ç»ç½‘ç»œ Neural Networks</li><li>åå‘ä¼ æ’­ç®—æ³• Backpropagation</li><li>å·ç§¯ç½‘ç»œ Convolutional Networks</li><li>CNNæ¶æ„ CNN Architectures</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>æŸ¯è¥¿-æ–½ç“¦èŒ¨ï¼ˆCauchy-Schwarzï¼‰ä¸ç­‰å¼çš„ç§¯åˆ†å½¢å¼</title>
    <link href="/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/"/>
    <url>/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼çš„ç§¯åˆ†å½¢å¼">æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼çš„ç§¯åˆ†å½¢å¼</h2><p>å‡è®¾ <span class="math inline">\(ğ‘“(ğ‘¥)\)</span> å’Œ <spanclass="math inline">\(ğ‘”(ğ‘¥)\)</span> åœ¨åŒºé—´ <spanclass="math inline">\([ğ‘,ğ‘]\)</span> ä¸Šé»æ›¼å¯ç§¯ï¼Œé‚£ä¹ˆ</p><p><span class="math display">\[\int_ğ‘^ğ‘ğ‘“^2(ğ‘¥)\mathrm{d}ğ‘¥â‹…\int_ğ‘^ğ‘ğ‘”^2(ğ‘¥)\mathrm{d}ğ‘¥â‰¥(\int_ğ‘^ğ‘ğ‘“(ğ‘¥)ğ‘”(ğ‘¥)\mathrm{d}ğ‘¥)^2\]</span></p><h2 id="å‰ç½®çŸ¥è¯†">å‰ç½®çŸ¥è¯†</h2><p>æŸ¯è¥¿ä¸ç­‰å¼ <span class="math display">\[(ğ‘^2+ğ‘^2)(ğ‘^2+ğ‘‘^2)â‰¥(ğ‘ğ‘+ğ‘ğ‘‘)^2\]</span> åŠå…¶ä¸€èˆ¬å½¢å¼ <span class="math display">\[\displaystyle\sum_{ğ‘–=1}^ğ‘›ğ‘_ğ‘–^2\sum_{ğ‘–=1}^ğ‘›ğ‘_ğ‘–^2â‰¥(\sum_{ğ‘–=1}^ğ‘›ğ‘_ğ‘–ğ‘_ğ‘–)^2\]</span> æé™ã€å¾®ç§¯åˆ†åŸºæœ¬çŸ¥è¯†</p><h2id="æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼çš„ç§¯åˆ†å½¢å¼çš„è¯æ˜">æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼çš„ç§¯åˆ†å½¢å¼çš„è¯æ˜</h2><p>äº‹å®ä¸Šï¼Œè¯´èµ·æŸ¯è¥¿ï¼Œåœ¨é«˜ä¸­è¯¾æœ¬ï¼ˆä¹ é¢˜ï¼‰ä¸Šæˆ‘ä»¬å·²ç»å­¦ä¹ è¿‡æŸ¯è¥¿ä¸ç­‰å¼çš„äºŒç»´å½¢å¼</p><p><span class="math display">\[(ğ‘^2+ğ‘^2)(ğ‘^2+ğ‘‘^2)â‰¥(ğ‘ğ‘+ğ‘ğ‘‘)^2\]</span> è¿›ä¸€æ­¥åœ°ï¼ŒæŸ¯è¥¿ä¸ç­‰å¼çš„ä¸€èˆ¬å½¢å¼å¦‚ä¸‹ <spanclass="math display">\[\displaystyle\sum_{ğ‘–=1}^ğ‘›ğ‘_ğ‘–^2\sum_{ğ‘–=1}^ğ‘›ğ‘_ğ‘–^2â‰¥(\sum_{ğ‘–=1}^ğ‘›ğ‘_ğ‘–ğ‘_ğ‘–)^2\]</span> æ—¢ç„¶éƒ½å«æŸ¯è¥¿ï¼ˆ</p><p>æˆ‘ä»¬è€ƒè™‘ä½¿ç”¨æŸ¯è¥¿ä¸ç­‰å¼çš„ä¸€èˆ¬å½¢å¼æ¥è¯æ˜æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼çš„ç§¯åˆ†å½¢å¼</p><p>ç”±å®šç§¯åˆ†çš„å®šä¹‰ï¼ŒæŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼çš„ç§¯åˆ†å½¢å¼ç­‰ä»·äº <spanclass="math display">\[\lim_{n\to\infty}\sum_{i=1}^nf^2(x_i)\Delta{x} \cdot\lim_{n\to\infty}\sum_{i=1}^ng^2(x_i)\Delta{x} \geq \left(\lim_{n\to\infty}\sum_{i=1}^nf(x_i)g(x_i)\Delta{x} \right)^2\\\]</span> æ­¤å¤„<spanclass="math inline">\(\Delta{x}=\frac{b-a}{n}\)</span>ï¼Œ<spanclass="math inline">\(x_i=a+i\Delta{x}\)</span></p><p>ç”±æé™çš„è¿ç®—æ³•åˆ™ï¼Œä¸Šå¼ç­‰ä»·äº <span class="math display">\[\lim_{n\to\infty} \left[ \sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i)- \left( \sum_{i=1}^nf(x_i)g(x_i) \right)^2 \right] \Delta^2{x} \geq0\\\]</span> ç”±æé™çš„ä¿å·æ€§å¹¶çº¦å»<spanclass="math inline">\(\Delta^2{x}\)</span>ï¼Œæœ‰ <spanclass="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) - \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \geq0\\\]</span> å³ <span class="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) \geq \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \\\]</span> æ­¤å³æŸ¯è¥¿ä¸ç­‰å¼çš„ä¸€èˆ¬å½¢å¼.</p><p>æŸ¯è¥¿ä¸ç­‰å¼çš„ä¸€èˆ¬å½¢å¼çš„å–ç­‰æ¡ä»¶ä¸º<spanclass="math inline">\(\frac{a_1}{b_1}=\frac{a_2}{b_2}=...=\frac{a_n}{b_n}\)</span>ï¼Œå› æ­¤å¯¹äºæŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼çš„ä¸€èˆ¬å½¢å¼ï¼Œ</p><p>å½“ä¸”ä»…å½“ <span class="math display">\[f(x)=\lambda g(x)\]</span> æ—¶ï¼Œç­‰å·æˆç«‹ï¼Œå¼ä¸­<spanclass="math inline">\(\lambda\)</span>ä¸ºä¸€å®æ•°.</p><h2 id="ä¸€é“ä¾‹é¢˜">ä¸€é“ä¾‹é¢˜</h2><p>è¿™ä¸ªä¸ç­‰å¼æ˜¯æœ€è¿‘åœ¨å‡†å¤‡å¾®ç§¯åˆ†çš„ç¬¬ä¸€æ¬¡æœŸä¸­è€ƒè¯•æ—¶é‡åˆ°çš„</p><p>ï¼ˆå…¶å®æˆ‘å¼€å§‹å†™æ–‡ç« çš„æ—¶å€™è·ç¦»è€ƒè¯•è¿˜æœ‰24åˆ†é’Ÿ</p><p>æƒ³æ‰¾ä¸€äº›å·å­åšä¸€åš</p><p>äºæ˜¯å‘ç°äº†ä¸€ä»½æµ™æ±ŸæŸ211å¤§å­¦å·ç§°<ahref="https://www.zhihu.com/question/265940112?utm_medium=social&amp;utm_oi=1650268599276343296&amp;utm_psn=1703111356650299392&amp;utm_source=qq">120å¹´æ¥æœ€éš¾çš„å¾®ç§¯åˆ†è¯•å·</a></p><p>å…¶æœ€åä¸€é¢˜åŸé¢˜å¦‚ä¸‹ï¼š</p><blockquote><ol start="12" type="1"><li>è®¾ <span class="math inline">\(ğ‘“(ğ‘¥)\)</span> åœ¨ <spanclass="math inline">\([0,1]\)</span> ä¸Šè¿ç»­ä¸”å¯å¯¼ï¼Œå½“ <spanclass="math inline">\(ğ‘¥âˆˆ[0,1]\)</span> æ—¶ï¼Œ <spanclass="math inline">\(\displaystyle\int_ğ‘¥^1ğ‘“(ğ‘¡)\mathrm{d}ğ‘¡â‰¥\frac{1-x^3}{2}\)</span> ï¼Œè¯æ˜ï¼š <spanclass="math display">\[\displaystyle \int_0^1\,[f(x)]^2\mathrm{d}x&gt;\frac{5}{12}\\\]</span></li></ol></blockquote><p>ä»¤<span class="math inline">\(\displaystyleF(x)=\int_x^1\,f(t)\mathrm{d}t=-\int_1^x\,f(t)\mathrm{d}t\)</span> ï¼Œåˆ™<span class="math inline">\(\displaystyle F^\prime(x)=-f(x)\)</span></p><p>ç”± <span class="math inline">\(\displaystyleF(x)\geq\frac{1-x^3}{2}\)</span> å¯å¾— <span class="math display">\[\int_0^1\,F(x)\mathrm{d}x \geq\int_0^1\frac{1-x^3}{2}\mathrm{d}x=\frac{3}{8}\\\]</span> ä¸‹é¢è¿ç”¨åˆ†éƒ¨ç§¯åˆ†æ³•æ¥è®¡ç®— <spanclass="math inline">\(\displaystyle \int_0^1\,F(x)\mathrm{d}x\)</span><span class="math display">\[\begin{align} \int_0^1\,F(x)\mathrm{d}x&amp;=xF(x)\Big]_0^1-\int_0^1x\mathrm{d}F(x)\\&amp;=F(1)+\int_0^1xf(x)\mathrm{d}x\\ &amp;=\int_0^1xf(x)\mathrm{d}x\\\end{align} \\\]</span></p><p>äºæ˜¯ï¼Œæˆ‘ä»¬æœ‰ <span class="math display">\[\int_0^1xf(x)\mathrm{d}x \geq \frac{3}{8} \\\]</span> ç”±æŸ¯è¥¿-æ–½ç“¦èŒ¨ä¸ç­‰å¼ <span class="math display">\[\int_0^1\,f^2(x)\mathrm{d}x\cdot\int_0^1\,x^2\mathrm{d}x \geq \left(\int_0^1\,xf(x)\mathrm{d}x \right)^2 \geq \frac{9}{64} \\\]</span> <span class="math inline">\(\displaystyle\int_0^1\,x^2\mathrm{d}x\)</span>æ˜¯å¥½ç§¯çš„ï¼Œå…¶ç»“æœä¸º <spanclass="math inline">\(\frac{1}{3}\)</span> ï¼Œæ•… <spanclass="math display">\[\int_0^1\,f^2(x)\mathrm{d}x \geq \frac{27}{64}&gt;\frac{5}{12}\\\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
