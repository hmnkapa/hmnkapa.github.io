<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>实系数与复系数的一阶线性常微分方程</title>
    <link href="/2025/06/02/%E5%AE%9E%E7%B3%BB%E6%95%B0%E4%B8%8E%E5%A4%8D%E7%B3%BB%E6%95%B0%E7%9A%84%E4%B8%80%E9%98%B6%E7%BA%BF%E6%80%A7%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    <url>/2025/06/02/%E5%AE%9E%E7%B3%BB%E6%95%B0%E4%B8%8E%E5%A4%8D%E7%B3%BB%E6%95%B0%E7%9A%84%E4%B8%80%E9%98%B6%E7%BA%BF%E6%80%A7%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1id="实系数与复系数的一阶线性常微分方程">实系数与复系数的一阶线性常微分方程</h1><h2id="一阶线性微分方程first-order-linear-ode">一阶线性微分方程（First-OrderLinear ODE）</h2><p>一阶线性常微分方程是形如：</p><p><span class="math display">\[\displaystyle y^{\prime}=a(t)y+b(t)\\\]</span> 的方程，其中 <span class="math inline">\(\displaystyley^{\prime}=\frac{dy}{dt}，\)</span>并且 <spanclass="math inline">\(\displaystyle a(t)\)</span> 和 <spanclass="math inline">\(\displaystyle b(t)\)</span> 是 <spanclass="math inline">\(\displaystyle t\)</span> 的函数</p><ul><li>如果 <span class="math inline">\(\displaystyle b(t)\equiv0\)</span>（意味着对于所有 <span class="math inline">\(\displaystyle t\)</span>，<span class="math inline">\(\displaystyle b(t)\)</span>均为零），该ODE称为<strong>齐次的（homogeneous）</strong></li><li>如果对于至少一个值， <span class="math inline">\(\displaystyleb(t)\ne0\)</span>，该ODE称为<strong>非齐次的（inhomogeneous）</strong></li></ul><p>这些ODE 的解通常在 <span class="math inline">\(\displaystylea(t)\)</span> 和 <span class="math inline">\(\displaystyle b(t)\)</span>有定义的任何地方都存在，并且具有显式形式（explicit form）</p><p>它们通常形成一个单参数的解族，并且与它们相关的初值问题（IVP）具有唯一的解</p><h2 id="实系数一阶线性ode">实系数一阶线性ODE</h2><h3 id="齐次情况">齐次情况</h3><p>对于齐次一阶线性 ODE：</p><p><span class="math display">\[\displaystyle y^{\prime}=a(t)y\\\]</span> 如果 <span class="math inline">\(\displaystyle a(t)\)</span>连续，其通解由下式给出：</p><p><span class="math display">\[\displaystyle y(t)=c\cdot e^{\int_{t_{0}}^{t}a(s)ds}\\\]</span> 其中 <span class="math inline">\(\displaystyle c\)</span>是一个任意常数（通常由初始条件 <span class="math inline">\(\displaystyley(t_{0})\)</span> 确定）</p><p><span class="math inline">\(\displaystyle y(t)\)</span> 的定义域与<span class="math inline">\(\displaystyle a(t)\)</span> 的定义域相同</p><h3 id="非齐次情况">非齐次情况</h3><p>对于非齐次一阶线性 ODE：</p><p><span class="math display">\[\displaystyle y^{\prime}=a(t)y+b(t)\\\]</span> 解是通过一种称为<strong>参数变易法（variation ofparameters）</strong>的技术找到的</p><p>其思想是采用齐次解 <span class="math inline">\(\displaystyley_{h}(t)=ce^{A(t)}\)</span> 的形式，并允许“常数”成为 <spanclass="math inline">\(\displaystyle t\)</span> 的函数，即 <spanclass="math inline">\(\displaystyle c(t)\)</span></p><p>令 <span class="math inline">\(\displaystyley_{p}(t)=c(t)e^{A(t)}\)</span> 为一个特解，其中 <spanclass="math inline">\(\displaystyleA(t)=\int_{t_{0}}^{t}a(s)ds\)</span></p><p>将其代入 ODE：</p><p><span class="math display">\[\displaystyle y_{p}^{\prime}(t)=c^{\prime}(t)e^{A(t)}+c(t)a(t)e^{A(t)}\\\]</span> 所以，</p><p><span class="math display">\[\displaystylec^{\prime}(t)e^{A(t)}+c(t)a(t)e^{A(t)}=a(t)c(t)e^{A(t)}+b(t)\\\]</span> 简化为 <span class="math inline">\(\displaystylec^{\prime}(t)e^{A(t)}=b(t)\)</span> 或 <spanclass="math inline">\(\displaystylec^{\prime}(t)=b(t)e^{-A(t)}\)</span></p><p>对 <span class="math inline">\(\displaystyle c^{\prime}(t)\)</span>积分得到</p><p><span class="math display">\[\displaystyle c(t)=\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 因此，一个特解是：</p><p><span class="math display">\[\displaystyle y_{p}(t)=e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 非齐次 ODE 的通解 <span class="math inline">\(\displaystyley(t)\)</span> 是齐次ODE的通解 <span class="math inline">\(\displaystyley_{h}(t)=Ce^{A(t)}\)</span> （其中<spanclass="math inline">\(C\)</span>是任意常数）与特解 <spanclass="math inline">\(\displaystyle y_{p}(t)\)</span> 的和：</p><p><span class="math display">\[\displaystyle y(t)=Ce^{A(t)}+e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 如果给定初始条件 <span class="math inline">\(\displaystyley(t_{0})=y_{0}\)</span>，则 <span class="math inline">\(\displaystyleCe^{A(t_{0})}=y_{0}\)</span> （因为根据积分限的选择， <spanclass="math inline">\(\displaystyle A(t_{0})=0\)</span> 且 <spanclass="math inline">\(\displaystyle y_{p}(t_{0})=0\)</span>）</p><p>所以 <span class="math inline">\(\displaystyleC=y_{0}\)</span>，得到：</p><p><span class="math display">\[\displaystyley(t)=y(t_{0})e^{A(t)}+e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 解的最大定义域是 <span class="math inline">\(\displaystylea(t)\)</span> 和 <span class="math inline">\(\displaystyle b(t)\)</span>定义域的交集</p><p>特解 <span class="math inline">\(\displaystyle y_{p}(t)\)</span>的另一种表示有时很有用： <span class="math display">\[\displaystyley_{p}(t)=\int_{t_{0}}^{t}b(s)e^{A(t)-A(s)}ds=\int_{t_{0}}^{t}G(s，t)b(s)ds\\\]</span> 其中<span class="math display">\[\displaystyleG(s，t)=\exp\left(\int_{s}^{t}a(\tau)d\tau\right)\\\]</span></p><h3 id="积分因子integrating-factors">积分因子（IntegratingFactors）</h3><p>解 <span class="math inline">\(\displaystyley^{\prime}=a(t)y+b(t)\)</span> 的另一种方法是使用积分因子</p><p>将ODE改写为</p><p><span class="math display">\[\displaystyle y^{\prime}-a(t)y=b(t)\\\]</span> 两边乘以一个函数 <span class="math inline">\(\displaystylem(t)\)</span>，即积分因子：</p><p><span class="math display">\[\displaystyle m(t)y^{\prime}-m(t)a(t)y=m(t)b(t)\\\]</span> 我们希望左边是乘积的导数，具体来说是 <spanclass="math inline">\(\displaystyle\frac{d}{dt}(m(t)y(t))=m(t)y^{\prime}+m^{\prime}(t)y\)</span></p><p>比较 <span class="math inline">\(\displaystylem(t)y^{\prime}-m(t)a(t)y\)</span> 和 <spanclass="math inline">\(\displaystylem(t)y^{\prime}+m^{\prime}(t)y，\)</span> 我们需要 <spanclass="math inline">\(\displaystyle m^{\prime}(t)=-a(t)m(t)\)</span></p><p>这是一个关于 <span class="math inline">\(\displaystyle m(t)\)</span>的可分离ODE：</p><p><span class="math display">\[\displaystyle \frac{dm}{m}=-a(t)dt\\\]</span> 积分得到</p><p><span class="math display">\[\displaystyle \ln|m(t)|=-\int a(t)dt=-A(t)\\\]</span> 所以 <span class="math inline">\(\displaystylem(t)=e^{-A(t)}\)</span> （我们可以选择积分常数为1）</p><p>有了这个 <span class="math inline">\(\displaystylem(t)\)</span>，方程变为：</p><p><span class="math display">\[\displaystyle \frac{d}{dt}(e^{-A(t)}y(t))=e^{-A(t)}b(t)\\\]</span> 两边积分：</p><p><span class="math display">\[\displaystyle e^{-A(t)}y(t)=\int e^{-A(t)}b(t)dt+C\\\]</span></p><p><span class="math display">\[\displaystyle y(t)=e^{A(t)}\left(\int e^{-A(t)}b(t)dt+C\right)\\\]</span></p><p>这与通过参数变易法得到的通解相同</p><h2 id="复系数一阶线性ode">复系数一阶线性ODE</h2><p>显式复系数一阶线性 ODE具有以下形式：</p><p><span class="math display">\[\displaystyle z^{\prime}(t)=a(t)z(t)+b(t)\\\]</span> <span class="math inline">\(\displaystyle z(t)\)</span>是实变量的复值函数，也就是说 <span class="math inline">\(\displaystylez(t)=x(t)+iy(t)\)</span></p><p><span class="math inline">\(\displaystyle a(t)\)</span>，<spanclass="math inline">\(b(t)\)</span> 是复值函数 <spanclass="math inline">\(\displaystyle D\rightarrow\mathbb{C}，\)</span> 令<span class="math inline">\(\displaystylea(t)=a_{1}(t)+ia_{2}(t)\)</span> 和 <spanclass="math inline">\(\displaystyle b(t)=b_{1}(t)+ib_{2}(t)\)</span></p><p>将这些代入 ODE 得到：</p><p><span class="math display">\[\displaystylex^{\prime}(t)+iy^{\prime}(t)=(a_{1}(t)+ia_{2}(t))(x(t)+iy(t))+(b_{1}(t)+ib_{2}(t))\\\]</span></p><p><span class="math display">\[\displaystylex^{\prime}(t)+iy^{\prime}(t)=(a_{1}x-a_{2}y+b_{1})+i(a_{2}x+a_{1}y+b_{2})\\\]</span></p><p>令实部和虚部分别相等，得到一个由两个实系数一阶线性ODE组成的系统：</p><p><span class="math display">\[\displaystyle x^{\prime}(t)=a_{1}(t)x(t)-a_{2}(t)y(t)+b_{1}(t)\\\]</span></p><p><span class="math display">\[\displaystyle y^{\prime}(t)=a_{2}(t)x(t)+a_{1}(t)y(t)+b_{2}(t)\\\]</span></p><p>矩阵形式为：</p><p><span class="math display">\[\displaystyle \begin{pmatrix}x^{\prime}(t)\\y^{\prime}(t)\end{pmatrix}=\begin{pmatrix}a_{1}(t)&amp;-a_{2}(t)\\a_{2}(t)&amp;a_{1}(t)\end{pmatrix}\begin{pmatrix}x(t)\\y(t)\end{pmatrix}+\begin{pmatrix}b_{1}(t)\\ b_{2}(t)\end{pmatrix}\\\]</span></p><h3 id="通解">通解</h3><p>求解方法与实数情况类似</p><p>通解为：</p><p><span class="math display">\[\displaystyle z(t)=Z_{0}e^{A(t)}+z_{p}(t)\\\]</span> 其中 <span class="math inline">\(\displaystyleZ_{0}\in\mathbb{C}\)</span> 是任意复常数，并且 <spanclass="math inline">\(\displaystyle A(t)=\int_{t_{0}}^{t}a(s)ds\)</span>（这现在是一个复值积分），此外：</p><p><span class="math display">\[\displaystyle z_{p}(t)=e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 证明依赖于实变量复值函数的微分和积分是逐分量进行的，并且 <spanclass="math inline">\(\displaystyle\frac{d}{dt}e^{A(t)}=A^{\prime}(t)e^{A(t)}\)</span> 对于复值 <spanclass="math inline">\(\displaystyle A(t)\)</span> 也成立</p><p>对于 IVP <span class="math inline">\(\displaystylez(t_{0})=z_{0}，\)</span> 唯一解是 <spanclass="math inline">\(\displaystyle z(t)=z_{0}e^{A(t)}+z_{p}(t)\)</span>（由于选择 <span class="math inline">\(\displaystyle t_{0}\)</span>作为积分下限， <span class="math inline">\(\displaystyle A(t_{0})=0，z_{p}(t_{0})=0\)</span>）</p><h3 id="实系数-ode-的复化complexification">实系数 ODE的复化（Complexification）</h3><p>为了解实系数 ODE</p><p><span class="math display">\[\displaystyle y^{\prime}(t)=a(t)y(t)+b(t)\\\]</span> 其中 <span class="math inline">\(\displaystyle a(t)\)</span>是实数，如果 <span class="math inline">\(\displaystyle b(t)\)</span>例如是 <span class="math inline">\(\displaystyle \sin(\omegat)\)</span>，我们可以使用<strong>复化（Complexification）</strong>方法</p><p>令 <span class="math inline">\(\displaystyleb(t)=\text{Im}(B(t))\)</span>，其中 <spanclass="math inline">\(\displaystyle B(t)\)</span>是一个复函数(例如，如果 <span class="math inline">\(\displaystyleb(t)=\sin(\omega t)\)</span> 可以选择 <spanclass="math inline">\(\displaystyle B(t)=e^{i\omega t}\)</span>，因为<span class="math inline">\(\displaystyle \text{Im}(e^{i\omegat})=\sin(\omega t))\)</span></p><p>解复系数 ODE</p><p><span class="math display">\[\displaystyle z^{\prime}(t)=a(t)z(t)+B(t)\\\]</span> 那么 <span class="math inline">\(\displaystyley(t)=\text{Im}(z(t))\)</span> 将是原始实系数 ODE 的一个解</p><p>类似地，如果 <span class="math inline">\(\displaystyleb(t)=\text{Re}(B(t))\)</span>，那么 <spanclass="math inline">\(\displaystyle y(t)=\text{Re}(z(t))\)</span>是解</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程、初值问题与方向场</title>
    <link href="/2025/05/16/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E3%80%81%E5%88%9D%E5%80%BC%E9%97%AE%E9%A2%98%E4%B8%8E%E6%96%B9%E5%90%91%E5%9C%BA/"/>
    <url>/2025/05/16/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E3%80%81%E5%88%9D%E5%80%BC%E9%97%AE%E9%A2%98%E4%B8%8E%E6%96%B9%E5%90%91%E5%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="常微分方程初值问题与方向场">常微分方程、初值问题与方向场</h1><h2 id="常微分方程-ordinary-differential-equation-ode">常微分方程(Ordinary Differential Equation, ODE)</h2><p><strong>常微分方程 (Ordinary Differential Equation, ODE)</strong>是一个包含单变量函数 <span class="math inline">\(f(t)\)</span> 及其导数<span class="math inline">\(f&#39;(t)\)</span>、<spanclass="math inline">\(f&#39;&#39;(t)\)</span> 等的方程。这与<strong>偏微分方程 (Partial Differential Equation, PDE)</strong>不同，后者涉及多变量函数及其偏导数</p><p>一个<span class="math inline">\(n\)</span>阶ODE具有以下形式： <spanclass="math display">\[F(t,y,y&#39;,y&#39;&#39;,...,y^{(n)})=0\]</span> 其中 <span class="math inline">\(F\)</span> 的定义域为 <spanclass="math display">\[D\subseteq\mathbb{R}\times\underbrace{\mathbb{R}^{m}\times\cdot\cdot\cdot\times\mathbb{R}^{m}}_{n+1}\]</span> 并且依赖于最后一个变量<spanclass="math inline">\(y^{(n)}\)</span>（否则阶数会小于n）</p><p>该方程的解是一个函数<spanclass="math inline">\(f:I\rightarrow\mathbb{R}^{m}\)</span>，定义在一个长度大于<spanclass="math inline">\(0\)</span>的区间 <spanclass="math inline">\(I\subseteq\mathbb{R}\)</span>上，该函数n次可微，并且对于所有 <span class="math inline">\(t\inI\)</span> 满足 <spanclass="math inline">\(F(t,f(t),f&#39;(t),f&#39;&#39;(t),...,f^{(n)}(t))=0\)</span></p><h2 id="初值问题-initial-value-problem-ivp">初值问题 (Initial ValueProblem, IVP)</h2><p>给定一个如上所述的ODE，并且 <span class="math display">\[t_{0}\in\mathbb{R},y_{0},...,y_{n}\in\mathbb{R}^{m}\]</span> 使得 <span class="math display">\[F(t_{0},y_{0},...,y_{n})=0\]</span> <strong>初值问题 (Initial Value Problem, IVP)</strong> 的解：<span class="math display">\[F(t,y,y&#39;,y&#39;&#39;,...,y^{(n)})=0, y^{(i)}(t_{0})=y_{i} \text{ for} 0\le i\le n\]</span> 是指定义域为<spanclass="math inline">\(I\)</span>的任何函数<spanclass="math inline">\(f:I\rightarrow\mathbb{R}^{m}\)</span>，它解决了上述ODE问题，并满足<span class="math display">\[f(t_{0})=y_{0},f&#39;(t_{0})=y_{1},...,f^{(n)}(t_{0})=y_{n}\]</span></p><h3 id="显式形式explicit-form">显式形式（Explicit Form）</h3><p>虽然以上ODE和IVP的定义是最通用的，但以下<spanclass="math inline">\(n\)</span>阶ODE的显式形式 (explicit form)最为常见： <span class="math display">\[y^{(n)}=G(t,y,y&#39;,...,y^{(n-1)})\]</span> 显式形式的IVP仅需指定 <span class="math display">\[y^{(i)}(t_{0})=y_{i}\text{ for } 0\le i\le n-1\]</span></p><h3 id="最大解maximal-solution">最大解（Maximal Solution）</h3><p>有时很容易找到给定ODE的解（或解族），问题是是否存在其他解</p><p>由于将解 <spanclass="math inline">\(y:I\rightarrow\mathbb{R}\)</span>限制到子区间<span class="math inline">\(J\subset I\)</span>会产生另一个解，因此我们只对<strong>最大解 (maximalsolutions)</strong>感兴趣，即那些不是通过从另一个解进行限制而产生的解</p><h2 id="方向场-direction-fields">方向场 (Direction fields)</h2><p>对于一阶ODE： <span class="math display">\[y&#39;=f(t,y)\]</span> 求解它相当于找到一个定义在某个区间<spanclass="math inline">\(I\subseteq\mathbb{R}\)</span> 上的函数<spanclass="math inline">\(y=y(t)\)</span>，并且对于所有的 <spanclass="math inline">\(t\in I\)</span>，满足以下条件：</p><ol type="1"><li>点<span class="math inline">\((t,y(t))\)</span>在 <spanclass="math inline">\(f\)</span> 的定义域内</li><li>在点 <span class="math inline">\((t,y(t))\)</span> 处， <spanclass="math inline">\(y\)</span> 的斜率等于 <spanclass="math inline">\(f(t,y(t))\)</span></li></ol><p><span class="math inline">\(y\)</span>的斜率我们可以用切线方向代替，而点 <spanclass="math inline">\((t,y(t))\)</span> 处图形的切线方向又可以用向量<span class="math inline">\((1,f(t,y(t)))\)</span> 图形化表示</p><p>因此，我们可以在采样点 <span class="math inline">\((t,y)\inD\)</span> 处绘制一个小向量<spanclass="math inline">\((1,f(t,y))\)</span> (或此向量的正倍数)</p><p>这些向量被称为 <span class="math inline">\(y&#39;=f(t,y)\)</span>的<strong>斜率场 (slope field)</strong> 或<strong>方向场 (directionfield)</strong></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>败家日记</title>
    <link href="/2025/03/17/%E8%B4%A5%E5%AE%B6%E6%97%A5%E8%AE%B0/"/>
    <url>/2025/03/17/%E8%B4%A5%E5%AE%B6%E6%97%A5%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<div id="main" style="width: 100%; height: 400px;"></div><script>  // 读取 total_position.json 数据  fetch('/total_position.json')  // Hexo 会把 `source/total_position.json` 复制到 `public/`    .then(response => response.json())    .then(data => {      // 解析 JSON 数据      let xAxisData = []; // 存放时间      let yAxisData = []; // 存放资产值      data.forEach(entry => {        entry.data.forEach(point => {          let timestamp = point[0];          let assetValue = parseFloat(point[1]); // 转换为数值              xAxisData.push(new Date(timestamp).toLocaleString()); // 转换为可读时间          yAxisData.push(assetValue);        });      });          // 初始化 ECharts      var myChart = echarts.init(document.getElementById('main'));          // ECharts 配置      var option = {        title: { text: '总资产折线图' },        tooltip: { trigger: 'axis' },        xAxis: {          type: 'category',          data: xAxisData,          axisLabel: { rotate: 30 }  // 避免标签重叠        },        yAxis: { type: 'value' },        series: [{          name: '总资产',          type: 'line',          data: yAxisData        }]      };          myChart.setOption(option);    })    .catch(error => console.error('加载 JSON 数据失败:', error));</script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>最大似然学习 Maximum Likelihood Learning</title>
    <link href="/2024/11/22/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/11/22/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="learning的目标">learning的目标</h2><p>假设我们已知数据集<spanclass="math inline">\(\mathcal{D}\)</span>为<spanclass="math inline">\(m\)</span>个从<spanclass="math inline">\(P_{\mathrm{data}}\)</span>中获得的采样构成的集合，同时还已知一些模型<spanclass="math inline">\(P_\theta\)</span>构成的集合<spanclass="math inline">\(\mathcal{M}\)</span></p><p>那么learning的目标就是利用<span class="math inline">\(\mathcalD\)</span>中的元素，在<span class="math inline">\(\mathcalM\)</span>中找到与<spanclass="math inline">\(P_{\mathrm{data}}\)</span>最为相似的<spanclass="math inline">\(P_\theta\)</span></p><h2 id="最相似">”最相似“</h2><p>我们需要一些指标来表征<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>的相似程度</p><p>一旦明确了该指标，求得该指标取极值时的<spanclass="math inline">\(P_\theta\)</span>即可</p><h3 id="相对熵-kullback-leibler-divergence">相对熵 Kullback-Leiblerdivergence</h3><p><span class="math display">\[D(p||q)=\sum_\mathbf x p(\mathbf x)\log\frac{p(\mathbf x)}{q(\mathbf x)}\]</span></p><p>相对熵的含义是，<spanclass="math inline">\(q\)</span>这种方案相对于<spanclass="math inline">\(p\)</span>需要的额外的数据的平均值</p><p>相对熵始终非负，当且仅当<spanclass="math inline">\(p=q\)</span>时相对熵为<spanclass="math inline">\(0\)</span>，因为： <span class="math display">\[D(p||q)=\mathbf E_{\mathbf x\sim p}\left[-\log\frac{q(\mathbfx)}{p(\mathbf x)}\right]\geq-\log\mathbf E_{\mathbf x\sim p}\left[\frac{q(\mathbf x)}{p(\mathbfx)}\right]=-\log\left(\sum_\mathbf x p(\mathbf x)\frac{q(\mathbf x)}{p(\mathbfx)}\right)=0\]</span> 我们可以选择用相对熵来表示<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>的相似程度： <spanclass="math display">\[D(P_{\mathrm{data}}||P_\theta)=\mathbf E_{\mathbf x\simP_{\mathrm{data}}}\left[\log\frac{P_{\mathrm{data}}}{P_\theta(\mathbfx)}\right]=\sum_\mathbf x P_{\mathrm{data}}(\mathbfx)\log\frac{P_{\mathrm{data}}(\mathbf x)}{P_\theta(\mathbf x)}\]</span> 当且仅当<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>完全相同时，相对熵为0</p><h3 id="最大似然-maximum-likelihood">最大似然 Maximum likelihood</h3><p>我们的目标是得到当相对熵取最小值时的<spanclass="math inline">\(P_\theta\)</span>，但是问题是我们并不知道<spanclass="math inline">\(P_{\mathrm{data}}\)</span></p><p>我们可以将相对熵的表达式简单变形： <span class="math display">\[\begin{align}D(P_{\mathrm{data}}||P_\theta)&amp;=\mathbf E_{\mathbf x\simP_{\mathrm{data}}}\left[\log\frac{P_{\mathrm{data}}}{P_\theta(\mathbfx)}\right]\\&amp;=\mathbf E_{\mathbf x\sim P_{\mathrm{data}}}[\logP_{\mathrm{data}}(\mathbf x)]-\mathbf E_{\mathbf x\simP_{\mathrm{data}}}[\log P_{\theta}(\mathbf x)]\end{align}\]</span> 显然<span class="math inline">\(E_{\mathbf x\simP_{\mathrm{data}}}[\logP_{\mathrm{data}}]\)</span>是个常数，那么相对熵的大小只取决于后面一项，即：<span class="math display">\[\arg\min_{P_\theta}D(P_{\mathrm{data}}||P_\theta) = \arg\min_{P_\theta}-\mathbf E_{\mathbf x\sim P_{\mathrm{data}}}[\logP_{\theta}(\mathbf x)]=\arg \max_{P_\theta}\mathbf E_{\mathbf x\simP_{\mathrm{data}}}[\log P_{\theta}(\mathbf x)]\]</span> 此时用empirical log-likelihood作近似： <spanclass="math display">\[\mathbf E_{\mathcal D}[\log P_\theta(\mathbf x)] = \frac1{|\mathcalD|}\sum_{\mathbf x\in \mathcal D}\log P_\theta(\mathbf x)\]</span> 所以，求最大似然时得到的<spanclass="math inline">\(P_\theta\)</span>与求最小相对熵得到的<spanclass="math inline">\(P_\theta\)</span>是相同的，因此我们可以使用似然来表征<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>的相似程度</p><p>最大似然学习就是指： <span class="math display">\[\max_{P_\theta}\frac1{|\mathcal D|}\sum_{\mathbf x\in \mathcal D}\logP_\theta(\mathbf x)\]</span></p><h2 id="例子自回归模型">例子：自回归模型</h2><p>已知一个有<span class="math inline">\(n\)</span>个变量的自回归模型：<span class="math display">\[P_\theta(\mathbf x) = \prod_{i=1}^np_{\mathrm{neural}}(x_i|\mathbfx_{&lt;i};\mathbf\theta_i)\]</span> 其中<spanclass="math inline">\(\mathbf\theta=(\theta_1,...,\theta_n)\)</span>为所有条件下的参数，训练数据集为<spanclass="math inline">\(\mathcal D=\{\mathbf x^{(1)},...,\mathbfx^{(m)}\}\)</span></p><p>此时，它的似然函数就是： <span class="math display">\[L(\theta,\mathcal D) = \prod_{j=1}^mP_\theta(\mathbf x^{(j)}) =\prod_{j=1}^m\prod_{i=1}^np_{\mathrm{neural}}(x_i^{(j)}|\mathbfx^{(j)}_{&lt;i};\mathbf\theta_i)\]</span> 我们的目的就是求出<span class="math inline">\(\arg \max_\thetaL(\theta, \mathcal D)=\arg \max_\theta \log L(\theta, \mathcalD)\)</span></p><p>为方便计算，一般使用对数似然： <span class="math display">\[\ell(\theta,\mathcal D) = \log L(\theta, \mathcal D)=\sum_{j=1}^m\sum_{i=1}^n\log p_{\mathrm{neural}}(x_i^{(j)}|\mathbfx^{(j)}_{&lt;i};\mathbf\theta_i)\]</span> 然后使用梯度下降等优化方法即可</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>自回归模型 Autoregressive Model</title>
    <link href="/2024/11/13/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/11/13/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="基于分类模型的神经网络">基于分类模型的神经网络</h2><p>对于一个带标签的二元数据集<span class="math inline">\((X,Y)\)</span>，<span class="math inline">\(X\in\{\mathbf{x}\}^n\)</span>，<span class="math inline">\(Y\in\{0,1\}^n\)</span>，分类模型关心的是<spanclass="math inline">\(p(Y|\mathbf{x})\)</span>的分布</p><p>虽然这个概率分布是任意的，但是我们假设该分布是由参数<spanclass="math inline">\(\mathbf{\alpha}\)</span>控制的，即 <spanclass="math display">\[p(Y=1\,|\,\mathbf{x};\mathbf{\alpha})=f(\mathbf{x},\mathbf{\alpha})\]</span></p><h3 id="逻辑回归-logistic-regression">逻辑回归 Logistic Regression</h3><p>对于二元分类，我们可以利用sigmoid函数<spanclass="math inline">\(\sigma(z) =\frac{1}{1+e^{-z}}\)</span>进行回归分析</p><p>由于sigmoid函数是单变量函数，我们先利用参数<spanclass="math inline">\(\mathbf{\alpha}\)</span>将数据的特征向量<spanclass="math inline">\(\mathbf{x}\)</span>映射<spanclass="math inline">\(\mathbb{R}\)</span>中： <spanclass="math display">\[z(\mathbf{\alpha},\mathbf{x})=\alpha_0+\sum_{i=1}^n\alpha_ix_i\]</span> 此时再进行回归分析： <span class="math display">\[p_{\mathrm{logit}}(Y=1\,|\,\mathbf{x};\mathbf\alpha)=\sigma(z(\mathbf\alpha,\mathbfx))\]</span></p><h3 id="非线性相关性">非线性相关性</h3><p>为了使分析更加灵活，我们可以先将输入的特征向量分解成中间向量，再对中间向量进行回归分析</p><p>显然分解输入的特征向量应为非线性的变换，将该变换过程记为<spanclass="math inline">\(f:\mathbb{R}^n \to\mathbb{R}^m, \mathbf x\mapsto\mathbf h(A, \mathbf b, \mathbf x)\)</span></p><p>那么神经网络的基本公式就是 <span class="math display">\[p_{\mathrm{Neural}}(Y=1\,|\,\mathbf x; \mathbf \alpha, A, \mathbf b) =f(\alpha_0+\sum_{i=1}^m\alpha_ih_i)\]</span></p><h2 id="自回归模型-autoregressive-model">自回归模型 AutoregressiveModel</h2><p>为了得到<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布，首先人为选定一个顺序使用链式法则将它展开<span class="math display">\[p(x_1,...,x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1, x_2)...p(x_n|x_1,..,x_{n-1})\]</span> 第一个概率我们直接从条件概率表（CPT）查得</p><p>剩余的条件概率我们假设是一个与参数<spanclass="math inline">\(\alpha\)</span>有关的函数： <spanclass="math display">\[p(x_k|x_1,...,x_{k-1}) = p(x_k|x_1,...,x_{k-1};\mathbf\alpha^k)\]</span> 那么我们就得到了<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布： <spanclass="math display">\[p(x_1,...,x_n)=p_{\mathrm{CPT}}(x_1;\alpha^1)p(x_2|x_1;\mathbf\alpha^2)p(x_3|x_1,x_2;\mathbf\alpha^3)...p(x_n|x_1,..,x_{n-1};\mathbf\alpha^n)\]</span>我们对于剩余的条件概率进行的模型假设，被称为<strong>自回归模型（AutoregressiveModel）</strong></p><h3id="完全可见的sigmoid置信网络-fully-visible-sigmoid-belief-network-fvsbn">完全可见的sigmoid置信网络Fully Visible Sigmoid Belief Network (FVSBN)</h3><p>在自回归模型中，当每个随机变量都是二元变量时，我们使用sigmoid函数进行逻辑回归来得到这个与参数<spanclass="math inline">\(\alpha\)</span>有关的函数，这种模型就被称为<strong>完全可见的sigmoid置信网络（FVSBN）</strong>，即：<span class="math display">\[p(x_k|x_1,...,x_{k-1}) =p_{\mathrm{logit}}(x_k|x_1,...,x_{k-1};\mathbf\alpha^k) =\sigma(\alpha^k_0+\sum_{i=1}^k\alpha_i^kx_i)\]</span> 那么我们就得到了<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布： <spanclass="math display">\[p(x_1,...,x_n)=p_{\mathrm{CPT}}(x_1;\alpha^1)p_{\mathrm{logit}}(x_2|x_1;\mathbf\alpha^2)p_{\mathrm{logit}}(x_3|x_1,x_2;\mathbf\alpha^3)...p_{\mathrm{logit}}(x_n|x_1,..,x_{n-1};\mathbf\alpha^n)\]</span></p><h3id="神经自回归密度估计-neural-autoregressive-density-estimation-nade">神经自回归密度估计Neural Autoregressive Density Estimation (NADE)</h3><p>为了提升模型的效果，我们使用单层的神经网络来替代FVSBN中的逻辑回归，这种模型就是<strong>神经自回归密度估计（NADE）</strong>，即：<span class="math display">\[\mathbf h_k=\sigma(A_k\mathbf x_{&lt;k}+\mathbf c_k)\]</span> <span class="math display">\[p(x_k|x_1,...,x_{k-1})=p_{\mathrm{Neural}}(x_k|x_1,...,x_{k-1};A_k,\mathbf c_k,\alpha_k,\mathbf b_k)=\sigma(\alpha_k\mathbf h_k+\mathbf b_k)\]</span></p><p>为了减少参数数量并且加速计算，在计算当前时间步的权重时，可以加入先前的权重</p><p>也就是说，假设最后一个权重<spanclass="math inline">\(A_n\)</span>为<spanclass="math inline">\(W\)</span>，那么<spanclass="math inline">\(A_k\)</span>就变为<spanclass="math inline">\(W_{.,&lt;k}\)</span></p><h3 id="rnade">RNADE</h3><p>对于非二元的离散型随机变量<span class="math inline">\(X_i\in\{1,...,K\}\)</span>，条件概率可以用categoricaldistribution来表示，并用softmax函数替代sigmoid，即： <spanclass="math display">\[\mathbf h_i = \sigma(W_{.,&lt;i}\mathbf x_{&lt;i}+\mathbf c)\]</span> <span class="math display">\[p(x_i|x_1,...,x_{i-1}) =\mathrm{Cat}(p_i^1,...,p_i^K)=\mathrm{Cat}(\mathrm{softmax}(A_i\mathbfh_i+\mathbf b_i))\]</span></p><p>对于连续型随机变量<spanclass="math inline">\(X_i\in\mathbb{R}\)</span>，条件概率用参数化的连续型分布来表示即可，例如我们可以用<spanclass="math inline">\(K\)</span>个混合高斯模型： <spanclass="math display">\[p(x_i|x_1,...,x_{i-1}) =\sum_{j=1}^K\frac1K\mathcalN(x_i;\mu_i^j,\sigma_i^j)\]</span></p><p>其中获取参数<spanclass="math inline">\(\mu_i^j,\sigma_i^j\)</span>与NADE的方法相同：<span class="math display">\[h_i=\sigma(W_{.,&lt;i}\mathbf x_{&lt;i}+\mathbf c)\]</span></p><p><span class="math display">\[(\mu_i^1,...,\mu_i^K,\sigma_i^1,...,\sigma_i^K)=f(\mathbf h_i)\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>统计生成模型 Statistical Generative Model</title>
    <link href="/2024/11/10/%E7%BB%9F%E8%AE%A1%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/11/10/%E7%BB%9F%E8%AE%A1%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="统计生成模型-statistical-generative-model">统计生成模型Statistical Generative Model</h2><p><strong>统计生成模型（Statistical GenerativeModel）</strong>就是将生成模型视为一个概率分布<spanclass="math inline">\(p(x)\)</span></p><p>相应的，数据就是对<span class="math inline">\(p(x)\)</span>采样的结果，</p><p>先验知识就是参数形式、损失函数、优化算法等等</p><p>如果我们用<span class="math inline">\(X\)</span>表示图像，<spanclass="math inline">\(Y\)</span>表示标签，discriminative model的目标是得到<span class="math inline">\(P(Y|X)\)</span>，而generativemodel则是为了得到<span class="math inline">\(P(X,Y)\)</span></p><h2 id="概率分布的表示形式">概率分布的表示形式</h2><p>我们希望得到<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布，直接的方法是列举所有的情形，然而代价过于昂贵</p><p>例如，假设<span class="math inline">\(x_i\sim Ber(p_i)\)</span>，<span class="math inline">\(1\leq i \leqn\)</span>，那么我们就需要<spanclass="math inline">\((2^n-1)\)</span>个参数来表示<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布</p><p>所以我们可以采用其他方法</p><h3 id="链式法则-chain-rule">链式法则 Chain Rule</h3><p><span class="math display">\[p(x_1,...,x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1, x_2)...p(x_n|x_1,..,x_{n-1})\]</span></p><p>但是使用了链式法则不能减少参数数量，我们依然需要<spanclass="math inline">\((2^n-1)\)</span>个参数</p><h3 id="贝叶斯网络-bayesian-network">贝叶斯网络 Bayesian Network</h3><p>我们可以尝试进行一些独立性假设，例如<spanclass="math inline">\(X_i\)</span>与<span class="math inline">\(X_{&lt;i-1}\)</span>均两两独立，那么 <span class="math display">\[\begin{align}p(x_1,...,x_n) &amp;= p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)...p(x_n|x_1,..,x_{n-1})\\&amp;=p(x_1)p(x_2|x_1)p(x_3|x_2)...p(x_n|x_{n-1})\end{align}\]</span> 此时我们仅需(2n-1)个参数</p><p>由此可见，独立性假设可以帮助我们减少参数：对于每个随机变量<spanclass="math inline">\(X_i\)</span>，假设其与集合<spanclass="math inline">\(\mathbf{X_{A_i}}\)</span>中的随机变量有关，那么，<span class="math display">\[\begin{align}p(x_1,...,x_n) &amp;= p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)...p(x_n|x_1,..,x_{n-1})\\&amp;=\prod_i p(x_i|\mathbf{x_{A_i}})\end{align}\]</span> 变量之间复杂的独立性关系可以用一个<strong>有向无环图（DirectedAcyclic Graph, DAG）</strong>表示，</p><p>对于一个有向无环图<spanclass="math inline">\(G=(V,E)\)</span>，如果：</p><ol type="1"><li><p>节点<span class="math inline">\(i\in V\)</span>表示随机变量<spanclass="math inline">\(X_i\)</span></p></li><li><p>每个随机变量只与其父节点的随机变量有关，即<spanclass="math inline">\(p(x_i|\mathbf{x_{A_i}})=p(x_i|\mathbf{x_{Pa(i)}})\)</span></p></li></ol><p>这样的有向无环图<spanclass="math inline">\(G\)</span>就被称为<strong>贝叶斯网络（BayesianNetwork）</strong></p><p>在贝叶斯网络的基础上， <span class="math display">\[p(x_1,...,x_n)=\prod_{i\in V}p(x_i|\mathbf{x_{Pa(i)}})\]</span></p><h3 id="神经网络模型-neural-models">神经网络模型 Neural Models</h3><p>因为足够深的神经网络可以拟合任意的函数，所以我们没有必要研究变量之间的独立性关系，我们可以在链式法则展开后直接利用神经网络进行函数的拟合，即：<span class="math display">\[p(x_1,...,x_n) \approx p(x_1)p(x_2|x_1)p_{\mathrm{Neural}}(x_3|x_1,x_2)...p_{\mathrm{Neural}}(x_n|x_1,..,x_{n-1})\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>IPv4与IPv6环境下Shadowsocks的搭建与使用</title>
    <link href="/2024/09/07/IPv4%E4%B8%8EIPv6%E7%8E%AF%E5%A2%83%E4%B8%8BShadowsocks%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2024/09/07/IPv4%E4%B8%8EIPv6%E7%8E%AF%E5%A2%83%E4%B8%8BShadowsocks%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Ubuntu版本 22.04</p></blockquote><h2id="ipv4环境下安装并配置shadowsocks">IPv4环境下安装并配置Shadowsocks</h2><h3 id="客户端的安装与配置">客户端的安装与配置</h3><p>安装Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get update<br>sudo pip install shadowsocks<br></code></pre></td></tr></table></figure><p>创建并编辑配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">touch</span> /etc/shadowsocks.json<br>sudo vim /etc/shadowsocks.json<br></code></pre></td></tr></table></figure><p>配置文件的格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;server&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;服务器的IPv4地址&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;server_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">8388</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_address&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;127.0.0.1&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1080</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;password&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;密码&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;timeout&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">300</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;method&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;aes-256-gcm&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;fast_open&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>启动Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo sslocal -c /etc/shadowsocks.json -d start<br></code></pre></td></tr></table></figure><p>关闭Shadowsocks时，在终端内输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo lsof -i:1080<br></code></pre></td></tr></table></figure><p>kill相应的pid即可</p><p>如果在启动Shadowsocks时出现如下报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">Traceback (most recent call last):<br>  File <span class="hljs-string">&quot;/usr/local/bin/sslocal&quot;</span>, line 5, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    from shadowsocks.local import main<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.10/dist-packages/shadowsocks/local.py&quot;</span>, line 27, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    from shadowsocks import shell, daemon, eventloop, tcprelay, udprelay, asyncdns<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.10/dist-packages/shadowsocks/udprelay.py&quot;</span>, line 71, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    from shadowsocks import encrypt, eventloop, lru_cache, common, shell<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.10/dist-packages/shadowsocks/lru_cache.py&quot;</span>, line 34, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    class LRUCache(collections.MutableMapping):<br>AttributeError: module <span class="hljs-string">&#x27;collections&#x27;</span> has no attribute <span class="hljs-string">&#x27;MutableMapping&#x27;</span><br></code></pre></td></tr></table></figure><p>打开文件<code>/usr/local/lib/python3.10/dist-packages/shadowsocks/lru_cache.py</code>，将第34行的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LRUCache</span>(collections.MutableMapping):<br></code></pre></td></tr></table></figure><p>修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LRUCache</span>(collections.abc.MutableMapping):<br></code></pre></td></tr></table></figure><p>即可</p><h3 id="服务器的安装与配置">服务器的安装与配置</h3><p>安装Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get update<br>sudo pip install shadowsocks<br></code></pre></td></tr></table></figure><p>创建并编辑配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">touch</span> /etc/shadowsocks.json<br>sudo vim /etc/shadowsocks.json<br></code></pre></td></tr></table></figure><p>配置文件的格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;server&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;0.0.0.0&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;server_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">8388</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_address&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;127.0.0.1&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1080</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;password&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;密码&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;timeout&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">300</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;method&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;aes-256-gcm&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;fast_open&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>启动Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo ssserver -c /etc/shadowsocks.json -d start<br></code></pre></td></tr></table></figure><p>关闭Shadowsocks时，在终端内输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo lsof -i:8388<br></code></pre></td></tr></table></figure><p>kill相应的pid即可</p><h2id="在终端中利用privoxy使用shadowsocks">在终端中利用privoxy使用Shadowsocks</h2><p>在客户端的终端使用Shadowsocks时，需要安装privoxy</p><p>安装privoxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get update<br>sudo apt-get install privoxy<br></code></pre></td></tr></table></figure><p>编辑配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo vim /etc/privoxy/config<br></code></pre></td></tr></table></figure><p>确保文件中第794行有<code>listen-address  127.0.0.1:8118</code></p><p>在文件中第1483行添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">forward-socks5 / 127.0.0.1:1080 .<br></code></pre></td></tr></table></figure><p>注意后面有<code>.</code></p><p>添加环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件的末尾添加如下两行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> http_proxy=<span class="hljs-string">&quot;http://127.0.0.1:8118&quot;</span><br><span class="hljs-built_in">export</span> https_proxy=<span class="hljs-string">&quot;http://127.0.0.1:8118&quot;</span><br></code></pre></td></tr></table></figure><p>重新加载该文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>启动privoxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo service privoxy start<br>sudo service privoxy status<br></code></pre></td></tr></table></figure><p>此时可以<code>curl cip.cc</code>检查是否成功，成功后就可以使用Shadowsocks了</p><p>关闭privoxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo service privoxy stop<br></code></pre></td></tr></table></figure><h2 id="ipv6环境中使用">IPv6环境中使用</h2><h3 id="客户端修改为ipv6">客户端修改为IPv6</h3><h4 id="修改环境变量">修改环境变量</h4><p>把<code>~/.bashrc</code>最后两行修改为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> http_proxy=<span class="hljs-string">&quot;http://[::1]:8118&quot;</span><br><span class="hljs-built_in">export</span> https_proxy=<span class="hljs-string">&quot;http://[::1]:8118&quot;</span><br></code></pre></td></tr></table></figure><p>重新加载<code>~/.bashrc</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><h4 id="修改shadowsocks">修改Shadowsocks</h4><p>修改Shadowsocks配置文件<code>/etc/shadowsocks.json</code>的<code>"server"</code>和<code>"local_address"</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>    <span class="hljs-string">&quot;server&quot;</span>:<span class="hljs-string">&quot;服务器的IPv6地址&quot;</span>,<br>    <span class="hljs-string">&quot;server_port&quot;</span>:8388,<br>    <span class="hljs-string">&quot;local_address&quot;</span>: <span class="hljs-string">&quot;::1&quot;</span>,<br>    <span class="hljs-string">&quot;local_port&quot;</span>:1080,<br>    <span class="hljs-string">&quot;password&quot;</span>:<span class="hljs-string">&quot;密码&quot;</span>,<br>    <span class="hljs-string">&quot;timeout&quot;</span>:300,<br>    <span class="hljs-string">&quot;method&quot;</span>:<span class="hljs-string">&quot;aes-256-gcm&quot;</span>,<br>    <span class="hljs-string">&quot;fast_open&quot;</span>: <span class="hljs-literal">false</span><br>&#125;<br></code></pre></td></tr></table></figure><p>修改后重启Shadowsocks，此时终端中输入<code>sudo lsof -i:1080</code>，可以看到TYPE被改成了ipv6</p><h4 id="修改privoxy">修改privoxy</h4><p>修改<code>/etc/privoxy/config</code>：</p><p>确保795行有<code>listen-address [::1]:8118</code></p><p>将1372行修改为<code>forward-socks5 / [::1]:1080     .</code></p><p>重启privoxy：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo service privoxy restart<br>sudo service privoxy status<br></code></pre></td></tr></table></figure><h3 id="服务器修改为ipv6">服务器修改为IPv6</h3><p>修改Shadowsocks配置文件<code>/etc/shadowsocks.json</code>的<code>"server"</code>和<code>"local_address"</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>    <span class="hljs-string">&quot;server&quot;</span>:<span class="hljs-string">&quot;::&quot;</span>,<br>    <span class="hljs-string">&quot;server_port&quot;</span>:8388,<br>    <span class="hljs-string">&quot;local_address&quot;</span>: <span class="hljs-string">&quot;::1&quot;</span>,<br>    <span class="hljs-string">&quot;local_port&quot;</span>:1080,<br>    <span class="hljs-string">&quot;password&quot;</span>:<span class="hljs-string">&quot;密码&quot;</span>,<br>    <span class="hljs-string">&quot;timeout&quot;</span>:300,<br>    <span class="hljs-string">&quot;method&quot;</span>:<span class="hljs-string">&quot;aes-256-gcm&quot;</span>,<br>    <span class="hljs-string">&quot;fast_open&quot;</span>: <span class="hljs-literal">false</span><br>&#125;<br></code></pre></td></tr></table></figure><p>修改后重启Shadowsocks，此时终端中输入<code>sudo lsof -i:1080</code>，可以看到TYPE被改成了ipv6</p><h3 id="测试">测试</h3><p>客户端终端输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl 6.ipw.cn<br></code></pre></td></tr></table></figure><p>若返回IPv6地址则成功</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>反向传播 Backpropagation</title>
    <link href="/2024/07/23/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <url>/2024/07/23/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    
    <content type="html"><![CDATA[<h1 id="反向传播-backpropagation">5. 反向传播 Backpropagation</h1><h2 id="反向传播-backpropagation-1">5.1 反向传播 Backpropagation</h2><p>优化算法中，无一例外需要计算梯度，由于直接推导梯度的解析式过于繁杂，数值计算梯度又不够精确，所以我们选择<strong>反向传播（backpropagation）</strong>来计算梯度</p><p>将复杂的表达式表示为计算图（computationalgraph），不妨设其中一个节点为<spanclass="math inline">\(z=f(x)\)</span></p><p>Upstream gradient: <spanclass="math inline">\(\displaystyle\frac{\partial L}{\partialz}\)</span></p><p>Local gradient: <span class="math inline">\(\displaystyle\frac{\partial z}{\partial x}\)</span></p><p>那么由链式法则，downstream gradient 就是 <spanclass="math display">\[\frac{\partial L}{\partial x} = \frac{\partial z}{\partialx}\frac{\partial L}{\partial z}\]</span> 如此反复便能得到每一个参数的梯度</p><h2 id="向量求导-vector-derivatives">5.2 向量求导 VectorDerivatives</h2><p><span class="math display">\[x \in \mathbb R,y\in \mathbb R, \frac{\partial y}{\partial x}\in \mathbbR\]</span></p><p><span class="math display">\[x \in \mathbb R^N,y\in \mathbb R, \frac{\partial y}{\partial x}\in\mathbb R^N,\left(\frac{\partial y}{\partial x}\right)_n = \frac{\partialy}{\partial x_n}\]</span></p><p><span class="math display">\[x \in \mathbb R^N,y\in \mathbb R^M, \frac{\partial y}{\partial x}\in\mathbb R^{N\times M},\left(\frac{\partial y}{\partial x}\right)_{n,m} = \frac{\partialy_m}{\partial x_n}\]</span></p><h2 id="矩阵相乘的反向传播">5.3 矩阵相乘的反向传播</h2><p>在computational graph中，对于矩阵相乘的节点： <spanclass="math display">\[y=xw,x \in \mathbb R^{N \times D}, y \in \mathbb R^{N\times M}, w \in\mathbb R^{M\times D}\]</span>若直接进行反向传播计算，Jacobians过于庞大且稀疏，产生一些不必要的内存开销</p><p>事实上，如下公式会简化计算： <span class="math display">\[\frac{\partial L}{\partial x} = \left(\frac{\partial L}{\partialy}\right)w^{\mathsf T}\]</span> 以下是该公式的一些简单应用：</p><h3 id="线性分类器">线性分类器</h3><p><span class="math display">\[z=Wx+b\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Backpropagation of linear classifier</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: Upstream derivative, of shape (N, M)</span><br><span class="hljs-string">    - cache: Tuple of:</span><br><span class="hljs-string">      - x: Input data, of shape (N, D)</span><br><span class="hljs-string">      - w: Weights, of shape (D, M)</span><br><span class="hljs-string">      - b: Biases, of shape (M,)</span><br><span class="hljs-string">    Returns a tuple of:</span><br><span class="hljs-string">    - dx: Gradient with respect to x, of shape (N, D)</span><br><span class="hljs-string">    - dw: Gradient with respect to w, of shape (D, M)</span><br><span class="hljs-string">    - db: Gradient with respect to b, of shape (M,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, w, b = cache<br>    <br>    dx = dout.mm(w.t())<br>    dw = x.t().mm(dout)<br>    db = dout.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">return</span> dx, dw, db<br></code></pre></td></tr></table></figure><h3 id="relu">ReLU</h3><p><span class="math display">\[\mathrm{ReLU}(z) = \max (0,z)\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Backpropagation of ReLU function</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReLU</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: Upstream derivatives, of any shape</span><br><span class="hljs-string">    - cache: Input x, of same shape as dout</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: Gradient with respect to x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x = cache    <br>    dx = dout * (x &gt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> dx<br></code></pre></td></tr></table></figure><h3 id="dropout">Dropout</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Backpropagation of dropout</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">dropout</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: Upstream derivatives, of any shape</span><br><span class="hljs-string">    - cache: (dropout_param, mask) from Dropout.forward.</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: Gradient with respect to x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    mask = cache  <br>    dx = dout * mask<br>    <span class="hljs-keyword">return</span> dx<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>神经网络 Neural Networks</title>
    <link href="/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络-neural-networks">4. 神经网络 Neural Networks</h1><h2 id="神经网络-neural-network">4.1 神经网络 Neural Network</h2><p>由于两个线性分类器的直接叠加等效于一个线性分类器（忽略bias）： <spanclass="math display">\[f=W_2W_1x=W^*x\]</span>我们在两个线性分类器中间添加一个非线性函数，就构成了一个两层的<strong>神经网络（NeuralNetwork）</strong>： <span class="math display">\[f=W_2\max(0, W_1x)\]</span></p><h2 id="激活函数-activation-function">4.2 激活函数 Activationfunction</h2><p>神经网络中的非线性函数被称为<strong>激活函数（Activationfunction）</strong></p><p>以下是一些常见的激活函数</p><p>ReLU(Rectified Linear Unit): <span class="math display">\[\mathrm{ReLU}(z) = \max (0,z)\]</span> Leaky ReLU: <span class="math display">\[\max(0.1x,x)\]</span></p><p>Sigmoid: <span class="math display">\[\sigma(x) = \frac{1}{1+e^{-x}}\]</span> tanh: <span class="math display">\[\tanh(x)\]</span> Maxout: <span class="math display">\[max(w_1^{\mathsf T}x+b_1, w_2^{\mathsf T}x+b_2)\]</span> ELU: <span class="math display">\[\begin{cases}x&amp; x\geq0\\\alpha (e^x-1)&amp; x&lt;0\end{cases}\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>优化理论 Optimization</title>
    <link href="/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    <url>/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="优化理论-optimization">3. 优化理论 Optimization</h1><h2 id="优化理论-optimization-1">3.1 优化理论 Optimization</h2><p>一个线性分类器的loss可以表示为 <span class="math display">\[L(W) = \frac{1}{N}\sum_{i=1}^N L_i(x_i, y_i,W) + \lambda R(W)\]</span> <strong>优化理论（optimization）</strong>就是求使<spanclass="math inline">\(L\)</span>最小时<spanclass="math inline">\(W\)</span>的值，即求 <span class="math display">\[w^*=\arg \min_w L(w)\]</span></p><h2 id="梯度下降法-gradient-descent">3.2 梯度下降法 GradientDescent</h2><p>通过迭代的方式，沿函数梯度的负方向下降以寻找函数最小值的方法为<strong>梯度下降法（gradientdescent）</strong>： <span class="math display">\[x_{t+1} = x_t - \alpha \nabla f(x_t)\]</span> 代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Vanilla gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(loss_fn, data, w)<br>    w -= lenrning_rate * dw<br></code></pre></td></tr></table></figure><p>其中hyperparameters有<code>initailize_weights()</code>、<code>num_steps</code>和<code>learning_rate</code></p><p>在计算梯度时，通常使用<spanclass="math inline">\(\nabla_WL\)</span>的解析式计算，并用数值计算的方式检验</p><h2 id="随机梯度下降法-stochastic-gradient-descent-sgd">3.3随机梯度下降法 Stochastic Gradient Descent (SGD)</h2><p>loss的梯度计算表达式为 <span class="math display">\[\nabla_WL(W) = \frac{1}{N}\sum_{i=1}^N \nabla _W L_i(x_i, y_i,W) +\lambda \nabla_WR(W)\]</span> 当<spanclass="math inline">\(N\)</span>的数值很大时，计算梯度的开销会很大</p><p>为了避免巨大的开销，我们可以从概率的角度考虑loss function</p><p>对于一个数据集： <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> 式中<span class="math inline">\(x_i\)</span>是图像，<spanclass="math inline">\(y_i\)</span>是该图像对应的正确标签，我们将<spanclass="math inline">\(L\)</span>视作关于<spanclass="math inline">\(x\)</span>、<spanclass="math inline">\(y\)</span>的joint probability distribution</p><p>那么loss就可以看做该分布的期望，即 <span class="math display">\[L(W) = \mathbb E(L) +\lambda R(W)\]</span> <span class="math display">\[\nabla _W L(W) = \nabla_W\mathbb E(L) + \lambda \nabla_WR(W)\]</span></p><p>为了方便计算<span class="math inline">\(\mathbbE(L)\)</span>，可以采用蒙特卡洛方法进行采样估计： <spanclass="math display">\[L(W) \approx \frac1n \sum_{i=1}^n L_i(x_i, y_i, W) + \lambda R(W)\]</span> <span class="math display">\[\nabla_WL(W) \approx \frac1n \sum _{i=1}^n \nabla_WL_i(x_i, y_i, W) +\lambda \nabla _WR(W)\]</span></p><p>所以我们会从整个数据集中采样出minibatch来估计梯度，称为<strong>随机梯度下降法（stochasticgradient descent）</strong>，minibatch的大小通常为32、64或128</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    minibatch = sample_data(data, batch_size)<br>    dw = compute_gradient(loss_fn, minibatch, w)<br>    w -= learning_rate * dw<br></code></pre></td></tr></table></figure><p>其中hyperparameters有<code>initialize_weights()</code>，<code>num_steps</code>、<code>learning_rate</code>、<code>batch_size</code>和<code>sample_data()</code></p><h2 id="sgd-momentum-sgdm">3.4 SGD + Momentum (SGDM)</h2><p>当loss function有局部最小值（local minimum）或鞍点（saddlepoint）时，SGD方法会立即停止，我们引入”速度向量“来模拟一个小球沿斜坡下滚的过程：<span class="math display">\[v_{t+1} = \rho v_t + \nabla f(x_t)\]</span> <span class="math display">\[x_{t+1} = x_t - \alpha v_{t+1}\]</span></p><p>速度向量的实质就是梯度的累计，函数将沿累计的梯度方向下降</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent with momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    v = rho * v + dw<br>    w -= learning_rate * v<br></code></pre></td></tr></table></figure><p>同时由于速度向量的引入，我们引入了一个新的hyperparameter<code>rho</code>模拟摩擦系数以保证小球最终会停止，通常<code>rho</code>的值为0.9或0.99</p><h2 id="nesterov-momentum">3.5 Nesterov Momentum</h2><p>在累计梯度时，我们选择不累计当前的梯度，而是假设小球以当前的速度运动一小段距离后，累计运动后小球处的梯度，这种方法被称为<strong>Nesterovmomentum</strong>： <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t)\]</span> <span class="math display">\[x_{t+1} = x_t + v_{t+1}\]</span></p><p>通常为了计算方便，我们令<span class="math inline">\(\tilde x = x_t +\rho v_t\)</span>，于是有： <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(\tilde x_t)\]</span> <span class="math display">\[\tilde x_{t+1} = \tilde x_t + v_{t+1} +\rho ( v_{t+1} - v_t)\]</span></p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Nesterov momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    old_v = v<br>    v = rho * v - learning_rate * dw<br>    w -= rho * old_v - (<span class="hljs-number">1</span> + rho) * v<br></code></pre></td></tr></table></figure><h2 id="adagrad">3.6 AdaGrad</h2><p>在梯度下降法中，由于learningrate是固定的，梯度大小不同的方向下降的速度是相同的</p><p>但是我们希望在梯度较大的方向减缓下降速度以防震荡，梯度较小的方向加快下降速度，于是我们用<strong>AdaGrad</strong>使learningrate自适应大小： <span class="math display">\[S_{t+1} = S_t + \nabla_W^2\]</span> <span class="math display">\[x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># AdaGrad</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared += dw * dw<br>    w -= learning_rate * dw / (grad_sqruared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><h2 id="rmsprop">3.7 RMSProp</h2><p>在AdaGrad中，若迭代的次数过多，可能会出现梯度平方累积过大的情况，导致learningrate过小而不能产生有效迭代</p><p><strong>RMSProp</strong>效仿SGD+Momentum引入摩擦系数<spanclass="math inline">\(\rho\)</span>保证小球最终停止的想法，引入“摩擦“以防止梯度平方的累积过大：<span class="math display">\[S_{t+1} = \rho S_t + (1-\rho)\nabla_W^2\]</span> <span class="math display">\[x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># RMSProp</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared = decay_rate * grad_squared + (<span class="hljs-number">1</span> - decay_rate) * dw * dw<br>    w -= learning_rate * dw / (grad_squared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>decay_rate</code>为新的hyperparameter</p><h2 id="adam">3.8 Adam</h2><p>将RMSProp和momentum的思想结合起来，我们就得到了<strong>Adam</strong>：<span class="math display">\[v_{t+1} = \rho_1 v_t + (1 - \rho_1)\nabla _W\]</span> <span class="math display">\[S_{t+1} = \rho_2 S_t + (1-\rho_2)\nabla_W^2\]</span></p><p><span class="math display">\[x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>和<code>beta2</code>为新的hyperparameters</p><p>但是在第一次迭代时，由于<spanclass="math inline">\(\rho_2\)</span>通常趋近于1，<spanclass="math inline">\(S_1\)</span>和<spanclass="math inline">\(S_2\)</span>通常会趋近于0，导致第一次迭代的跨度会非常大，容易产生不好的结果，所以我们在Adam中会加入<strong>biascorrection</strong>来修正： <span class="math display">\[v_{t+1} = \frac{\rho_1 v_t + (1 - \rho_1)\nabla_W}{\sqrt{1-\rho_1^{t+1}}}\]</span> <span class="math display">\[S_{t+1} = \frac{\rho_2 S_t +(1-\rho_2)\nabla_W^2}{\sqrt{1-\rho_2^{t+1}}}\]</span></p><p><span class="math display">\[x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>修正后的代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam with bias correction</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    moment1_unbias = moment1 / (<span class="hljs-number">1</span> - beta1 ** t)<br>    moment2_unbias = moment2 / (<span class="hljs-number">1</span> - beta2 ** t)<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>通常取0.9，<code>beta2</code>通常取0.999，<code>learning_rate</code>通常取1e-3、5e-4、1e-4</p><h2 id="二阶优化-second-order-optimization">3.9 二阶优化 Second-OrderOptimization</h2><p>以上我们提到的优化方式均为<strong>一阶优化</strong>，它们通过梯度来线性拟合函数并迭代以得到函数最小值</p><p>那么，我们可以考虑通过梯度和黑塞矩阵来二次拟合函数，不妨设二次拟合的迭代式为<span class="math display">\[x_{t+1} = x_t+d\]</span> 我们希望<spanclass="math inline">\(x_{t+1}\)</span>尽可能小，即求 <spanclass="math display">\[d = \arg \min_d f(x_t+d)\]</span> <span class="math inline">\(f(x)\)</span>在<spanclass="math inline">\(x_t\)</span>处的二阶泰勒展开式为 <spanclass="math display">\[f(x) = f(x_t) + (x - x_t)^{\mathsf{T}}\nabla f(x_t) +\frac12(x-x_t)^{\mathsf{T}}\mathbf{H}f(x_t)(x-x_t)\]</span> 代入<span class="math inline">\(x_{t+1}\)</span>的值得 <spanclass="math display">\[f(x_{t+1}) = f(x_t+d)=f(x_t)+d^{\mathsf{T}}\nablaf(x_t)+\frac12d^{\mathsf{T}}\mathbf{H}f(x_t)d\]</span> 当<span class="math inline">\(d=-[\mathbf{H}f(x_t)]^{-1}\nablaf(x_t)\)</span>时取最小值</p><p>则二阶优化的迭代式为： <span class="math display">\[x_{t+1} = x_t-\left[\mathbf{H}f(x_t)\right]^{-1}\nabla f(x_t)\]</span>然而由于黑塞矩阵元素数量过多且矩阵求逆复杂度过高，实践中很少使用二阶优化</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>线性分类器 Linear Classifier</title>
    <link href="/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="线性分类器-linear-classifier">2. 线性分类器 LinearClassifier</h1><h2 id="评分函数-score-function">2.1 评分函数 Score Function</h2><p><strong>评分函数（score function）</strong> 是<spanclass="math inline">\(\mathbb{R}^D \to\mathbb{R}^C\)</span>的线性映射，即一张图片在每个标签上所得到的评分：<span class="math display">\[f(x, W) = Wx+b,x\in \mathbb{R}^D, W\in \mathbb{R}^{C\times D}, b\in \mathbb{R}^D\]</span></p><p>式中<spanclass="math inline">\(x\)</span>是将图像数据拉长（flatten）得到的<spanclass="math inline">\(D\)</span>维列向量，<spanclass="math inline">\(W\)</span>是<strong>参数（parameter）</strong>或称<strong>权重（weight）</strong> ，<spanclass="math inline">\(b\)</span>为<strong>偏差向量（biasvector）</strong></p><p><span class="math inline">\(C\)</span>为待分类的标签个数，<spanclass="math inline">\(f(x,W)\)</span>即为该图像对于每个标签的评分</p><h2 id="损失函数-loss-function">2.2 损失函数 Loss Function</h2><p><strong>损失函数（loss funtion）</strong>量化了线性分类器的效果，其值越高，则线性分类器效果越差，又被称为目标函数（objectivefunction）、代价函数（cost function）</p><p>对于一个数据集： <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> 式中<span class="math inline">\(x_i\)</span>是图像，<spanclass="math inline">\(y_i\)</span>是该图像对应的正确标签</p><p>则对于单个的一张图像的损失为： <span class="math display">\[L_i(f(x_i, W), y_i)\]</span> 对于数据集来说，损失是每张图像损失的平均值： <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)\]</span> <span class="math inline">\(L_i\)</span>即为损失函数</p><h2 id="多类支持向量机损失-multiclass-svm-loss">2.3 多类支持向量机损失Multiclass SVM Loss</h2><p>朴素的想法是，正确的标签的评分应当比其他标签的评分要高</p><p>所以，对于给定的一张图像<span class="math inline">\(x_i,y_i\)</span>，其评分<span class="math inline">\(s=f(x_i,W)\)</span>，则SVM损失有如下形式： <span class="math display">\[L_i = \sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)\]</span> 当评分均为很小的随机值时，损失应当接近<spanclass="math inline">\(C-1\)</span>，<spanclass="math inline">\(C\)</span>为待分类的总标签数，此性质可作为debug的依据</p><h2 id="正则化-regularization">2.4 正则化 Regularization</h2><p>使上述的损失<span class="math inline">\(L\)</span>最小的<spanclass="math inline">\(W\)</span>并不唯一</p><p>若<span class="math inline">\(W\)</span>可使损失<spanclass="math inline">\(L\)</span>最小，则<spanclass="math inline">\(\lambda W\)</span>也可使<spanclass="math inline">\(L\)</span>最小</p><p>于是，我们在损失函数的表达式中引入一项<spanclass="math inline">\(\lambda R(W)\)</span> : <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)+\lambda R(W)\]</span> 式中<spanclass="math inline">\(\lambda\)</span>为<strong>正则化强度（regularizationstrength）</strong> ，为超参数</p><p>正则化的好处：</p><ol type="1"><li><p>进一步地筛选<spanclass="math inline">\(W\)</span>，使所选定的<spanclass="math inline">\(W\)</span>拥有除最小化损失以外的其他功能</p></li><li><p>避免<strong>过拟合（overfitting）</strong></p></li><li><p>通过增加曲率以提高<strong>优化（optimization）</strong>效果</p></li></ol><p>常用的正则化有：</p><p>L2正则化 <span class="math display">\[R(W) = \sum_k \sum_l W_{k,l}^2\]</span> L1正则化 <span class="math display">\[R(W)= \sum_k \sum_l \left| W_{k,l}^2 \right |\]</span> 或者将二者联系起来： <span class="math display">\[R(W) = \sum_k \sum_l \beta W_{k,l}^2 + \left |W_{k,l}^2 \right |\]</span> 其他还有dropout、Batchnormalization、Cutout、Mixup、Stochastic depth等等</p><h2 id="交叉熵损失-cross-entropy-loss">2.5 交叉熵损失 Cross-EntropyLoss</h2><p>另一种损失函函数使用<strong>归一化指数函数（softmaxfunction）</strong> 将评分用概率来描述，被称为<strong>交叉熵损失（Cross- Entropy Loss）</strong>或者<strong>多元逻辑回归（Multinomial Logistic Regression）</strong></p><p>对于一个评分函数<spanclass="math inline">\(s=f(x_i,W)\)</span>，其softmax function的形式为：<span class="math display">\[P(Y = k|X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}\]</span> 我们假设真实的概率分布为<spanclass="math inline">\(P\)</span>，训练得到的概率分布为<spanclass="math inline">\(Q\)</span>，我们使用<spanclass="math inline">\(Q\)</span>来拟合<spanclass="math inline">\(P\)</span>，则交叉熵为： <spanclass="math display">\[H(P, Q) = H(P) + D_{KL}(P||Q)\]</span> 式中<spanclass="math inline">\(D_{KL}(P||Q)\)</span>为<strong>相对熵（Kullback-LeiblerDivergence）</strong> ： <span class="math display">\[D_{KL}(P||Q) = \sum_{i=1}^n P(x_i)\log \frac{P(x_i)}{Q(x_i)}\]</span> 由于真实的概率分布不变，即<spanclass="math inline">\(H(P)\)</span>不变，则若交叉熵<spanclass="math inline">\(H(P,Q)\)</span>最小，只需相对熵<spanclass="math inline">\(D_{KL}(P||Q)\)</span>最小即可</p><p>当单张图片的损失具有如下形式时，交叉熵最小： <spanclass="math display">\[L_i = - \log P(Y=y_i|X = x_i)\]</span> 所以交叉熵损失的具体形式为： <span class="math display">\[L = \frac1N \sum_i\left(-\log\left(\frac{e^{s_{y_i}}}{\sum_je^{s_j}}\right)\right)+\lambda R(W)\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>图像分类 Image Classification</title>
    <link href="/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    <url>/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="图像分类-image-classification">1. 图像分类 ImageClassification</h1><h2 id="图像分类器">1.1 图像分类器</h2><p>图像分类的算法难以用如下的函数进行硬编码(hard-code)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify_image</span>(<span class="hljs-params">image</span>):<br><span class="hljs-comment"># Some magic here?</span><br><span class="hljs-keyword">return</span> class_label<br></code></pre></td></tr></table></figure><p>所以通常采用机器学习，即Data-Driven的方式</p><p>先用包含图像与标签的数据集训练分类器，再评估分类器在分类新图片的表现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">images, labels</span>):<br><span class="hljs-comment"># Machine learning!</span><br><span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">model, test_images</span>):<br><span class="hljs-comment"># Use model to predict labels</span><br><span class="hljs-keyword">return</span> test_labels<br></code></pre></td></tr></table></figure><h2 id="常见数据集">1.2 常见数据集</h2><p>MNIST</p><p>CIFAR10</p><p>CIFAR100</p><p>ImageNet</p><p>MIT Places</p><h2 id="邻近算法-nearest-neighbor">1.3 邻近算法 Nearest Neighbor</h2><ol type="1"><li>记忆数据集中所有的图像和对应的标签</li><li>与数据集中最相似的图像的标签即为新图像的标签</li></ol><p>使用<strong>距离度量</strong>比较图像，以下是常见的两种距离度量：</p><p>L1 距离（曼哈顿距离 Manhattan distance）: <spanclass="math display">\[d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|\]</span> L2 距离（欧拉距离 Euclidean distance）： <spanclass="math display">\[d_2(I_1, I_2) = \sqrt{\sum_p (I_1^p - I_2^p)^2}\]</span></p><h2 id="k近邻算法-k-nearest-neighbors">1.4 K近邻算法 K-NearestNeighbors</h2><p>若 𝑘 个最相似的图像中的大多数为某一个标签，则该图像也属于这个标签</p><p>当训练的样本足够多时，K近邻算法可以表示任何函数</p><h2 id="超参数-hyperparameter">1.5 超参数 Hyperparameter</h2><p>从数据集中无法通过训练得到的参数为<strong>超参数</strong>，超参数需要在学习之前设定</p><p>例如K邻近算法中的K就是超参数</p><p>超参数的设定方法：</p><p>将数据集分为train、validate和test三部分，选择超参数的值在train上训练，并在validate中验证，只在最后使用test上查看效果</p><p>如果条件允许，可以将数据集分割成许多部分，每次选择不同的部分作为validate，剩下的部分作为train，同样的只在最后在test上检验</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习与计算机视觉</title>
    <link href="/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    <url>/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h2 id="写在前面">写在前面</h2><p>这是一篇Umich的EECS498-007课程的学习笔记，开始阅读之前您需要学习过这门课程或已经掌握了深度学习与计算机视觉的基本知识。本篇旨在复习课程要点，不适合初学者作为tutorial学习。</p><h2 id="关于umich-eecs-498-007">关于Umich EECS 498-007</h2><p>来自<ahref="https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/EECS498-007/">csdiy</a>的课程简介</p><blockquote><p>UMich 的 Computer Vision课，课程视频和作业质量极高，涵盖的主题非常全，同时 Assignments的难度由浅及深，覆盖了 CV 主流模型发展的全阶段，是一门非常好的 ComputerVision 入门课。</p></blockquote><p>本篇笔记基于Fall2019的课程，课程链接：https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r</p><p>与此同时，该课程的assignments（含答案）已上传至<ahref="https://github.com/hmnkapa/eecs598">github</a>，答案是我自己做的，仅供参考</p><p>Assignments来自Fall 2020的<ahref="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">课程主页</a></p><h2 id="目录">目录</h2><ol type="1"><li>图像分类 Image Classification</li><li>线性分类器 Linear Classifier</li><li>优化理论 Optimization</li><li>神经网络 Neural Networks</li><li>反向传播算法 Backpropagation</li><li>卷积网络 Convolutional Networks</li><li>CNN架构 CNN Architectures</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>柯西-施瓦茨（Cauchy-Schwarz）不等式的积分形式</title>
    <link href="/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/"/>
    <url>/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="柯西-施瓦茨不等式的积分形式">柯西-施瓦茨不等式的积分形式</h2><p>假设 <span class="math inline">\(𝑓(𝑥)\)</span> 和 <spanclass="math inline">\(𝑔(𝑥)\)</span> 在区间 <spanclass="math inline">\([𝑎,𝑏]\)</span> 上黎曼可积，那么</p><p><span class="math display">\[\int_𝑎^𝑏𝑓^2(𝑥)\mathrm{d}𝑥⋅\int_𝑎^𝑏𝑔^2(𝑥)\mathrm{d}𝑥≥(\int_𝑎^𝑏𝑓(𝑥)𝑔(𝑥)\mathrm{d}𝑥)^2\]</span></p><h2 id="前置知识">前置知识</h2><p>柯西不等式 <span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 及其一般形式 <span class="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 极限、微积分基本知识</p><h2id="柯西-施瓦茨不等式的积分形式的证明">柯西-施瓦茨不等式的积分形式的证明</h2><p>事实上，说起柯西，在高中课本（习题）上我们已经学习过柯西不等式的二维形式</p><p><span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 进一步地，柯西不等式的一般形式如下 <spanclass="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 既然都叫柯西（</p><p>我们考虑使用柯西不等式的一般形式来证明柯西-施瓦茨不等式的积分形式</p><p>由定积分的定义，柯西-施瓦茨不等式的积分形式等价于 <spanclass="math display">\[\lim_{n\to\infty}\sum_{i=1}^nf^2(x_i)\Delta{x} \cdot\lim_{n\to\infty}\sum_{i=1}^ng^2(x_i)\Delta{x} \geq \left(\lim_{n\to\infty}\sum_{i=1}^nf(x_i)g(x_i)\Delta{x} \right)^2\\\]</span> 此处<spanclass="math inline">\(\Delta{x}=\frac{b-a}{n}\)</span>，<spanclass="math inline">\(x_i=a+i\Delta{x}\)</span></p><p>由极限的运算法则，上式等价于 <span class="math display">\[\lim_{n\to\infty} \left[ \sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i)- \left( \sum_{i=1}^nf(x_i)g(x_i) \right)^2 \right] \Delta^2{x} \geq0\\\]</span> 由极限的保号性并约去<spanclass="math inline">\(\Delta^2{x}\)</span>，有 <spanclass="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) - \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \geq0\\\]</span> 即 <span class="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) \geq \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \\\]</span> 此即柯西不等式的一般形式.</p><p>柯西不等式的一般形式的取等条件为<spanclass="math inline">\(\frac{a_1}{b_1}=\frac{a_2}{b_2}=...=\frac{a_n}{b_n}\)</span>，因此对于柯西-施瓦茨不等式的一般形式，</p><p>当且仅当 <span class="math display">\[f(x)=\lambda g(x)\]</span> 时，等号成立，式中<spanclass="math inline">\(\lambda\)</span>为一实数.</p><h2 id="一道例题">一道例题</h2><p>这个不等式是最近在准备微积分的第一次期中考试时遇到的</p><p>（其实我开始写文章的时候距离考试还有24分钟</p><p>想找一些卷子做一做</p><p>于是发现了一份浙江某211大学号称<ahref="https://www.zhihu.com/question/265940112?utm_medium=social&amp;utm_oi=1650268599276343296&amp;utm_psn=1703111356650299392&amp;utm_source=qq">120年来最难的微积分试卷</a></p><p>其最后一题原题如下：</p><blockquote><ol start="12" type="1"><li>设 <span class="math inline">\(𝑓(𝑥)\)</span> 在 <spanclass="math inline">\([0,1]\)</span> 上连续且可导，当 <spanclass="math inline">\(𝑥∈[0,1]\)</span> 时， <spanclass="math inline">\(\displaystyle\int_𝑥^1𝑓(𝑡)\mathrm{d}𝑡≥\frac{1-x^3}{2}\)</span> ，证明： <spanclass="math display">\[\displaystyle \int_0^1\,[f(x)]^2\mathrm{d}x&gt;\frac{5}{12}\\\]</span></li></ol></blockquote><p>令<span class="math inline">\(\displaystyleF(x)=\int_x^1\,f(t)\mathrm{d}t=-\int_1^x\,f(t)\mathrm{d}t\)</span> ，则<span class="math inline">\(\displaystyle F^\prime(x)=-f(x)\)</span></p><p>由 <span class="math inline">\(\displaystyleF(x)\geq\frac{1-x^3}{2}\)</span> 可得 <span class="math display">\[\int_0^1\,F(x)\mathrm{d}x \geq\int_0^1\frac{1-x^3}{2}\mathrm{d}x=\frac{3}{8}\\\]</span> 下面运用分部积分法来计算 <spanclass="math inline">\(\displaystyle \int_0^1\,F(x)\mathrm{d}x\)</span><span class="math display">\[\begin{align} \int_0^1\,F(x)\mathrm{d}x&amp;=xF(x)\Big]_0^1-\int_0^1x\mathrm{d}F(x)\\&amp;=F(1)+\int_0^1xf(x)\mathrm{d}x\\ &amp;=\int_0^1xf(x)\mathrm{d}x\\\end{align} \\\]</span></p><p>于是，我们有 <span class="math display">\[\int_0^1xf(x)\mathrm{d}x \geq \frac{3}{8} \\\]</span> 由柯西-施瓦茨不等式 <span class="math display">\[\int_0^1\,f^2(x)\mathrm{d}x\cdot\int_0^1\,x^2\mathrm{d}x \geq \left(\int_0^1\,xf(x)\mathrm{d}x \right)^2 \geq \frac{9}{64} \\\]</span> <span class="math inline">\(\displaystyle\int_0^1\,x^2\mathrm{d}x\)</span>是好积的，其结果为 <spanclass="math inline">\(\frac{1}{3}\)</span> ，故 <spanclass="math display">\[\int_0^1\,f^2(x)\mathrm{d}x \geq \frac{27}{64}&gt;\frac{5}{12}\\\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
