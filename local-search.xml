<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>常微分方程的现代视角：沃尔泰拉积分方程</title>
    <link href="/2025/06/12/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E7%9A%84%E7%8E%B0%E4%BB%A3%E8%A7%86%E8%A7%92%EF%BC%9A%E6%B2%83%E5%B0%94%E6%B3%B0%E6%8B%89%E7%A7%AF%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    <url>/2025/06/12/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E7%9A%84%E7%8E%B0%E4%BB%A3%E8%A7%86%E8%A7%92%EF%BC%9A%E6%B2%83%E5%B0%94%E6%B3%B0%E6%8B%89%E7%A7%AF%E5%88%86%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1id="常微分方程的现代视角沃尔泰拉积分方程">常微分方程的现代视角：沃尔泰拉积分方程</h1><h2 id="沃尔泰拉volterra积分方程">沃尔泰拉（Volterra）积分方程</h2><p>考虑一个标准的一阶常微分方程初值问题 (IVP)，其形式如下：</p><p><span class="math display">\[\displaystyle y&#39; = f(t, y) \quad \land \quad y(t_0) = y_0\]</span></p><p>其中函数 <span class="math inline">\(\displaystyle f: D \rightarrow\mathbb{R}\)</span> 在开集 <span class="math inline">\(\displaystyle D\subseteq \mathbb{R}^2\)</span> 上连续，且点 <spanclass="math inline">\(\displaystyle (t_0, y_0)\)</span> 属于 <spanclass="math inline">\(\displaystyle D\)</span>。</p><p>我们可以通过对该微分方程从 <span class="math inline">\(\displaystylet_0\)</span> 到 <span class="math inline">\(\displaystyle t\)</span>进行积分，将其转化为一个等价的积分方程。</p><p>若 <span class="math inline">\(\displaystyle \phi(t)\)</span> 是该IVP 在某个包含 <span class="math inline">\(\displaystyle t_0\)</span>的区间 <span class="math inline">\(\displaystyle I\)</span>上的解，那么它必须满足： <span class="math display">\[\displaystyle \phi(t) = y_0 + \int_{t_0}^{t} \phi&#39;(s) \, ds\]</span></p><p>由于 <span class="math inline">\(\displaystyle \phi&#39;(s) = f(s,\phi(s))\)</span>，我们可以将上式改写为：</p><p><span class="math display">\[\displaystyle \phi(t) = y_0 + \int_{t_0}^{t} f(s, \phi(s)) \, ds \quad\text{(IE)}\]</span></p><p>这个方程被称为<strong>沃尔泰拉（Volterra）积分方程</strong>。</p><p>反之，任何满足此积分方程 (IE) 的连续函数 <spanclass="math inline">\(\displaystyle\phi(t)\)</span>，其函数图像包含于定义域 <spanclass="math inline">\(\displaystyle D\)</span>内，也必然是原初值问题的解。</p><p>因此，求解一个一阶 IVP的问题被完全转化为了求解一个等价的积分方程的问题。</p><h2 id="算子与不动点">算子与不动点</h2><p>积分方程的表述引出了一种非常有力的分析工具：<strong>算子（Operator）</strong></p><p>我们可以定义一个作用于函数空间上的算子 <spanclass="math inline">\(\displaystyle T\)</span> 如下： <spanclass="math display">\[\displaystyle (T\psi)(t) = y_0 + \int_{t_0}^{t} f(s, \psi(s)) \, ds\]</span></p><p>此算子 <span class="math inline">\(\displaystyle T\)</span>将一个定义在区间 <span class="math inline">\(\displaystyle I\)</span>上的连续函数 <span class="math inline">\(\displaystyle \psi\)</span>映射到另一个函数 <span class="math inline">\(\displaystyleT\psi\)</span>。</p><p>借助这个算子，积分方程 (IE) 可以被简洁地表示为：</p><p><span class="math display">\[\displaystyle T\phi = \phi\]</span></p><p>满足这个条件的函数 <span class="math inline">\(\displaystyle\phi\)</span> 被称为算子 <span class="math inline">\(\displaystyleT\)</span> 的 <strong>不动点</strong> (Fixed Point)。</p><p>这个发现至关重要，因为它将微分方程的求解问题转化为了一个在函数空间中寻找算子不动点的问题。</p><p>这使得我们可以应用强大的不动点理论（如巴拿赫不动点定理）来证明解的存在性和唯一性。</p><blockquote><p>考虑以下初值问题：</p><p><span class="math display">\[\displaystyle y&#39; = 2y \quad \land \quad y(1) = 3\]</span></p><p>其对应的算子 <span class="math inline">\(\displaystyle T\)</span>为：</p><p><span class="math display">\[\displaystyle (T\psi)(t) = 3 + \int_{1}^{t} 2\psi(s) \, ds\]</span></p><p>该 IVP 的解是 <span class="math inline">\(\displaystyle \phi(t) =3e^{2(t-1)}\)</span>。我们可以验证 <spanclass="math inline">\(\displaystyle \phi(t)\)</span> 确实是算子 <spanclass="math inline">\(\displaystyle T\)</span> 的一个不动点：</p><p><span class="math display">\[\displaystyle\begin{aligned}(T\phi)(t) &amp;= 3 + \int_{1}^{t} 2 \cdot (3e^{2(s-1)}) \, ds \\&amp;= 3 + \int_{1}^{t} 6e^{2s-2} \, ds \\&amp;= 3 + [3e^{2s-2}]_{1}^{t} \\&amp;= 3 + (3e^{2t-2} - 3e^{2(1)-2}) \\&amp;= 3e^{2t-2} = \phi(t)\end{aligned}\]</span></p><p>由于 <span class="math inline">\(\displaystyle T\phi =\phi\)</span>，函数 <span class="math inline">\(\displaystyle\phi(t)\)</span> 是算子 <span class="math inline">\(\displaystyleT\)</span> 的不动点，从而也是原 IVP 的解。</p></blockquote><h2 id="高阶常微分方程的降阶">高阶常微分方程的降阶</h2><p>我们可以将一个 n阶常微分方程转化为一个等价的一阶常微分方程组，从而将高阶问题简化为我们已经熟悉的一阶系统问题。</p><p>考虑一个一般的 n 阶显式常微分方程：</p><p><span class="math display">\[\displaystyle y^{(n)} = f(t, y, y&#39;, \dots, y^{(n-1)})\]</span></p><p>其中 <span class="math inline">\(\displaystyle f\)</span>是一个在其定义域上连续的函数。</p><p>为了实现降阶，我们引入一个向量 <spanclass="math inline">\(\displaystyle\mathbf{y}\)</span>，其分量由未知函数 <spanclass="math inline">\(\displaystyle y\)</span> 及其直到 <spanclass="math inline">\(\displaystyle (n-1)\)</span> 阶的导数构成：</p><p><span class="math display">\[\displaystyle \mathbf{y}(t) = \begin{pmatrix} y_0(t) \\ y_1(t) \\ \vdots\\ y_{n-1}(t) \end{pmatrix} = \begin{pmatrix} y(t) \\ y&#39;(t) \\\vdots \\ y^{(n-1)}(t) \end{pmatrix}\]</span></p><p>对该向量求导，我们得到：</p><p><span class="math display">\[\displaystyle \mathbf{y}&#39;(t) = \begin{pmatrix} y_0&#39; \\ y_1&#39;\\ \vdots \\ y_{n-2}&#39; \\ y_{n-1}&#39; \end{pmatrix} =\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_{n-1} \\ y^{(n)} \end{pmatrix}= \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_{n-1} \\ f(t, y_0, y_1,\dots, y_{n-1}) \end{pmatrix}\]</span></p><p>这样，我们就得到了一个一阶向量微分方程 <spanclass="math inline">\(\displaystyle \mathbf{y}&#39; = \mathbf{f}(t,\mathbf{y})\)</span>，其中函数 <span class="math inline">\(\displaystyle\mathbf{f}\)</span> 定义为：</p><p><span class="math display">\[\displaystyle \mathbf{f}(t, y_0, \dots, y_{n-1}) = \begin{pmatrix} y_1\\ \vdots \\ y_{n-1} \\ f(t, y_0, y_1, \dots, y_{n-1}) \end{pmatrix}\]</span></p><p>原 n 阶方程的解 <span class="math inline">\(\displaystyley(t)\)</span> 正是这个一阶方程组解向量 <spanclass="math inline">\(\displaystyle \mathbf{y}(t)\)</span> 的第一个分量<span class="math inline">\(\displaystyley_0(t)\)</span>。同样的降阶方法也适用于相应的初值问题。</p><blockquote><p>考虑二阶线性方程 <span class="math inline">\(\displaystyley&#39;&#39; + y = 0\)</span>。</p><p>我们令 <span class="math inline">\(\displaystyle y_0 = y\)</span> 和<span class="math inline">\(\displaystyle y_1 =y&#39;\)</span>。那么可以得到：</p><ul><li><span class="math inline">\(\displaystyle y_0&#39; =y_1\)</span></li><li><span class="math inline">\(\displaystyle y_1&#39; = y&#39;&#39; =-y = -y_0\)</span></li></ul><p>因此，原二阶方程等价于以下一阶方程组：</p><p><span class="math display">\[\displaystyle \begin{pmatrix} y_0 \\ y_1 \end{pmatrix}&#39; =\begin{pmatrix} y_1 \\ -y_0 \end{pmatrix}\]</span></p><p>我们知道原方程的通解是 <span class="math inline">\(\displaystyle y(t)= A\cos t + B\sin t\)</span>。因此，对应系统的解向量为：</p><p><span class="math display">\[\displaystyle \mathbf{y}(t) = \begin{pmatrix} y_0(t) \\ y_1(t)\end{pmatrix} = \begin{pmatrix} A\cos t + B\sin t \\ -A\sin t + B\cos t\end{pmatrix}\]</span></p></blockquote><p>通过这种降阶技术，包括高阶方程和方程组在内的大多数问题都可以被转化为一阶系统问题进行研究，这极大地统一了微分方程的理论体系。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>多变量函数与含参反常积分的一致收敛性</title>
    <link href="/2025/06/11/%E5%A4%9A%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E4%B8%8E%E5%90%AB%E5%8F%82%E5%8F%8D%E5%B8%B8%E7%A7%AF%E5%88%86%E7%9A%84%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E6%80%A7/"/>
    <url>/2025/06/11/%E5%A4%9A%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E4%B8%8E%E5%90%AB%E5%8F%82%E5%8F%8D%E5%B8%B8%E7%A7%AF%E5%88%86%E7%9A%84%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<h1id="多变量函数与含参反常积分的一致收敛性">多变量函数与含参反常积分的一致收敛性</h1><h2 id="dirichlet-判别法与-abel-判别法">Dirichlet 判别法与 Abel判别法</h2><p><strong>定理：</strong></p><p>设 <span class="math inline">\((\displaystyle f_n)\)</span>是定义在集合 <span class="math inline">\(\displaystyle D\)</span>上的单调递减实值函数序列（即对任意 <spanclass="math inline">\(\displaystyle x \in D\)</span>，都有 <spanclass="math inline">\(\displaystyle f_1(x) \ge f_2(x) \ge f_3(x) \ge\dots\)</span>），<span class="math inline">\((\displaystyleg_n)\)</span> 是定义在 <span class="math inline">\(\displaystyleD\)</span> 上的复值函数序列</p><p>在满足以下两个准则之一时，函数级数 <spanclass="math inline">\(\displaystyle\sum_{n=1}^{\infty}f_{n}g_{n}\)</span> 在 <spanclass="math inline">\(\displaystyle D\)</span> 上一致收敛：</p><ol type="1"><li><strong>Dirichlet 判别法</strong><ul><li><span class="math inline">\((\displaystyle f_n)\)</span> 在 <spanclass="math inline">\(\displaystyle D\)</span> 上一致收敛到 <spanclass="math inline">\(\displaystyle 0\)</span></li><li>级数 <span class="math inline">\(\displaystyle\sum_{n=1}^{\infty}g_{n}\)</span> 的部分和序列 <spanclass="math inline">\(\displaystyle G_n = \sum_{k=1}^{n}g_k\)</span> 在<span class="math inline">\(\displaystyle D\)</span>上一致有界（即存在常数 <span class="math inline">\(\displaystyleM&gt;0\)</span>，使得对所有 <span class="math inline">\(\displaystylen\)</span> 和所有 <span class="math inline">\(\displaystyle x \inD\)</span>，都有 <span class="math inline">\(\displaystyle |G_n(x)| \leM\)</span>）</li></ul></li><li><strong>Abel 判别法</strong><ul><li><span class="math inline">\((\displaystyle f_n)\)</span> 在 <spanclass="math inline">\(\displaystyle D\)</span> 上一致有界</li><li>级数 <span class="math inline">\(\displaystyle\sum_{n=1}^{\infty}g_{n}\)</span> 在 <spanclass="math inline">\(\displaystyle D\)</span> 上一致收敛</li></ul></li></ol><p>这两个判别法的证明都依赖于<strong>分部求和法</strong>（或称<strong>Abel求和法</strong>）和<strong>柯西一致收敛准则</strong></p><p>柯西准则指出，一个函数级数一致收敛的充要条件是，其部分和序列是满足一致柯西条件的</p><h3 id="abel-极限定理-abels-limit-theorem">Abel 极限定理 (Abel's LimitTheorem)</h3><p>Abel判别法的一个重要推论是Abel极限定理，它描述了幂级数在其收敛圆边界上的连续性</p><p><strong>定理 :</strong></p><p>假设幂级数 <span class="math inline">\(\displaystyle\sum_{n=0}^{\infty}a_{n}(z-a)^{n}\)</span> 的收敛半径为 <spanclass="math inline">\(\displaystyle R\)</span>，<spanclass="math inline">\(\displaystyle 0 &lt; R &lt; \infty\)</span></p><p>如果该幂级数在其收敛圆边界上的某一点 <spanclass="math inline">\(\displaystyle z_1 = a + Re^{i\phi}\)</span>收敛</p><p>则该级数在连接中心 <span class="math inline">\(\displaystylea\)</span> 与 <span class="math inline">\(\displaystyle z_1\)</span>的闭合线段 <span class="math inline">\(\displaystyle [a, z_1]\)</span>上一致收敛</p><p>因此，由该级数定义的函数在 <span class="math inline">\(\displaystyle[a, z_1]\)</span> 上是连续的</p><p><strong>证明：</strong></p><p>将级数在点 <span class="math inline">\(\displaystyle z = a +re^{i\phi}\)</span> (<span class="math inline">\(\displaystyle 0 \le r\le R\)</span>) 处写作</p><p><span class="math display">\[\displaystyle \sum_{n=0}^{\infty} \left(\frac{r}{R}\right)^n (a_n R^ne^{in\phi})\]</span></p><p>令 <span class="math inline">\(\displaystyle f_n(r) =(r/R)^n\)</span>，<span class="math inline">\(\displaystyle g_n(r) = a_nR^n e^{in\phi}\)</span></p><p>序列 <span class="math inline">\(\displaystyle f_n(r)\)</span> 对<span class="math inline">\(\displaystyle r \in [0,R]\)</span>是单调递减且一致有界的（<span class="math inline">\(\displaystyle 0 \lef_n(r) \le 1\)</span>）</p><p>而级数 <span class="math inline">\(\displaystyle \sum g_n\)</span>根据假设收敛，且其项与 <span class="math inline">\(\displaystyler\)</span> 无关，因此它在 <span class="math inline">\(\displaystyle[0,R]\)</span> 上一致收敛</p><p>应用Abel判别法，即可证明原级数在 <spanclass="math inline">\(\displaystyle [a, z_1]\)</span> 上一致收敛</p><h2 id="多变量函数的一致收敛">多变量函数的一致收敛</h2><p>一致收敛的理论可以自然地推广到多变量函数</p><p>这里我们只叙述微分定理的多变量形式</p><p><strong>定理：</strong></p><p>假设函数序列 <span class="math inline">\(\displaystyle f_k: D\rightarrow \mathbb{R}\)</span>（其中 <spanclass="math inline">\(\displaystyle D \subseteq\mathbb{R}^n\)</span>）满足：</p><ul><li>每个 <span class="math inline">\(\displaystyle f_k\)</span> 都是<span class="math inline">\(\displaystyle C^1\)</span> 函数</li><li>序列 <span class="math inline">\((\displaystyle f_k)\)</span> 在<span class="math inline">\(\displaystyle D\)</span> 上逐点收敛于函数<span class="math inline">\(\displaystyle f\)</span></li><li>对于每一个 <span class="math inline">\(\displaystyle i=1, \dots,n\)</span>，偏导数序列 <span class="math inline">\((\displaystyle\partial f_k / \partial x_i)\)</span> 都在 <spanclass="math inline">\(\displaystyle D\)</span> 上一致收敛</li></ul><p>那么，极限函数 <span class="math inline">\(\displaystyle f(x) =\lim_{k\rightarrow\infty} f_k(x)\)</span> 也是一个 <spanclass="math inline">\(\displaystyle C^1\)</span>函数，并且其偏导数可以和极限运算交换顺序：</p><p><span class="math display">\[\displaystyle \frac{\partial f}{\partial x_i}(x) =\lim_{k\rightarrow\infty} \frac{\partial f_k}{\partial x_i}(x) \quad\text{for } 1 \le i \le n \text{ and } x \in D\]</span></p><h2 id="含参反常积分的一致收敛性">含参反常积分的一致收敛性</h2><p>函数级数的理论有其连续的对应物，即含参反常积分</p><h3 id="含参反常积分的收敛性定义">含参反常积分的收敛性定义</h3><ul><li><p><strong>逐点收敛:</strong> 含参反常积分 <spanclass="math inline">\(\displaystyle \int_a^\infty f(x,t) dt\)</span> 在<span class="math inline">\(\displaystyle D\)</span>上逐点收敛，是指对每一个固定的 <span class="math inline">\(\displaystylex \in D\)</span>，极限 <span class="math inline">\(\displaystyle\lim_{R\rightarrow\infty} \int_a^R f(x,t) dt\)</span> 都存在</p></li><li><p><strong>一致收敛:</strong> 如果积分逐点收敛于 <spanclass="math inline">\(\displaystyle F(x)\)</span>，并且对于任意 <spanclass="math inline">\(\displaystyle \epsilon &gt; 0\)</span>，存在一个与<span class="math inline">\(\displaystyle x\)</span> 无关的 <spanclass="math inline">\(\displaystyle R_0\)</span>，使得当 <spanclass="math inline">\(\displaystyle R &gt; R_0\)</span> 时，对所有 <spanclass="math inline">\(\displaystyle x \in D\)</span> 都有：</p><p><span class="math display">\[  \displaystyle \left| F(x) - \int_a^R f(x,t) dt \right| = \left|\int_R^\infty f(x,t) dt \right| &lt; \epsilon  \]</span></p><p>则称该积分在 <span class="math inline">\(\displaystyle D\)</span>上一致收敛</p></li></ul><h2 id="主要定理">主要定理</h2><p>与函数序列相类似，我们有关于连续性和可微性的核心定理</p><h3 id="连续性定理">连续性定理</h3><p>若 <span class="math inline">\(\displaystyle f(x,t)\)</span>是一个连续的双变量函数，且积分<span class="math inline">\(\displaystyle\int_a^\infty f(x,t) dt\)</span> 在区间 <spanclass="math inline">\(\displaystyle I\)</span> 上一致收敛</p><p>则其极限函数 <span class="math inline">\(\displaystyle F(x) =\int_a^\infty f(x,t) dt\)</span> 在 <spanclass="math inline">\(\displaystyle I\)</span> 上是连续的</p><h3 id="可微性定理leibniz积分法则">可微性定理（Leibniz积分法则）</h3><p>若 <span class="math inline">\(\displaystyle f(x,t)\)</span>及其偏导数 <span class="math inline">\(\displaystyle f_x(x,t)\)</span>都是连续的双变量函数</p><p>积分 <span class="math inline">\(\displaystyle \int_a^\infty f(x,t)dt\)</span> 逐点收敛，且导数积分 <spanclass="math inline">\(\displaystyle \int_a^\infty f_x(x,t) dt\)</span>在区间 <span class="math inline">\(\displaystyle I\)</span>上一致收敛</p><p>则 <span class="math inline">\(\displaystyle F(x) = \int_a^\inftyf(x,t) dt\)</span> 在 <span class="math inline">\(\displaystyleI\)</span> 上可微，并且可以交换微分与积分的次序： <spanclass="math display">\[\displaystyle F&#39;(x) = \frac{d}{dx} \int_a^\infty f(x,t) dt =\int_a^\infty \frac{\partial f}{\partial x}(x,t) dt\]</span></p><h3 id="一致收敛判别法"><strong>一致收敛判别法</strong></h3><ol type="1"><li><p><strong>Cauchy 判别法:</strong></p><p><span class="math inline">\(\displaystyle \int_a^\infty f(x,t)dt\)</span> 在 <span class="math inline">\(\displaystyle D\)</span>上一致收敛的充要条件是：对任意 <span class="math inline">\(\displaystyle\epsilon &gt; 0\)</span>，存在 <span class="math inline">\(\displaystyleR_0 &gt; 0\)</span>，使得对所有 <spanclass="math inline">\(\displaystyle R&#39; &gt; R &gt; R_0\)</span>和所有 <span class="math inline">\(\displaystyle x \in D\)</span>，都有<span class="math inline">\(\displaystyle |\int_R^{R&#39;} f(x,t) dt|&lt; \epsilon\)</span></p></li><li><p><strong>Weierstrass 判别法:</strong></p><p>如果存在一个函数 <span class="math inline">\(\displaystyle\Phi(t)\)</span> 使得对所有 <span class="math inline">\(\displaystyle(x,t) \in D \times [a, \infty)\)</span> 都有 <spanclass="math inline">\(\displaystyle |f(x,t)| \le \Phi(t)\)</span>，且<span class="math inline">\(\displaystyle \int_a^\infty \Phi(t)dt\)</span> 收敛，则 <span class="math inline">\(\displaystyle\int_a^\infty f(x,t) dt\)</span> 在 <spanclass="math inline">\(\displaystyle D\)</span> 上一致且绝对收敛</p></li><li><p><strong>Dirichlet 判别法与 Abel 判别法:</strong></p><p>假设 <span class="math inline">\(\displaystyle t \mapstof(x,t)\)</span> 对每个 <span class="math inline">\(\displaystylex\)</span> 都是单调函数，而 <span class="math inline">\(\displaystyle t\mapsto g(x,t)\)</span> 是连续函数积分 <spanclass="math inline">\(\displaystyle \int_a^\infty f(x,t)g(x,t)dt\)</span> 在满足以下条件之一时一致收敛：</p><ul><li><strong>Dirichlet:</strong> <spanclass="math inline">\(\displaystyle f(x,t)\)</span> 随着 <spanclass="math inline">\(\displaystyle t \to \infty\)</span> 在 <spanclass="math inline">\(\displaystyle D\)</span> 上一致收敛到 <spanclass="math inline">\(\displaystyle 0\)</span>，且积分 <spanclass="math inline">\(\displaystyle \int_a^R g(x,t)dt\)</span> 对所有<span class="math inline">\(\displaystyle R\)</span> 和 <spanclass="math inline">\(\displaystyle x\)</span> 一致有界</li><li><strong>Abel:</strong> <span class="math inline">\(\displaystylef(x,t)\)</span> 在 <span class="math inline">\(\displaystyle D \times[a, \infty)\)</span> 上一致有界，且积分 <spanclass="math inline">\(\displaystyle \int_a^\infty g(x,t) dt\)</span> 在<span class="math inline">\(\displaystyle D\)</span> 上一致收敛</li></ul></li></ol><h2 id="计算-dirichlet-积分">计算 Dirichlet 积分</h2><p>利用含参变量积分的一致收敛性来计算Dirichlet 积分： <spanclass="math display">\[\displaystyle F(0) = \int_0^\infty \frac{\sin t}{t} dt\]</span> 构造一个含参积分 <span class="math display">\[\displaystyle F(x) = \int_0^\infty \frac{\sin t}{t} e^{-xt} dt\]</span></p><p>其中 <span class="math inline">\(\displaystyle x \in [0,\infty)\)</span></p><p>对于 <span class="math inline">\(\displaystyle x &gt;0\)</span>，利用 Weierstrass 判别法可以证明，可以对 <spanclass="math inline">\(\displaystyle F(x)\)</span> 在积分号下求导：</p><p><span class="math display">\[\displaystyle F&#39;(x) = -\int_0^\infty e^{-xt} \sin t \, dt =-\frac{1}{1+x^2}\]</span> 对 <span class="math inline">\(\displaystyleF&#39;(x)\)</span> 积分得到 <span class="math display">\[\displaystyle F(x) = -\arctan x + C\]</span> 由于 <span class="math inline">\(\displaystyle\lim_{x\to\infty} F(x) = 0\)</span>（可以证明），可得 <spanclass="math inline">\(\displaystyle C = \pi/2\)</span></p><p>所以对 <span class="math inline">\(\displaystyle x &gt;0\)</span>，<span class="math inline">\(\displaystyle F(x) = \pi/2 -\arctan x\)</span></p><p>最关键的一步是证明 <span class="math inline">\(\displaystyleF(x)\)</span> 在 <span class="math inline">\(\displaystyle x=0\)</span>处是连续的，这样我们就能通过取极限来求 <spanclass="math inline">\(\displaystyle F(0)\)</span>：</p><p><span class="math display">\[\displaystyle F(0) = \lim_{x \to 0^+} F(x) = \lim_{x \to 0^+} (\pi/2 -\arctan x) = \pi/2\]</span> 为了证明在 <span class="math inline">\(\displaystylex=0\)</span> 的连续性，需要证明积分： <span class="math display">\[\displaystyle \int_0^\infty \frac{\sin t}{t} e^{-xt} dt\]</span> 在 <span class="math inline">\(\displaystyle [0,\infty)\)</span> 上是一致收敛的</p><p>由于该积分不是绝对收敛的，Weierstrass 判别法失效，但我们可以使用<strong>Dirichlet 判别法</strong> 将积分写为 <spanclass="math display">\[\displaystyle \int_0^1 \dots dt + \int_1^\infty \dots dt\]</span> 第一部分是正常积分，其连续性容易证明</p><p>对于第二部分 <span class="math inline">\(\displaystyle \int_1^\inftye^{-xt} \frac{\sin t}{t} dt\)</span>，令 <spanclass="math inline">\(\displaystyle f(x,t) = e^{-xt}/t\)</span> 和 <spanclass="math inline">\(\displaystyle g(x,t) = \sin t\)</span></p><p>函数 <span class="math inline">\(\displaystyle f(x,t)\)</span> 对<span class="math inline">\(\displaystyle t \ge 1\)</span>单调递减，并一致收敛到0，而 <span class="math inline">\(\displaystyle\int_1^R \sin t \, dt\)</span> 是一致有界的</p><p>因此，根据 Dirichlet 判别法，该积分在 <spanclass="math inline">\(\displaystyle [0, \infty)\)</span>上一致收敛，从而 <span class="math inline">\(\displaystyle F(x)\)</span>在 <span class="math inline">\(\displaystyle x=0\)</span> 处连续</p><p>所以，</p><p><span class="math display">\[\displaystyle \int_0^\infty \frac{\sin t}{t} dt = \frac{\pi}{2}\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>复幂级数、复对数与Abel求和法</title>
    <link href="/2025/06/05/%E5%A4%8D%E5%B9%82%E7%BA%A7%E6%95%B0%E3%80%81%E5%A4%8D%E5%AF%B9%E6%95%B0%E4%B8%8EAbel%E6%B1%82%E5%92%8C%E6%B3%95/"/>
    <url>/2025/06/05/%E5%A4%8D%E5%B9%82%E7%BA%A7%E6%95%B0%E3%80%81%E5%A4%8D%E5%AF%B9%E6%95%B0%E4%B8%8EAbel%E6%B1%82%E5%92%8C%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="复幂级数复对数与abel求和法">复幂级数、复对数与Abel求和法</h1><h2 id="复幂级数">复幂级数</h2><p>一个中心为 <span class="math inline">\(\displaystyle a \in\mathbb{C}\)</span> 的复幂级数是指以下形式的表达式：</p><p><span class="math display">\[\displaystyle \sum_{n=0}^{\infty}a_{n}(z-a)^{n}\]</span></p><p>其中系数 <span class="math inline">\(\displaystyle a_n \in\mathbb{C}\)</span>，变量 <span class="math inline">\(\displaystyle z\in \mathbb{C}\)</span></p><p>幂级数的一个核心特性是其收敛域对于任意幂级数，其收敛域必然是以下三种情况之一：</p><ul><li>仅在中心点 <span class="math inline">\(\displaystyle z=a\)</span>收敛</li><li>在整个复平面 <span class="math inline">\(\displaystyle\mathbb{C}\)</span> 上收敛</li><li>在一个以 <span class="math inline">\(\displaystyle a\)</span>为中心的开圆盘内收敛，这个圆盘的半径被称为<strong>收敛半径 (Radius ofConvergence)</strong>，记为 <span class="math inline">\(\displaystyleR\)</span></li></ul><p>对于复幂级数 <span class="math inline">\(\displaystyle\sum_{n=0}^{\infty}a_{n}(z-a)^{n}\)</span>，存在一个唯一的 <spanclass="math inline">\(\displaystyle R \in [0,\infty]\)</span>，使得：</p><ul><li><p>当 <span class="math inline">\(\displaystyle |z-a| &lt;R\)</span> 时，级数绝对收敛</p></li><li><p>当 <span class="math inline">\(\displaystyle |z-a| &gt;R\)</span> 时，级数发散</p></li></ul><p>在边界 <span class="math inline">\(\displaystyle |z-a|=R\)</span>上，级数的收敛性需要单独讨论</p><p>收敛半径 <span class="math inline">\(\displaystyle R\)</span>可以通过 <strong>柯西-阿达马 (Cauchy-Hadamard) 公式</strong> 计算：</p><p><span class="math display">\[\displaystyle R = \frac{1}{L}, \quad \text{where} \quad L =\limsup_{n\to\infty} \sqrt[n]{|a_n|}\]</span></p><p>当极限<span class="math inline">\(\displaystyle\lim_{n\to\infty}\left| \frac{a_n}{a_{n+1}}\right|\)</span>存在时，也可以使用更简便的比值判别法计算：</p><p><span class="math display">\[\displaystyle R = \lim_{n\to\infty} \left| \frac{a_n}{a_{n+1}} \right|\]</span></p><h2 id="复幂级数的连续性与可微性">复幂级数的连续性与可微性</h2><h3 id="连续性">连续性</h3><p>一个收敛半径为 <span class="math inline">\(\displaystyleR&gt;0\)</span> 的幂级数 <span class="math display">\[\displaystyle f(z) = \sum_{n=0}^{\infty}a_{n}(z-a)^{n}\]</span> 在其收敛圆盘 <span class="math inline">\(\displaystyle B_R(a)= \{z \in \mathbb{C} : |z-a| &lt; R\}\)</span> 内定义了一个连续函数</p><p>原因在于，幂级数在任何半径为 <spanclass="math inline">\(\displaystyle R&#39; &lt; R\)</span> 的闭圆盘<span class="math inline">\(\displaystyle\overline{B_{R&#39;}(a)}\)</span> 上都是 <strong>一致收敛</strong>的</p><p>由于幂级数的部分和多项式是连续的，根据一致收敛的连续性定理，极限函数<span class="math inline">\(\displaystyle f(z)\)</span> 也是连续的</p><h3 id="复可微性-全纯性-holomorphic">复可微性 (全纯性 holomorphic)</h3><p>幂级数 <span class="math inline">\(\displaystyle f(z)\)</span>在其收敛圆盘 <span class="math inline">\(\displaystyle B_R(a)\)</span>内是复可微的（即全纯的），其导数可以通过逐项求导得到：</p><p><span class="math display">\[\displaystyle f&#39;(z) = \sum_{n=1}^{\infty} n a_n (z-a)^{n-1}\]</span></p><p>求导后得到的幂级数与原级数有相同的收敛半径 <spanclass="math inline">\(\displaystyle R\)</span></p><p>由于可以反复进行逐项求导，一个幂级数在其收敛域内是无穷次可微的 (<spanclass="math inline">\(\displaystyle C^\infty\)</span>)其 <spanclass="math inline">\(\displaystyle k\)</span> 阶导数为：</p><p><span class="math display">\[\displaystyle f^{(k)}(z) = \sum_{n=k}^{\infty} n(n-1)\cdots(n-k+1) a_n(z-a)^{n-k}\]</span></p><p>由此可得，幂级数的系数与其在中心点 <spanclass="math inline">\(\displaystyle a\)</span> 的各阶导数满足关系 <spanclass="math display">\[\displaystyle a_k = \frac{f^{(k)}(a)}{k!}\]</span> 这意味着一个幂级数就是它自身的泰勒级数</p><h2 id="复对数-the-complex-logarithm">复对数 (The ComplexLogarithm)</h2><p>复对数函数是复指数函数 <span class="math inline">\(\displaystylee^z\)</span> 的反函数</p><p>其<strong>主分支 (principal branch)</strong>定义在 <spanclass="math inline">\(\displaystyle Re(z)&gt;0\)</span>的右半平面上，为： <span class="math display">\[\displaystyle \ln z = \ln|z| + i \arg z\]</span></p><p>其中 <span class="math inline">\(\displaystyle \arg z \in (-\pi,\pi]\)</span> 是主辐角</p><p>通过柯西-黎曼方程可以证明，<span class="math inline">\(\displaystyle\ln z\)</span> 是复可微的，其导数为：</p><p><span class="math display">\[\displaystyle (\ln z)&#39; = \frac{1}{z}\]</span></p><p>在 <span class="math inline">\(\displaystyle |z-1|&lt;1\)</span>的圆盘内，<span class="math inline">\(\displaystyle \ln z\)</span>可以展开为幂级数：</p><p><span class="math display">\[\displaystyle \ln z = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{n}(z-1)^n\]</span></p><h2 id="幂级数的一些应用">幂级数的一些应用</h2><h3 id="三角函数与正弦积分">三角函数与正弦积分</h3><p>在严格的数学体系中，三角函数可以通过幂级数来定义</p><p>例如，基于欧拉公式 <span class="math inline">\(\displaystyle e^{ix} =\cos x + i \sin x\)</span>，可以得到： <span class="math display">\[\displaystyle \cos x = \sum_{k=0}^{\infty} \frac{(-1)^k}{(2k)!}x^{2k}\]</span></p><p><span class="math display">\[\sin x = \sum_{k=0}^{\infty} \frac{(-1)^k}{(2k+1)!}x^{2k+1}\]</span></p><p>这两个级数的收敛半径均为 <span class="math inline">\(\displaystyleR=\infty\)</span></p><p>利用逐项求导法则，可以轻松验证它们的基本性质，如 <spanclass="math inline">\(\displaystyle (\sin x)&#39; = \cos x\)</span></p><p>对于 <strong>正弦积分函数 (Sine Integral)</strong> <spanclass="math inline">\(\displaystyle Si(x)\)</span>，</p><p>可以利用 <span class="math inline">\(\displaystyle \sin t\)</span>的幂级数展开，并通过逐项积分得到其自身的幂级数表示： <spanclass="math display">\[\begin{align}\displaystyle Si(x) &amp;= \int_0^x \frac{\sin t}{t} dt \\&amp;= \int_0^x \sum_{n=0}^{\infty} (-1)^n \frac{t^{2n}}{(2n+1)!} dt \\&amp;= \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!(2n+1)}\end{align}\]</span></p><h3 id="生成函数与平方和">生成函数与平方和</h3><p>幂级数是组合数学中强大的工具，其应用之一是求数列的和</p><p>例如，求前 <span class="math inline">\(\displaystyle n\)</span>个正整数的平方和 <span class="math inline">\(\displaystyle s_n =\sum_{k=1}^n k^2\)</span></p><p>从几何级数出发，我们有 <span class="math display">\[\displaystyle \sum_{n=0}^\infty x^n = \frac{1}{1-x}\]</span> 通过微分算子 <span class="math inline">\(\displaystyle x\frac{d}{dx}\)</span> 作用于系数，可以得到 <span class="math display">\[\displaystyle \sum_{n=1}^\infty n^2 x^n = \frac{x+x^2}{(1-x)^3}\]</span> 数列 <span class="math inline">\(\displaystyle (s_n)\)</span>的生成函数是 <span class="math display">\[\displaystyle \sum_{n=1}^\infty s_n x^n = \frac{1}{1-x}\sum_{n=1}^\infty n^2 x^n = \frac{x+x^2}{(1-x)^4}\]</span> 将右侧函数展开为幂级数并比较系数，即可得到 <spanclass="math inline">\(\displaystyle s_n\)</span> 的封闭形式：</p><p><span class="math display">\[\displaystyle s_n = \binom{n+2}{3} + \binom{n+1}{3} =\frac{n(n+1)(2n+1)}{6}\]</span></p><h2 id="abel求和法">Abel求和法</h2><p>阿贝尔求和法，又称为偏和分或分部求和法，可以看作是离散形式的“分部积分法”</p><p>它主要用于处理非绝对收敛的级数，特别是证明级数的（一致）收敛性</p><p>考虑两个函数序列 <span class="math inline">\((\displaystylef_n(x))\)</span> 和 <span class="math inline">\((\displaystyleg_n(x))\)</span></p><p>设 <span class="math inline">\(\displaystyle G_n(x)\)</span> 是 <spanclass="math inline">\(\displaystyle g_k(x)\)</span> 的部分和，即 <spanclass="math display">\[\displaystyle G_n(x) = \sum_{k=m}^n g_k(x)\]</span> 通过代入 <span class="math inline">\(\displaystyle g_k = G_k -G_{k-1}\)</span> 并重新整理求和项，可以得到以下恒等式</p><p><span class="math display">\[\displaystyle \sum_{k=m}^{n}f_{k}(x)g_{k}(x) = f_{n}(x)G_{n}(x) -f_{m}(x)G_{m-1}(x) + \sum_{k=m}^{n-1}[f_{k}(x)-f_{k+1}(x)]G_{k}(x)\]</span></p><h3 id="三角级数">三角级数</h3><p>考虑幂级数 <span class="math display">\[\displaystyle f(z) = \sum_{n=1}^{\infty} \frac{z^n}{n}\]</span> 其收敛半径为 <span class="math inline">\(\displaystyleR=1\)</span></p><p>在收敛域 <span class="math inline">\(\displaystyle |z|&lt;1\)</span>内，我们知道 <span class="math display">\[\displaystyle f(z) = -\ln(1-z)\]</span> 我们的目标是研究当 <span class="math inline">\(\displaystylez\)</span> 趋近于单位圆边界 <span class="math inline">\(\displaystyleS^1 = \{z \in \mathbb{C} : |z|=1\}\)</span> 时级数的行为</p><p>令 <span class="math inline">\(\displaystylez=e^{ix}\)</span>，则级数变为： <span class="math display">\[\displaystyle f(e^{ix}) = \sum_{n=1}^{\infty} \frac{e^{inx}}{n} =\sum_{n=1}^{\infty} \frac{\cos(nx)}{n} + i \sum_{n=1}^{\infty}\frac{\sin(nx)}{n}\]</span></p><p>当 <span class="math inline">\(\displaystyle x=0\)</span> (即 <spanclass="math inline">\(\displaystyle z=1\)</span>)时，级数为调和级数，发散</p><p>我们需要证明级数在单位圆上其他点处是一致收敛的</p><p>记集合 <span class="math display">\[\displaystyle D_r = \{z \in \mathbb{C} : |z| \le 1 \land|z-1| \ger\}\text{ where }r&gt;0\]</span> 我们首先对级数的尾部进行变换</p><p>设 <span class="math inline">\(\displaystyle s_n(z) = \sum_{k=1}^{n}z^k\)</span> 为几何级数的部分和</p><p>对于 <span class="math inline">\(\displaystyle m &lt;n\)</span>，我们有： <span class="math display">\[\displaystyle \sum_{k=m+1}^{n} \frac{z^k}{k} = \sum_{k=m+1}^{n}\frac{s_k(z) - s_{k-1}(z)}{k}\]</span> 利用Abel求和法，我们得到： <span class="math display">\[\displaystyle \sum_{k=m+1}^{n} \frac{s_k(z) - s_{k-1}(z)}{k} =\frac{s_n(z)}{n} - \frac{s_m(z)}{m+1} + \sum_{k=m+1}^{n-1} s_k(z) \left(\frac{1}{k} - \frac{1}{k+1} \right)\]</span> 接下来，我们需要证明部分和 <spanclass="math inline">\(\displaystyle s_n(z)\)</span> 在我们定义的区域<span class="math inline">\(\displaystyle D_r\)</span>上是一致有界的</p><p>对于 <span class="math inline">\(\displaystyle z \ne1\)</span>，我们有等比数列求和公式： <span class="math display">\[\displaystyle s_n(z) = \sum_{k=1}^{n} z^k = \frac{z(1-z^n)}{1-z}\]</span> 对于任意 <span class="math inline">\(\displaystyle z \inD_r\)</span>，我们有 <span class="math inline">\(\displaystyle |z| \le1\)</span> 且 <span class="math inline">\(\displaystyle |z-1| \ger\)</span></p><p>因此： <span class="math display">\[\begin{align}\displaystyle |s_n(z)| &amp;= \left| \frac{z(1-z^n)}{1-z} \right| =\frac{|z||1-z^n|}{|1-z|}\\ &amp;\le \frac{1 \cdot (|1| + |-z^n|)}{|1-z|}\le \frac{1+1}{r} \\&amp;= \frac{2}{r}\end{align}\]</span> 令 <span class="math inline">\(\displaystyle M =\frac{2}{r}\)</span>，我们便得到了一个不依赖于 <spanclass="math inline">\(\displaystyle n\)</span> 和 <spanclass="math inline">\(\displaystyle z\)</span> 的一致界<spanclass="math inline">\(\displaystyle |s_n(z)| \le M\)</span> 对所有 <spanclass="math inline">\(\displaystyle z \in D_r\)</span> 和所有 <spanclass="math inline">\(\displaystyle n \in \mathbb{N}\)</span> 均成立</p><p>现在我们可以估计级数尾部的大小，利用第1步的结果和第2步的界 <spanclass="math inline">\(\displaystyle M\)</span>：</p><p><span class="math display">\[\begin{align}\displaystyle \left| \sum_{k=m}^{n} \frac{z^k}{k} \right|&amp;\le \frac{|s_{m-1}(z)|}{m} + \sum_{k=m}^{n-1} |s_k(z)|\left(\frac{1}{k} - \frac{1}{k+1}\right) + \frac{|s_n(z)|}{n}\\\displaystyle &amp;\le \frac{M}{m} + M \sum_{k=m}^{n-1}\left(\frac{1}{k} - \frac{1}{k+1}\right) + \frac{M}{n}\end{align}\]</span> 注意到中间的和是一个伸缩级数 (telescoping sum)： <spanclass="math display">\[\displaystyle \sum_{k=m}^{n-1} \left(\frac{1}{k} - \frac{1}{k+1}\right)= \left(\frac{1}{m} - \frac{1}{m+1}\right) + \dots + \left(\frac{1}{n-1}- \frac{1}{n}\right) = \frac{1}{m} - \frac{1}{n}\]</span> 因此，估计式变为： <span class="math display">\[\displaystyle \left| \sum_{k=m}^{n} \frac{z^k}{k} \right| \le\frac{M}{m} + M\left(\frac{1}{m} - \frac{1}{n}\right) + \frac{M}{n} =\frac{2M}{m}\]</span> 对于任意给定的 <span class="math inline">\(\displaystyle\epsilon &gt; 0\)</span>，我们总能选择一个足够大的 <spanclass="math inline">\(\displaystyle N\)</span>，使得当 <spanclass="math inline">\(\displaystyle m &gt; N\)</span> 时，<spanclass="math inline">\(\displaystyle \frac{2M}{m} &lt;\epsilon\)</span></p><p>具体来说，我们可以取 <span class="math inline">\(\displaystyle N =\lceil \frac{2M}{\epsilon} \rceil = \lceil \frac{4}{r\epsilon}\rceil\)</span></p><p>这样，对于任意 <span class="math inline">\(\displaystyle n \ge m &gt;N\)</span>，我们都有 <span class="math inline">\(\displaystyle \left|\sum_{k=m}^{n} \frac{z^k}{k} \right| &lt; \epsilon\)</span> 对所有 <spanclass="math inline">\(\displaystyle z \in D_r\)</span> 成立</p><p>根据柯西一致收敛准则，级数 <span class="math inline">\(\displaystyle\sum_{n=1}^{\infty} \frac{z^n}{n}\)</span> 在区域 <spanclass="math inline">\(\displaystyle D_r = \{z \in \mathbb{C} : |z| \le1, |z-1| \ge r\}\)</span> 上是一致收敛的</p><p>由于 <span class="math inline">\(\displaystyle r&gt;0\)</span>可以是任意小的正数，这表明级数在任何不包含 <spanclass="math inline">\(\displaystyle z=1\)</span>点的闭单位圆盘的紧子集上都是一致收敛的</p><p>这一结论（也是<strong>狄利克雷一致收敛判别法</strong>的一个应用）非常重要，因为它允许我们应用连续性定理，从而证明<span class="math inline">\(\displaystyle f(z)\)</span> 在其收敛域 <spanclass="math inline">\(\displaystyle \overline{B_1(0)} \setminus\{1\}\)</span> 上是一个连续函数</p><p>由于级数在 <span class="math inline">\(\displaystyle S^1 \setminus\{1\}\)</span> 上一致收敛，其和函数 <spanclass="math inline">\(\displaystyle f(z)\)</span> 在此区域连续</p><p>因此，我们可以通过取径向极限来计算边界上的值： <spanclass="math display">\[\displaystyle f(e^{i\phi}) = \lim_{r \to 1^-} f(re^{i\phi}) = \lim_{r\to 1^-} [-\ln(1-re^{i\phi})]\]</span></p><p>计算此极限并分离实部和虚部，可得以下著名的三角级数求和公式（对于<span class="math inline">\(\displaystyle 0 &lt; x &lt;2\pi\)</span>）：</p><p><span class="math display">\[\displaystyle \sum_{n=1}^{\infty} \frac{\cos(nx)}{n} =-\ln\left(2\sin\frac{x}{2}\right)\]</span></p><p><span class="math display">\[\displaystyle \sum_{n=1}^{\infty} \frac{\sin(nx)}{n} = \frac{\pi - x}{2}\]</span></p><h3 id="计算-displaystyle-zeta2-sum_n1infty-frac1n2">计算 <spanclass="math inline">\(\displaystyle \zeta(2) = \sum_{n=1}^{\infty}\frac{1}{n^2}\)</span></h3><p>利用上述结果，可以进一步计算其他重要级数考虑函数 <spanclass="math inline">\(\displaystyle g(x) = \sum_{n=1}^{\infty}\frac{\cos(nx)}{n^2}\)</span></p><p>该级数在整个 <span class="math inline">\(\displaystyle\mathbb{R}\)</span> 上一致收敛（由 Weierstrass 判别法可知）</p><p>其导数级数为 <span class="math display">\[\displaystyle -\sum \frac{\sin(nx)}{n}\]</span> 根据微分定理，在 <span class="math inline">\(\displaystyle (0,2\pi)\)</span> 内，我们有：</p><p><span class="math display">\[\displaystyle g&#39;(x) = - \sum_{n=1}^{\infty} \frac{\sin(nx)}{n} = -\left(\frac{\pi-x}{2}\right) = \frac{x-\pi}{2}\]</span></p><p>对 <span class="math inline">\(\displaystyle g&#39;(x)\)</span>积分可得 <span class="math inline">\(\displaystyle g(x) =\frac{(x-\pi)^2}{4} + C\)</span>通过计算 <spanclass="math inline">\(\displaystyle \int_0^{2\pi} g(x) dx =0\)</span>，可以定出常数 <span class="math inline">\(\displaystyle C =-\frac{\pi^2}{12}\)</span></p><p>最终得到：</p><p><span class="math display">\[\displaystyle \sum_{n=1}^{\infty} \frac{\cos(nx)}{n^2} =\frac{(x-\pi)^2}{4} - \frac{\pi^2}{12}, \quad \text{for } 0 \le x \le2\pi\]</span></p><p>将 <span class="math inline">\(\displaystyle x=0\)</span>代入上式，即可得到巴塞尔问题的解：</p><p><span class="math display">\[\displaystyle \sum_{n=1}^{\infty} \frac{1}{n^2} = \frac{(0-\pi)^2}{4} -\frac{\pi^2}{12} = \frac{\pi^2}{4} - \frac{\pi^2}{12} = \frac{\pi^2}{6}\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>复可微性与柯西-黎曼方程</title>
    <link href="/2025/06/05/%E5%A4%8D%E5%8F%AF%E5%BE%AE%E6%80%A7%E4%B8%8E%E6%9F%AF%E8%A5%BF-%E9%BB%8E%E6%9B%BC%E6%96%B9%E7%A8%8B/"/>
    <url>/2025/06/05/%E5%A4%8D%E5%8F%AF%E5%BE%AE%E6%80%A7%E4%B8%8E%E6%9F%AF%E8%A5%BF-%E9%BB%8E%E6%9B%BC%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="复可微性与柯西-黎曼方程">复可微性与柯西-黎曼方程</h1><h2 id="从复变函数到实变函数">从复变函数到实变函数</h2><p>要理解复变函数的可微性，我们首先需要建立它与我们熟悉的实变微积分之间的联系</p><p>一个复变函数 <span class="math inline">\(\displaystyle f(z)\)</span>可以被看作是将其定义域 <span class="math inline">\(\displaystyle D\subseteq \mathbb{C}\)</span> 中的每个复数 <spanclass="math inline">\(\displaystyle z\)</span> 映射到另一个复数</p><p>如果我们将 <span class="math inline">\(\displaystyle z\)</span>写成<span class="math inline">\(\displaystyle z =x+iy\)</span>，那么函数值也通常是一个复数</p><p>我们可以将其写成如下形式： <span class="math display">\[\displaystyle f(z) = u(x,y) + i v(x,y)\]</span> 这里：</p><ul><li><p><span class="math inline">\(\displaystyle u(x,y) =\text{Re}(f(z))\)</span> 是函数的 <strong>实部</strong></p></li><li><p><span class="math inline">\(\displaystyle v(x,y) =\text{Im}(f(z))\)</span> 是函数的 <strong>虚部</strong></p></li></ul><p>这样，一个复变函数 <span class="math inline">\(\displaystylef\)</span> 就对应了一个从 <span class="math inline">\(\displaystyle\mathbb{R}^2\)</span> 到 <span class="math inline">\(\displaystyle\mathbb{R}^2\)</span> 的向量值函数 <spanclass="math inline">\(\displaystyle (u(x,y), v(x,y))\)</span></p><h2 id="复可微性的定义">复可微性的定义</h2><p>一个函数 <span class="math inline">\(\displaystyle f\)</span> 在点<span class="math inline">\(\displaystyle z\)</span> <strong>复可微(complex differentiable)</strong>，是指以下极限存在：</p><p><span class="math display">\[\displaystyle f&#39;(z) = \lim_{h\to 0} \frac{f(z+h) - f(z)}{h}\]</span></p><p>这里的关键在于，<span class="math inline">\(\displaystyle h\)</span>是一个复数，<span class="math inline">\(\displaystyle h \to 0\)</span>意味着 <span class="math inline">\(\displaystyle h\)</span>可以从复平面上的 <strong>任何方向</strong> 趋近于零</p><p>无论路径如何，上述极限都必须存在且为同一个值</p><p>这比实变函数中仅从左侧或右侧趋近要严格得多</p><h2 id="柯西-黎曼方程">柯西-黎曼方程</h2><p>函数 <span class="math display">\[\displaystyle f(z) = u(x,y) + i v(x,y)\]</span> 在点 <span class="math inline">\(\displaystylez=(x,y)\)</span> 复可微的 <strong>充要条件</strong> 是：</p><ul><li><p>函数 <span class="math inline">\(\displaystyle u\)</span> 和<span class="math inline">\(\displaystyle v\)</span> 在点 <spanclass="math inline">\(\displaystyle (x,y)\)</span> 是实可微的</p></li><li><p>并且它们的偏导数满足<strong>柯西-黎曼方程 (Cauchy-RiemannEquations)</strong>：</p></li></ul><p><span class="math display">\[\displaystyle \frac{\partial u}{\partial x} = \frac{\partial v}{\partialy} \quad \land \quad \frac{\partial u}{\partial y} = - \frac{\partialv}{\partial x}\]</span></p><p>如果满足这些条件，那么复导数可以表示为：</p><p><span class="math display">\[\displaystyle f&#39;(z) = \frac{\partial u}{\partial x} + i\frac{\partial v}{\partial x}\]</span></p><h3 id="证明"><strong>证明</strong></h3><p>假设 <span class="math inline">\(\displaystyle f\)</span> 在 <spanclass="math inline">\(\displaystyle z\)</span> 点复可微，且其导数为<span class="math display">\[\displaystyle f&#39;(z) = a+bi\]</span> 根据导数定义，当 <span class="math inline">\(\displaystyleh=h_1+ih_2\)</span> 很小时，我们有 <span class="math display">\[\displaystyle f(z+h) - f(z) \approx f&#39;(z)h\]</span> 将复数形式展开： <span class="math display">\[\displaystyle f&#39;(z)h = (a+bi)(h_1+ih_2) = (ah_1 - bh_2) + i(bh_1 +ah_2)\]</span> 另一方面，<span class="math inline">\(\displaystyle f(z+h) -f(z)\)</span> 的实部是 <span class="math display">\[\displaystyle u(x+h_1, y+h_2) - u(x,y)\]</span> 虚部是 <span class="math display">\[\displaystyle v(x+h_1, y+h_2) - v(x,y)\]</span> 通过比较实部和虚部，并利用实可微的定义，我们可以得到：</p><p><span class="math display">\[\displaystyle \frac{\partial u}{\partial x} = a\quad\land\quad\displaystyle \frac{\partial u}{\partial y} = -b\]</span></p><p><span class="math display">\[\displaystyle \frac{\partial v}{\partial x} = b\quad\land\quad\displaystyle \frac{\partial v}{\partial y} = a\]</span></p><p>从这些关系中，我们可以得到 <span class="math inline">\(\displaystyleu_x = v_y\)</span> 和 <span class="math inline">\(\displaystyle u_y =-v_x\)</span> 成立</p><p>反过来，如果 <span class="math inline">\(\displaystyle u\)</span> 和<span class="math inline">\(\displaystyle v\)</span>是实可微的，且它们的偏导数满足柯西-黎曼方程</p><p>通过类似的代数推导，可以证明 <spanclass="math inline">\(\displaystyle \lim_{h\to 0} \frac{f(z+h) -f(z)}{h}\)</span> 的极限存在且唯一，其值为 <spanclass="math inline">\(\displaystyle u_x + i v_x\)</span></p><h2 id="柯西-黎曼方程的几何意义">柯西-黎曼方程的几何意义</h2><p>柯西-黎曼方程有一个非常优美的几何意义</p><p>一个从 <span class="math inline">\(\displaystyle\mathbb{R}^2\)</span> 到 <span class="math inline">\(\displaystyle\mathbb{R}^2\)</span> 的映射，其局部线性近似由<strong>雅可比矩阵(Jacobian matrix)</strong>描述</p><p>对于函数 <span class="math inline">\(\displaystyle f\)</span>对应的映射 <span class="math inline">\(\displaystyle(u,v)\)</span>，其雅可比矩阵为： <span class="math display">\[\displaystyle J_f = \begin{pmatrix} u_x &amp; u_y \\ v_x &amp; v_y\end{pmatrix}\]</span></p><p>如果柯西-黎曼方程成立，这个矩阵就变成了：</p><p><span class="math display">\[\displaystyle J_f = \begin{pmatrix} u_x &amp; -v_x \\ v_x &amp; u_x\end{pmatrix}\]</span></p><p>这个矩阵的形式正是一个<strong>缩放旋转矩阵</strong></p><p>它表示复可微函数在局部上会将微小的图形进行旋转和均匀缩放，但不会产生剪切或形变</p><p>这也是为什么复可微函数（或称全纯函数）具有保持角度不变（保角性conformal）等优良的几何性质</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>一致收敛与Weierstrass判别法</title>
    <link href="/2025/06/04/%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E4%B8%8EWeierstrass%E5%88%A4%E5%88%AB%E6%B3%95/"/>
    <url>/2025/06/04/%E4%B8%80%E8%87%B4%E6%94%B6%E6%95%9B%E4%B8%8EWeierstrass%E5%88%A4%E5%88%AB%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="一致收敛与weierstrass判别法">一致收敛与Weierstrass判别法</h1><p>在微积分中，我们经常遇到函数序列或函数项级数的极限</p><p>一个自然的问题是：极限函数是否继承了序列中函数所具有的性质？</p><p>例如，如果序列中的每个函数都是连续的、可微的或可积的，那么它们的极限函数是否也具有相同的性质？</p><p>对于普通的<strong>逐点收敛 (point-wiseconvergence)</strong>，答案通常是否定的</p><p>这意味着，即使一个函数序列 <spanclass="math inline">\(f_n(x)\)</span> 中的每个 <spanclass="math inline">\(f_n(x)\)</span>都很“好”（比如连续），其逐点收敛的极限函数 <spanclass="math inline">\(f(x)\)</span> 却可能不那么“好”（比如不连续）</p><p><strong>一致收敛 (uniform convergence)</strong>的概念正是为了解决这一问题而引入的</p><p>它是一种更强的收敛方式，能够保证在一定条件下，极限函数可以“继承”函数序列的良好性质，如连续性、可微性和可积性</p><p>理解一致收敛对于常微分方程解的存在性定理、傅里叶级数理论以及实分析中的许多重要内容至关重要</p><h2 id="三个反例">三个反例</h2><p>以下三个例子展示了函数序列虽然逐点收敛，但其极限函数可能不具备原序列函数的良好性质，或者其导数序列不一致收敛</p><p>这些例子反驳了19世纪初普遍存在的一种朴素看法，即连续/可微/可积函数的逐点极限函数也会继承相应的性质</p><h4 id="反例1连续性">反例1：连续性</h4><p>考虑函数序列 <span class="math display">\[\displaystyle f_n(x) = x^n\]</span> 其中 <span class="math inline">\(\displaystyle x \in[0,1]\)</span></p><p>序列中的每一个函数 <span class="math inline">\(\displaystylef_n(x)\)</span> 在 <span class="math inline">\(\displaystyle[0,1]\)</span> 上都是连续的</p><p>该序列逐点收敛于函数 <span class="math display">\[\displaystyle f(x) = \lim_{n\to\infty} x^n = \begin{cases} 0 &amp;\text{if } 0 \le x &lt; 1 \\ 1 &amp; \text{if } x = 1 \end{cases}\]</span> 然而，极限函数 <span class="math inline">\(\displaystylef(x)\)</span> 在 <span class="math inline">\(\displaystyle x=1\)</span>处不连续</p><h4 id="反例2可微性">反例2：可微性</h4><p>考虑函数序列 <span class="math display">\[\displaystyle g_n(x) = \frac{\sin(nx)}{\sqrt{n}}\]</span> 其中 <span class="math inline">\(\displaystyle x \in [0,2\pi]\)</span></p><p>该序列逐点收敛（甚至一致收敛）到 <spanclass="math inline">\(\displaystyle g(x) \equiv0\)</span>，这是一个处处可微的函数，且 <spanclass="math inline">\(\displaystyle g&#39;(x) \equiv 0\)</span></p><p>但是，其导数序列为 <span class="math display">\[\displaystyle g_n&#39;(x) = \sqrt{n}\cos(nx)\]</span> 该导数序列对于任何 <span class="math inline">\(\displaystyle x\in [0, 2\pi]\)</span>，极限 <span class="math inline">\(\displaystyle\lim_{n\to\infty} g_n&#39;(x)\)</span> 都不（正常）存在</p><h4 id="反例3可积性">反例3：可积性</h4><p>考虑定义在 <span class="math inline">\(\displaystyle [0,1]\)</span>上的函数序列 <span class="math inline">\(\displaystyle h_n(x)\)</span>：<span class="math display">\[\displaystyle h_n(x) = \begin{cases} 2n^2x &amp; \text{if } 0 \le x \le\frac{1}{2n} \\ 2n - 2n^2x &amp; \text{if } \frac{1}{2n} \le x \le\frac{1}{n} \\ 0 &amp; \text{if } \frac{1}{n} \le x \le 1 \end{cases}\]</span> <span class="math inline">\(\displaystyle h_n(x)\)</span>的图像与x轴构成一个底为 <span class="math inline">\(\displaystyle1/n\)</span>、高为 <span class="math inline">\(\displaystyle n\)</span>的三角形（顶点为 <span class="math inline">\(\displaystyle (0,0),(\frac{1}{2n}, n), (\frac{1}{n}, 0)\)</span>）</p><p>并且 <span class="math inline">\(\displaystyle h_n(x)\)</span> 在<span class="math inline">\(\displaystyle [1/n, 1]\)</span>上为0，该序列逐点收敛到 <span class="math inline">\(\displaystyle h(x)\equiv 0\)</span></p><p><span class="math inline">\(\displaystyle h_n(x)\)</span>图像下的面积为 <span class="math display">\[\displaystyle \int_0^1 h_n(x)dx = \frac{1}{2} \cdot \frac{1}{n} \cdot n= \frac{1}{2}\]</span> 对所有 <span class="math inline">\(\displaystyle n\)</span>成立，因此， <span class="math display">\[\displaystyle \lim_{n\to\infty} \int_0^1 h_n(x)dx = \frac{1}{2}\]</span> 但是 <span class="math display">\[\displaystyle \int_0^1 \lim_{n\to\infty} h_n(x)dx = \int_0^1 0 dx = 0\]</span></p><p>所以， <span class="math display">\[\displaystyle \lim_{n\to\infty} \int_0^1 h_n(x)dx \neq \int_0^1\lim_{n\to\infty} h_n(x)dx\]</span></p><h2 id="逐点收敛与一致收敛">逐点收敛与一致收敛</h2><p>设 <span class="math inline">\(\displaystyle I \subseteq\mathbb{R}\)</span> 为一个区间，<spanclass="math inline">\(\displaystyle (f_n)_{n=0}^{\infty}\)</span>为定义在 <span class="math inline">\(\displaystyle I\)</span>上的函数序列 <span class="math inline">\(\displaystyle f_n: I \to\mathbb{R}\)</span></p><p><strong>逐点收敛 (Point-wise Convergence)</strong>: 如果对于每一个<span class="math inline">\(\displaystyle x \in I\)</span>，序列 <spanclass="math inline">\(\displaystyle(f_n(x))\)</span>（这是一个普通的实数序列）都收敛，则称函数序列 <spanclass="math inline">\(\displaystyle (f_n)\)</span> 在 <spanclass="math inline">\(\displaystyle I\)</span> 上逐点收敛 此时，极限函数<span class="math inline">\(\displaystyle f(x) = \lim_{n\to\infty}f_n(x)\)</span> 定义了一个函数 <span class="math inline">\(\displaystylef: I \to \mathbb{R}\)</span></p><p><strong>一致收敛 (Uniform Convergence)</strong>: 如果函数序列 <spanclass="math inline">\(\displaystyle (f_n)\)</span> 逐点收敛于 <spanclass="math inline">\(\displaystyle f\)</span>，并且对于任意给定的 <spanclass="math inline">\(\displaystyle \epsilon &gt;0\)</span>，都存在一个<strong>与 <spanclass="math inline">\(\displaystyle x\)</span> 无关的</strong>自然数<span class="math inline">\(\displaystyle N \in\mathbb{N}\)</span>，使得对于所有 <spanclass="math inline">\(\displaystyle n &gt; N\)</span> 和所有 <spanclass="math inline">\(\displaystyle x \in I\)</span>，都有 <spanclass="math inline">\(\displaystyle |f(x) - f_n(x)| &lt;\epsilon\)</span></p><p>两者的关键区别在于，一致收敛要求的 <spanclass="math inline">\(\displaystyle N = N_\epsilon\)</span> 仅依赖于<span class="math inline">\(\displaystyle \epsilon\)</span>，不依赖于<span class="math inline">\(\displaystyle x \inI\)</span>；而逐点收敛允许 <span class="math inline">\(\displaystyle N =N_{\epsilon, x}\)</span> 依赖于 <spanclass="math inline">\(\displaystyle x\)</span> 和 <spanclass="math inline">\(\displaystyle \epsilon\)</span></p><p><span class="math inline">\(\displaystyle (f_n)\)</span> 一致收敛于<span class="math inline">\(\displaystyle f\)</span> 等价于：对于任意<span class="math inline">\(\displaystyle \epsilon &gt;0\)</span>，除有限多项外，<span class="math inline">\(\displaystylef_n\)</span> 的图像都位于 <span class="math inline">\(\displaystylef\)</span> 图像周围宽度为 <span class="math inline">\(\displaystyle2\epsilon\)</span> 的带状区域内。这是一致收敛的几何解释</p><h3 id="一致收敛的范数观点">一致收敛的范数观点</h3><p>设 <span class="math display">\[\displaystyle d_\infty(f,g) = \sup_{x \in I} \{|f(x)-g(x)|\}\]</span> 则 <span class="math inline">\(\displaystyle (f_n)\)</span>一致收敛于 <span class="math inline">\(\displaystyle f\)</span> 等价于当<span class="math inline">\(\displaystyle n \to \infty\)</span> 时<spanclass="math inline">\(\displaystyle d_\infty(f, f_n) \to 0\)</span></p><p>所以说，一致收敛可以看作是在广义度量空间 <spanclass="math inline">\(\displaystyle (\mathbb{R}^I, d_\infty)\)</span>中的普通收敛</p><p><span class="math inline">\(\displaystyled_\infty(f,g)\)</span>称为<strong>一致收敛度量或 <spanclass="math inline">\(\displaystyleL^\infty\)</span>-度量（metric）</strong></p><h3 id="推广">推广</h3><p>一致收敛的定义可以推广到定义域为任意集合 <spanclass="math inline">\(\displaystyle X\)</span> 的函数 <spanclass="math inline">\(\displaystyle f_n: X \to \mathbb{R}\)</span></p><p>值域也可以推广到 <span class="math inline">\(\displaystyle\mathbb{R}^k\)</span> 或任意带有距离函数 <spanclass="math inline">\(\displaystyle d: M \times M \to\mathbb{R}\)</span> 的度量空间 <span class="math inline">\(\displaystyle(M,d)\)</span></p><p>在定义收敛时，使用 <span class="math inline">\(\displaystyle&lt;\)</span> 或 <span class="math inline">\(\displaystyle \le\)</span>没有本质区别</p><h2 id="一致收敛的三个重要定理">一致收敛的三个重要定理</h2><p>开头我们提到过，引入一致收敛概念的目的是为了避免出现之前的“反例”情况</p><p>当要求函数序列 <span class="math inline">\(\displaystyle(f_n)\)</span> 和/或其导数序列 <span class="math inline">\(\displaystyle(f_n&#39;)\)</span>一致收敛时，极限函数便可以继承原函数的连续性、可微性、可积性</p><h3 id="连续性定理">连续性定理</h3><p>如果序列中的所有函数 <span class="math inline">\(\displaystylef_n\)</span> 都在点 <span class="math inline">\(\displaystyle x_0 \inI\)</span> 处连续，并且序列 <span class="math inline">\(\displaystyle(f_n)\)</span> 在 <span class="math inline">\(\displaystyle I\)</span>上一致收敛于函数 <span class="math inline">\(\displaystyle f(x) =\lim_{n\to\infty} f_n(x)\)</span>，则极限函数 <spanclass="math inline">\(\displaystyle f\)</span> 在点 <spanclass="math inline">\(\displaystyle x_0\)</span> 处也连续</p><p>特别地，一致收敛的连续函数序列的极限函数本身也是连续的</p><p><strong>证明：</strong></p><p>给定 <span class="math inline">\(\displaystyle \epsilon &gt;0\)</span>，由 <span class="math inline">\(\displaystyle (f_n)\)</span>一致收敛于 <span class="math inline">\(\displaystyle f\)</span>，存在<span class="math inline">\(\displaystyle N \in \mathbb{N}\)</span>使得对所有 <span class="math inline">\(\displaystyle n &gt; N\)</span>和 <span class="math inline">\(\displaystyle x \in I\)</span>，有 <spanclass="math display">\[\displaystyle |f(x) - f_n(x)| &lt; \epsilon/3\]</span> 特别地，对 <span class="math inline">\(\displaystylen=N+1\)</span> 成立</p><p>因为 <span class="math inline">\(\displaystyle f_{N+1}\)</span> 在<span class="math inline">\(\displaystyle x_0\)</span> 处连续，存在<span class="math inline">\(\displaystyle \delta &gt; 0\)</span>使得对所有 <span class="math inline">\(\displaystyle x \in I\)</span> 且<span class="math inline">\(\displaystyle |x-x_0| &lt;\delta\)</span>，有 <span class="math display">\[\displaystyle |f_{N+1}(x) - f_{N+1}(x_0)| &lt; \epsilon/3\]</span> 利用三角不等式： <span class="math display">\[\begin{align}\displaystyle |f(x) - f(x_0)| &amp;\le  |f(x) - f_{N+1}(x)| +|f_{N+1}(x) - f_{N+1}(x_0)| + |f_{N+1}(x_0) - f(x_0)| \\ &amp;&lt;\frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} \\&amp;=\epsilon\end{align}\]</span></p><p>这就证明了 <span class="math inline">\(\displaystyle f\)</span> 在<span class="math inline">\(\displaystyle x_0\)</span> 处连续</p><h3 id="微分定理">微分定理</h3><p>如果序列中的所有函数 <span class="math inline">\(\displaystylef_n\)</span> 都是 <span class="math inline">\(\displaystyle C^1\)</span>函数（即一阶导数连续），序列 <span class="math inline">\(\displaystyle(f_n)\)</span> 在 <span class="math inline">\(\displaystyle I\)</span>上逐点收敛于函数 <span class="math inline">\(\displaystyle f(x) =\lim_{n\to\infty} f_n(x)\)</span>，</p><p>并且导数序列 <span class="math inline">\(\displaystyle(f_n&#39;)\)</span> 在 <span class="math inline">\(\displaystyleI\)</span> 上一致收敛于函数 <span class="math inline">\(\displaystyleg(x)\)</span>，则极限函数 <span class="math inline">\(\displaystylef\)</span> 也是 <span class="math inline">\(\displaystyle C^1\)</span>函数，并且其导数满足 <span class="math display">\[\displaystyle f&#39;(x) = g(x) = \lim_{n\to\infty} f_n&#39;(x)\]</span> <strong>证明：</strong></p><p>对任意 <span class="math inline">\(\displaystyle a \inI\)</span>，由微积分基本定理，有 <span class="math display">\[\displaystyle f_n(x) = f_n(a) + \int_a^x f_n&#39;(t)dt\]</span> 由于 <span class="math inline">\(\displaystyle(f_n&#39;)\)</span> 一致收敛于 <span class="math inline">\(\displaystyleg\)</span>，由连续性定理可知 <span class="math inline">\(\displaystyleg\)</span> 是连续的</p><p>可以选择一个可积的界 <span class="math display">\[\displaystyle \Phi(t) = 1 + |g(t)|\]</span> 来约束 <span class="math inline">\(\displaystyle(f_n&#39;)\)</span></p><p>应用<strong>勒贝格有界收敛定理（Lebesgue’s Bounded ConvergenceTheorem）</strong>，可得 <span class="math display">\[\displaystyle \lim_{n\to\infty} \int_a^x f_n&#39;(t)dt = \int_a^x g(t)dt\]</span> 在第一步的等式两边取极限 <spanclass="math inline">\(\displaystyle n \to \infty\)</span>，得到 <spanclass="math display">\[\displaystyle f(x) = f(a) + \int_a^x g(t)dt\]</span> 再次应用微积分基本定理，可知 <spanclass="math inline">\(\displaystyle f\)</span> 可微且 <spanclass="math display">\[\displaystyle f&#39;(x) = g(x) = \lim_{n\to\infty} f_n&#39;(x)\]</span> 由于 <span class="math inline">\(\displaystyle g\)</span>是连续的，所以 <span class="math inline">\(\displaystyle f\)</span> 是<span class="math inline">\(\displaystyle C^1\)</span> 函数</p><p><strong>注记</strong></p><ul><li>此证明显示 <span class="math inline">\(\displaystyle (f_n)\)</span>不仅逐点收敛于 <span class="math inline">\(\displaystylef\)</span>，而且在 <span class="math inline">\(\displaystyle I\)</span>的每个有界子区间上一致收敛</li><li>微分定理的关键假设是<strong>导数序列 <spanclass="math inline">\(\displaystyle (f_n&#39;)\)</span>的一致收敛性</strong>，而不是函数序列 <spanclass="math inline">\(\displaystyle (f_n)\)</span>本身的一致收敛性（对于 <span class="math inline">\(\displaystyle(f_n)\)</span>，逐点收敛就足够了，甚至只需 <spanclass="math inline">\(\displaystyle (f_n(a))\)</span> 收敛）</li></ul><h3 id="积分定理">积分定理</h3><p>如果 <span class="math inline">\(\displaystyle I\)</span>是一个有界区间，序列中的所有函数 <spanclass="math inline">\(\displaystyle f_n\)</span> 在 <spanclass="math inline">\(\displaystyle I\)</span> 上（勒贝格）可积，</p><p>并且序列 <span class="math inline">\(\displaystyle (f_n)\)</span> 在<span class="math inline">\(\displaystyle I\)</span> 上一致收敛于函数<span class="math inline">\(\displaystyle f(x) = \lim_{n\to\infty}f_n(x)\)</span>，则极限函数 <span class="math inline">\(\displaystylef\)</span> 也是可积的，并且我们有： <span class="math display">\[\displaystyle \int_I f(x)dx = \lim_{n\to\infty} \int_I f_n(x)dx\\\]</span> <strong>证明：</strong></p><p>存在 <span class="math inline">\(\displaystyle N \in\mathbb{N}\)</span> 使得对所有 <span class="math inline">\(\displaystylen&gt;N\)</span> 和 <span class="math inline">\(\displaystyle x \inI\)</span>，有 <span class="math display">\[\displaystyle |f(x) - f_n(x)| &lt; 1\]</span> 从而 <span class="math inline">\(\displaystyle |f_n(x) -f_{N+1}(x)| &lt; 2\)</span> 对 <span class="math inline">\(\displaystylen&gt;N\)</span> 和 <span class="math inline">\(\displaystyle x \inI\)</span> 成立</p><p>一个可积的界是 <span class="math display">\[\displaystyle \Phi(x) := |f_{N+1}(x)| + 2\]</span> <span class="math inline">\(\displaystyle\Phi(x)\)</span>之所以可积是因为 <spanclass="math inline">\(\displaystyle f_{N+1}\)</span> 可积且区间 <spanclass="math inline">\(\displaystyle I\)</span>有界（常数函数2在有界区间上可积）</p><p>应用勒贝格有界收敛定理即可得出结论</p><p>事实上，对于紧区间 <span class="math inline">\(\displaystyleI=[a,b]\)</span> 上的连续函数序列，有更简单的证明： <spanclass="math display">\[\begin{align}\displaystyle |\int_a^b f(x)dx - \int_a^b f_n(x)dx| &amp;= |\int_a^b(f(x)-f_n(x))dx|\\&amp;\le \int_a^b |f(x)-f_n(x)|dx \\&amp;\le (b-a)\sup_{x \in [a,b]} \{|f(x)-f_n(x)|\}\end{align}\]</span> 由于 <span class="math inline">\(\displaystyle f_n \tof\)</span> 一致收敛，<span class="math inline">\(\displaystyle \sup|f(x)-f_n(x)| \to 0\)</span>，因此 <span class="math display">\[\displaystyle \int_a^b f_n(x)dx \to \int_a^b f(x)dx\]</span> 需要注意的是，区间 <span class="math inline">\(\displaystyleI\)</span> <strong>有界</strong>的假设是本质的。例如， <spanclass="math display">\[\displaystyle f_n(x) =\begin{cases}\frac 1n &amp; \text{if }0 \le x \le n\\0 &amp; \text{otherwise}\end{cases}\]</span> 一致收敛于 <span class="math inline">\(\displaystyle\mathbb{R}\)</span>，但 <span class="math display">\[\displaystyle \int_{\mathbb{R}} f_n(x)dx = 1 \neq 0 = \int_{\mathbb{R}}0 dx\]</span></p><h2id="魏尔斯特拉斯判别法-weierstrasss-criterion-m-test">魏尔斯特拉斯判别法(Weierstrass's Criterion / M-Test)</h2><p>假设 <span class="math inline">\(\displaystyle f_n: D \to\mathbb{R}\)</span> (<span class="math inline">\(\displaystylen=0,1,2,...\)</span>) 是一系列定义在共同定义域 <spanclass="math inline">\(\displaystyle D\)</span> 上的函数</p><p>并且存在常数序列 <span class="math inline">\(\displaystyle M_n \in\mathbb{R}\)</span> 使得对所有 <span class="math inline">\(\displaystylen \in \mathbb{N}\)</span> 和 <span class="math inline">\(\displaystyle x\in D\)</span>，都有 <span class="math inline">\(\displaystyle |f_n(x)|\le M_n\)</span></p><p>如果级数 <span class="math inline">\(\displaystyle \sum_{n=0}^\inftyM_n\)</span> 收敛 (即 <span class="math inline">\(\displaystyle\sum_{n=0}^\infty M_n &lt; \infty\)</span>)，则函数级数 <spanclass="math inline">\(\displaystyle \sum_{n=0}^\infty f_n(x)\)</span> 在<span class="math inline">\(\displaystyle D\)</span> 上一致收敛</p><p><strong>证明：</strong></p><p>对任意固定的 <span class="math inline">\(\displaystyle x \inD\)</span>，由于 <span class="math inline">\(\displaystyle |f_n(x)| \leM_n\)</span> 且 <span class="math inline">\(\displaystyle \sumM_n\)</span> 收敛</p><p>根据比较判别法，级数 <span class="math inline">\(\displaystyle \sumf_n(x)\)</span> 绝对收敛，因此也收敛记其和函数为 <spanclass="math display">\[\displaystyle F(x) = \sum_{n=0}^\infty f_n(x)\]</span> 考虑部分和序列 <span class="math display">\[\displaystyle F_N(x) = \sum_{k=0}^N f_k(x)\]</span> 那么 <span class="math display">\[\displaystyle |F(x) - F_N(x)| = |\sum_{k=N+1}^\infty f_k(x)| \le\sum_{k=N+1}^\infty |f_k(x)| \le \sum_{k=N+1}^\infty M_k\]</span> 因为 <span class="math inline">\(\displaystyle \sumM_n\)</span> 收敛，所以对于任意 <spanclass="math inline">\(\displaystyle \epsilon &gt; 0\)</span>，存在一个<span class="math inline">\(\displaystyle N_0\)</span> 使得当 <spanclass="math inline">\(\displaystyle N &gt; N_0\)</span> 时，余项 <spanclass="math inline">\(\displaystyle \sum_{k=N+1}^\infty M_k &lt;\epsilon\)</span></p><p>因此，对于 <span class="math inline">\(\displaystyle N &gt;N_0\)</span>，我们有 <span class="math display">\[\displaystyle |F(x) - F_N(x)| &lt; \epsilon\]</span> 对所有 <span class="math inline">\(\displaystyle x \inD\)</span> 成立，这说明级数是一致收敛的</p><blockquote><p>示例：证明函数级数 <span class="math inline">\(\displaystyle\sum_{n=1}^\infty \frac{\cos(nx)}{n^2}\)</span> 和 <spanclass="math inline">\(\displaystyle \sum_{n=1}^\infty\frac{\sin(nx)}{n^2}\)</span> 在 <spanclass="math inline">\(\displaystyle \mathbb{R}\)</span> 上一致收敛对于第一个级数，因为 <span class="math display">\[\displaystyle |\frac{\cos(nx)}{n^2}| \le \frac{1}{n^2}\]</span> 对所有 <span class="math inline">\(\displaystyle x \in\mathbb{R}\)</span> 成立，并且级数 <spanclass="math inline">\(\displaystyle \sum_{n=1}^\infty\frac{1}{n^2}\)</span> 收敛</p><p>所以根据魏尔斯特拉斯判别法（取 <spanclass="math inline">\(\displaystyle M_n =1/n^2\)</span>），该级数一致收敛</p><p>第二个级数同理，因此，这两个级数都表示定义在 <spanclass="math inline">\(\displaystyle \mathbb{R}\)</span> 上的连续函数</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>一阶恰当微分方程</title>
    <link href="/2025/06/03/%E4%B8%80%E9%98%B6%E6%81%B0%E5%BD%93%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%20/"/>
    <url>/2025/06/03/%E4%B8%80%E9%98%B6%E6%81%B0%E5%BD%93%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%20/</url>
    
    <content type="html"><![CDATA[<h1 id="一阶恰当微分方程">一阶恰当微分方程</h1><h2 id="一阶恰当微分方程-exact-first-order-equations">一阶恰当微分方程(Exact First-Order Equations）</h2><p>一个形如</p><p><span class="math display">\[\displaystyle M(x,y)dx + N(x,y)dy = 0\\\]</span> 的一阶常微分方程，其中 <spanclass="math inline">\(\displaystyle M, N: D \rightarrow\mathbb{R}\)</span> 定义在 <span class="math inline">\(\displaystyle\mathbb{R}^2\)</span> 的某个开集 <spanclass="math inline">\(\displaystyle D\)</span> 上，如果存在一个函数<span class="math inline">\(\displaystyle f: D \rightarrow\mathbb{R}\)</span> (称为<strong>势函数 potentialfunction</strong>或原函数)，使得其全微分为 <span class="math display">\[\displaystyle df = M(x,y)dx + N(x,y)dy\]</span> 那么这种方程被称为<strong>恰当方程（exactequation)</strong></p><p>全微分条件等价于梯度条件 <span class="math inline">\(\displaystyle\nabla f = (M, N)\)</span>，即： <span class="math display">\[\displaystyle \frac{\partial f}{\partial x} = M(x,y) \quad \land \quad\frac{\partial f}{\partial y} = N(x,y)\\\]</span> 这种微分形式的方程与显式形式 <spanclass="math inline">\(\displaystyle y&#39; = \frac{dy}{dx} =-\frac{M(x,y)}{N(x,y)}\)</span> (假设 <spanclass="math inline">\(\displaystyle N(x,y) \neq 0\)</span>)是密切相关的</p><p>方程 <span class="math inline">\(\displaystyleM(x,y)dx+N(x,y)dy=0\)</span> 的<strong>奇异点（singularpoint）</strong>是指满足 <span class="math inline">\(\displaystyleM(x_0,y_0)=0\)</span> 和 <span class="math inline">\(\displaystyleN(x_0,y_0)=0\)</span> 的点 <span class="math inline">\(\displaystyle(x_0,y_0)\)</span></p><p>如果方程是恰当的，即 <span class="math display">\[\displaystyle (M,N)=\nabla f\]</span> 则其奇异点正是势函数 <span class="math inline">\(\displaystylef\)</span> 的临界点</p><h2 id="恰当方程的判别条件-condition-for-exactness">恰当方程的判别条件(Condition for Exactness)</h2><p>如果函数 <span class="math inline">\(\displaystyle M(x,y)\)</span> 和<span class="math inline">\(\displaystyle N(x,y)\)</span> 在区域 <spanclass="math inline">\(\displaystyle D\)</span> 上具有一阶连续偏导数(<span class="math inline">\(\displaystyle C^1\)</span> 函数)，则方程<span class="math display">\[\displaystyle M(x,y)dx + N(x,y)dy = 0\]</span> 是恰当方程的一个<strong>必要条件</strong>是： <spanclass="math display">\[\displaystyle \frac{\partial M}{\partial y} = \frac{\partial N}{\partialx}\\\]</span> 如果区域 <span class="math inline">\(\displaystyle D\)</span>是一个<strong>单连通区域（simply connecteddomain）</strong>，则上述条件也是<strong>充分条件</strong></p><h2 id="恰当方程的解法">恰当方程的解法</h2><p>如果方程 <span class="math display">\[\displaystyle M(x,y)dx + N(x,y)dy = 0\]</span> 是恰当的，那么其解由势函数 <spanclass="math inline">\(\displaystyle f(x,y)\)</span>的<strong>参数化水平集（parametrized level sets，或称等值线contours）</strong>给出： <span class="math display">\[\displaystyle f(x,y) = C\\\]</span> 其中 <span class="math inline">\(\displaystyle C\)</span>是任意常数</p><h3 id="求解势函数-displaystyle-fxy-的步骤"><strong>求解势函数 <spanclass="math inline">\(\displaystyle f(x,y)\)</span> 的步骤</strong></h3><ol type="1"><li><p>根据 <span class="math inline">\(\displaystyle \frac{\partialf}{\partial x} = M(x,y)\)</span>，对 <spanclass="math inline">\(\displaystyle M(x,y)\)</span> 关于 <spanclass="math inline">\(\displaystyle x\)</span> 积分 (将 <spanclass="math inline">\(\displaystyle y\)</span> 视为常数)：</p><p><span class="math display">\[\displaystyle f(x,y) = \int M(x,y)dx + g(y)\\\]</span> 其中 <span class="math inline">\(\displaystyle g(y)\)</span>是一个只依赖于 <span class="math inline">\(\displaystyle y\)</span>的待定函数 (积分常数)</p></li><li><p>将上述 <span class="math inline">\(\displaystyle f(x,y)\)</span>对 <span class="math inline">\(\displaystyle y\)</span> 求偏导数：</p><p><span class="math display">\[\displaystyle \frac{\partial f}{\partial y} = \frac{\partial}{\partialy} \left( \int M(x,y)dx \right) + g&#39;(y)\\\]</span></p></li><li><p>利用 <span class="math inline">\(\displaystyle \frac{\partialf}{\partial y} = N(x,y)\)</span>，得到：</p><p><span class="math display">\[\displaystyle N(x,y) = \frac{\partial}{\partial y} \left( \int M(x,y)dx\right) + g&#39;(y)\\\]</span></p></li><li><p>从上式解出 <span class="math inline">\(\displaystyleg&#39;(y)\)</span>：</p><p><span class="math display">\[\displaystyle g&#39;(y) = N(x,y) - \frac{\partial}{\partial y} \left(\int M(x,y)dx \right)\\\]</span> 如果方程是恰当的，上式右边将只依赖于 <spanclass="math inline">\(\displaystyle y\)</span></p></li><li><p>对 <span class="math inline">\(\displaystyle g&#39;(y)\)</span>关于 <span class="math inline">\(\displaystyle y\)</span> 积分，得到<span class="math display">\[\displaystyle g(y) = \int g&#39;(y)dy\]</span></p></li><li><p>将 <span class="math inline">\(\displaystyle g(y)\)</span>代回步骤1中的表达式，即可得到势函数 <spanclass="math inline">\(\displaystyle f(x,y)\)</span></p></li></ol><h2 id="积分因子-integrating-factors">积分因子 (IntegratingFactors)</h2><p>如果一个非恰当的微分方程 <span class="math display">\[\displaystyle M(x,y)dx + N(x,y)dy = 0\]</span> 乘以一个函数 <span class="math inline">\(\displaystyle\mu(x,y)\)</span> 后，新的方程 <span class="math display">\[\displaystyle \mu(x,y)M(x,y)dx + \mu(x,y)N(x,y)dy = 0\\\]</span> 变为恰当方程，则称 <span class="math inline">\(\displaystyle\mu(x,y)\)</span> 为原方程的<strong>积分因子（integrating factor 或Euler multiplier）</strong></p><p>积分因子 <span class="math inline">\(\displaystyle \mu(x,y)\)</span>必须在其定义域内不为零</p><h3 id="寻找积分因子-displaystyle-muxy"><strong>寻找积分因子 <spanclass="math inline">\(\displaystyle \mu(x,y)\)</span></strong></h3><p>新方程 <span class="math inline">\(\displaystyle \mu M dx + \mu N dy= 0\)</span> 为恰当方程的条件是：</p><p><span class="math display">\[\displaystyle \frac{\partial (\mu M)}{\partial y} = \frac{\partial (\muN)}{\partial x}\\\]</span> 展开后得到关于 <span class="math inline">\(\displaystyle\mu\)</span> 的偏微分方程：</p><p><span class="math display">\[\displaystyle \mu \left(\frac{\partial M}{\partial y} - \frac{\partialN}{\partial x}\right) = N \frac{\partial \mu}{\partial x} - M\frac{\partial \mu}{\partial y}\\\]</span> 或者写作 <span class="math display">\[\displaystyle M_y \mu + M \mu_y = N_x \mu + N \mu_x\]</span>一般情况下求解这个偏微分方程很困难，但在一些特殊情况下可以找到积分因子</p><h3 id="特殊情况"><strong>特殊情况</strong></h3><ol type="1"><li><p><strong>如果 <span class="math inline">\(\displaystyle\frac{1}{N}\left(\frac{\partial M}{\partial y} - \frac{\partialN}{\partial x}\right) = g(x)\)</span> (只依赖于 <spanclass="math inline">\(\displaystyle x\)</span>)</strong></p><p>则积分因子为</p><p><span class="math display">\[\displaystyle \mu(x) = e^{\int g(x)dx}\\\]</span> 此时偏微分方程简化为 <span class="math display">\[\displaystyle \mu&#39;(x) = g(x)\mu(x)\]</span></p></li><li><p><strong>如果 <span class="math inline">\(\displaystyle\frac{1}{M}\left(\frac{\partial N}{\partial x} - \frac{\partialM}{\partial y}\right) = h(y)\)</span> (只依赖于 <spanclass="math inline">\(\displaystyle y\)</span>)</strong></p><p>则积分因子为</p><p><span class="math display">\[\displaystyle \mu(y) = e^{\int h(y)dy}\\\]</span> 此时 <span class="math display">\[\displaystyle \mu&#39;(y) = -(-h(y))\mu(y) = h(y)\mu(y)\]</span></p></li><li><p><strong>如果 <span class="math inline">\(\displaystyle\frac{1}{Ny - Mx}\left(\frac{\partial M}{\partial y} - \frac{\partialN}{\partial x}\right) = g(xy)\)</span> (只依赖于 <spanclass="math inline">\(\displaystyle xy\)</span>，且 <spanclass="math inline">\(\displaystyle Ny-Mx \neq0\)</span>)</strong>：</p><p>则积分因子为 <span class="math display">\[\displaystyle \mu(s) = e^{\int g(s)ds}\]</span> 其中 <span class="math inline">\(\displaystyles=xy\)</span>，此时 <span class="math display">\[\displaystyle \mu&#39;(s) = g(s)\mu(s)\]</span></p></li><li><p><strong>如果 <span class="math inline">\(\displaystyle\frac{x^2}{Ny + Mx}\left(\frac{\partial M}{\partial y} - \frac{\partialN}{\partial x}\right) = g(y/x)\)</span> (只依赖于 <spanclass="math inline">\(\displaystyle y/x\)</span>，且 <spanclass="math inline">\(\displaystyle Ny+Mx \neq0\)</span>)</strong>：</p><p>则积分因子为 <span class="math display">\[\displaystyle \mu(s) = e^{-\int g(s)ds}\]</span> 其中 <span class="math inline">\(\displaystyles=y/x\)</span>，此时 <span class="math display">\[\displaystyle \mu&#39;(s) = -g(s)\mu(s)\]</span></p></li></ol><h3 id="积分因子的非唯一性"><strong>积分因子的非唯一性</strong></h3><p>积分因子不是唯一的，如果 <span class="math inline">\(\displaystyle\mu\)</span> 是一个积分因子，使得 <span class="math display">\[\displaystyle \mu M dx + \mu N dy = df = 0\]</span> 那么对于任意连续函数 <span class="math inline">\(\displaystyleF\)</span>，<span class="math inline">\(\displaystyle \mu \cdotF(f)\)</span> 也是一个积分因子（只要 <spanclass="math inline">\(\displaystyle F(f) \neq 0\)</span>）</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>可分离的一阶常微分方程</title>
    <link href="/2025/06/03/%E5%8F%AF%E5%88%86%E7%A6%BB%E7%9A%84%E4%B8%80%E9%98%B6%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    <url>/2025/06/03/%E5%8F%AF%E5%88%86%E7%A6%BB%E7%9A%84%E4%B8%80%E9%98%B6%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="可分离的一阶常微分方程">可分离的一阶常微分方程</h1><h2 id="可分离的一阶常微分方程-1">可分离的一阶常微分方程</h2><p>一个形如 <span class="math display">\[\displaystyle y&#39; = f(x,y)\]</span> 的一阶常微分方程，如果其右端函数 <spanclass="math inline">\(\displaystyle f(x,y)\)</span>可以分解为两个分别只依赖于 <span class="math inline">\(\displaystylex\)</span> 和只依赖于 <span class="math inline">\(\displaystyley\)</span> 的函数之积，即 <span class="math display">\[\displaystyle f(x,y) = f_1(x)f_2(y)\]</span> 则称该方程为<strong>可分离方程</strong>（SeparableEquation）</p><p>我们通常假设 <span class="math inline">\(\displaystyle f_1\)</span>和 <span class="math inline">\(\displaystyle f_2\)</span> 的定义域 <spanclass="math inline">\(\displaystyle I\)</span> 和 <spanclass="math inline">\(\displaystyle J\)</span> 分别是开区间</p><p>特别地，如果 <span class="math inline">\(\displaystylef_2(y)\)</span> 在区间 <span class="math inline">\(\displaystyleJ\)</span> 上没有零点，那么函数 <spanclass="math inline">\(\displaystyle N(y) = 1/f_2(y)\)</span> 在 <spanclass="math inline">\(\displaystyle J\)</span>上是良定义的，并且也没有零点令 <span class="math inline">\(\displaystyleM(x) = f_1(x)\)</span>，则原方程 <spanclass="math inline">\(\displaystyle y&#39; = f_1(x)f_2(y)\)</span>可以改写为： <span class="math display">\[\displaystyle \frac{dy}{dx} = \frac{M(x)}{N(y)}\\\]</span> 或者写作微分形式： <span class="math display">\[\displaystyle M(x)dx - N(y)dy = 0\\\]</span></p><h2 id="解法定理">解法定理</h2><p>可分离方程的解法通常由以下定理给出：</p><h3 id="定理">定理</h3><p>假设函数 <span class="math inline">\(\displaystyle M: I \rightarrow\mathbb{R}\)</span> 和 <span class="math inline">\(\displaystyle N: J\rightarrow \mathbb{R}\)</span> 均连续，并且 <spanclass="math inline">\(\displaystyle N(y)\)</span> 在开区间 <spanclass="math inline">\(\displaystyle J\)</span> 上没有零点</p><p>设 <span class="math inline">\(\displaystyle (x_0, y_0) \in I \timesJ\)</span>定义函数 <span class="math inline">\(\displaystyle H_1: I\rightarrow \mathbb{R}\)</span> 和 <spanclass="math inline">\(\displaystyle H_2: J \rightarrow\mathbb{R}\)</span> 如下： <span class="math display">\[\displaystyle H_1(x) = \int_{x_0}^{x} M(\xi) d\xi\\\]</span></p><p><span class="math display">\[\displaystyle H_2(y) = \int_{y_0}^{y} N(\eta) d\eta\\\]</span></p><p>再设 <span class="math inline">\(\displaystyle I&#39; \subseteqI\)</span> 是一个包含 <span class="math inline">\(\displaystylex_0\)</span> 的区间，并且满足 <span class="math inline">\(\displaystyleH_1(I&#39;) \subseteq H_2(J)\)</span></p><p>那么，初值问题： <span class="math display">\[\displaystyle y&#39; = \frac{M(x)}{N(y)} \quad \land \quad y(x_0) =y_0\\\]</span> 存在唯一的解 <span class="math inline">\(\displaystyle y:I&#39; \rightarrow \mathbb{R}\)</span>，该解由下式给出： <spanclass="math display">\[\displaystyle y(x) = H_2^{-1}(H_1(x)) \quad \text{for } x \in I&#39;\\\]</span> 这个解也可以隐式地表示为 <spanclass="math inline">\(\displaystyle H_2(y) = H_1(x)\)</span></p><h3 id="证明">证明</h3><p>由于 <span class="math inline">\(\displaystyle N(y)\)</span> 连续且在<span class="math inline">\(\displaystyle J\)</span> 上无零点，所以<span class="math inline">\(\displaystyle N(y)\)</span> 在 <spanclass="math inline">\(\displaystyle J\)</span> 上恒大于零或恒小于零</p><p>这意味着 <span class="math inline">\(\displaystyle H_2(y)\)</span> 在<span class="math inline">\(\displaystyle J\)</span> 上是严格单调的</p><p>因此，<span class="math inline">\(\displaystyle H_2: J \rightarrowH_2(J)\)</span> 是一个双射函数，其反函数 <spanclass="math inline">\(\displaystyle H_2^{-1}\)</span> 存在且定义在 <spanclass="math inline">\(\displaystyle H_2(J)\)</span> 上</p><p>定义的 <span class="math display">\[\displaystyle y(x) = H_2^{-1}(H_1(x))\]</span> 是良定义的</p><p>通过求导可以验证它满足微分方程： <span class="math display">\[\displaystyle y&#39;(x) = (H_2^{-1})&#39;(H_1(x)) \cdot H_1&#39;(x) =\frac{H_1&#39;(x)}{H_2&#39;(H_2^{-1}(H_1(x)))} = \frac{M(x)}{N(y(x))}\]</span> 同时，由于 <span class="math inline">\(\displaystyle H_1(x_0)= 0\)</span> 且 <span class="math inline">\(\displaystyle H_2(y_0) =0\)</span>，所以 <span class="math display">\[\displaystyle y(x_0) = H_2^{-1}(H_1(x_0)) = H_2^{-1}(0) = y_0\]</span> 满足初始条件</p><p>若 <span class="math inline">\(\displaystyle y(x)\)</span>是该初值问题的任一解，将方程改写为 <span class="math display">\[\displaystyle N(y(x))y&#39;(x) = M(x)\]</span> 然后从 <span class="math inline">\(\displaystyle x_0\)</span>到 <span class="math inline">\(\displaystyle x\)</span> 积分： <spanclass="math display">\[\displaystyle \int_{x_0}^{x} N(y(\xi))y&#39;(\xi)d\xi = \int_{x_0}^{x}M(\xi)d\xi\]</span> 左边进行换元 <span class="math inline">\(\displaystyle \eta =y(\xi)\)</span>，<span class="math inline">\(\displaystyle d\eta =y&#39;(\xi)d\xi\)</span>，得到 <span class="math display">\[\displaystyle \int_{y(x_0)}^{y(x)} N(\eta)d\eta = \int_{y_0}^{y(x)}N(\eta)d\eta = H_2(y(x))\]</span> 右边即为 <span class="math inline">\(\displaystyleH_1(x)\)</span></p><p>因此，任何解都必须满足 <span class="math inline">\(\displaystyleH_2(y(x)) = H_1(x)\)</span></p><h3 id="注记">注记</h3><p>证明过程显示 <span class="math inline">\(\displaystyleH_2(J)\)</span> 是一个包含 <span class="math inline">\(\displaystyle 0 =H_2(y_0)\)</span> 的开区间</p><p>由于 <span class="math inline">\(\displaystyle H_1\)</span> 连续且<span class="math inline">\(\displaystyle H_1(x_0)=0\)</span>，因此存在<span class="math inline">\(\displaystyle \delta &gt; 0\)</span> 使得当<span class="math inline">\(\displaystyle x \in (x_0-\delta,x_0+\delta)\)</span> 时，<span class="math inline">\(\displaystyleH_1(x) \in H_2(J)\)</span></p><p>这意味着初值问题在 <span class="math inline">\(\displaystyle (x_0,y_0)\)</span> 附近局部存在唯一解</p><p>条件 <span class="math display">\[\displaystyle H_1(I&#39;) \subseteq H_2(J)\]</span> 保证了对于每个 <span class="math inline">\(\displaystyle x \inI&#39;\)</span>，方程 <span class="math inline">\(\displaystyle H_2(y) =H_1(x)\)</span> 都有一个解 <span class="math inline">\(\displaystyle y\in J\)</span></p><p>如果幸运的话，我们或许能够从中显式解出 <spanclass="math inline">\(\displaystyle y\)</span> 得到 <spanclass="math inline">\(\displaystyle y(x)\)</span> 的表达式</p><h2 id="关于-displaystyle-y-f_1xf_2y-的一般性讨论">关于 <spanclass="math inline">\(\displaystyle y&#39; = f_1(x)f_2(y)\)</span>的一般性讨论</h2><p>假设 <span class="math inline">\(\displaystyle f_1: I \rightarrow\mathbb{R}\)</span> 和 <span class="math inline">\(\displaystyle f_2: J\rightarrow \mathbb{R}\)</span> 是定义在开区间 <spanclass="math inline">\(\displaystyle I, J \subseteq \mathbb{R}\)</span>上的连续函数</p><h3 id="displaystyle-f_2y-的零点"><spanclass="math inline">\(\displaystyle f_2(y)\)</span> 的零点</h3><p>如果 <span class="math inline">\(\displaystyle y_0\)</span> 是 <spanclass="math inline">\(\displaystyle f_2(y)\)</span> 的一个零点 (即 <spanclass="math inline">\(\displaystyle f_2(y_0)=0\)</span>)</p><p>那么 <span class="math inline">\(\displaystyle y(x) \equivy_0\)</span> 是方程的一个<strong>稳态解</strong> (或平衡解、常数解)</p><p>这些零点会将区间 <span class="math inline">\(\displaystyle J\)</span>分割成若干开子区间，在这些子区间上 <spanclass="math inline">\(\displaystyle f_2(y)\)</span> 没有零点</p><h3 id="局部存在性与唯一性">局部存在性与唯一性</h3><p>在由 <span class="math inline">\(\displaystyle I\)</span> 和 <spanclass="math inline">\(\displaystyle f_2(y)\)</span> 无零点的子区间 <spanclass="math inline">\(\displaystyle J&#39;\)</span> 构成的开矩形区域<span class="math inline">\(\displaystyle I \times J&#39;\)</span>内</p><p>对于任何初始点 <span class="math inline">\(\displaystyle (x_0, y_0)\in I \times J&#39;\)</span>，初值问题 <span class="math display">\[\displaystyle y&#39; = f_1(x)f_2(y) \land y(x_0)=y_0\]</span> 的解是局部存在且唯一的</p><h3 id="隐式解">隐式解</h3><p>在 <span class="math inline">\(\displaystyle I \times J&#39;\)</span>区域内的解 <span class="math inline">\(\displaystyle y(x)\)</span>可以隐式地表示为 <span class="math display">\[\displaystyle F(x,y) = C\]</span> 其中 <span class="math display">\[\displaystyle F(x,y) = \int_{x_0}^{x} M(\xi)d\xi - \int_{y_0}^{y}N(\eta)d\eta\]</span> 此处 <span class="math inline">\(\displaystyle M(x)=f_1(x),N(y)=1/f_2(y)\)</span></p><p>这表明微分形式 <span class="math display">\[\displaystyle M(x)dx - N(y)dy = 0\]</span> 在 <span class="math inline">\(\displaystyle I \timesJ&#39;\)</span> 上是<strong>恰当（exact）</strong>的，这是因为 <spanclass="math display">\[\displaystyle \frac{\partial M}{\partial y} = 0 = \frac{\partialN}{\partial x}\]</span> 且 <span class="math inline">\(\displaystyle I \timesJ&#39;\)</span> 是矩形区域，因此是单连通的</p><h3 id="全局唯一性">全局唯一性</h3><p>在整个定义域 <span class="math inline">\(\displaystyle I \timesJ\)</span> 上的解并不能保证具有唯一性</p><p>除非对常微分方程有额外的假设</p><p>例如，<span class="math inline">\(\displaystyle y&#39;=y^2\)</span>的初值问题解是全局唯一的（在其定义域内），而 <spanclass="math inline">\(\displaystyle y&#39;=\sqrt{|y|}\)</span>则不是</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>线性代数视角下的微分方程</title>
    <link href="/2025/06/02/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    <url>/2025/06/02/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="线性代数视角下的微分方程">线性代数视角下的微分方程</h1><p>在微分方程的研究中，视函数为向量空间中的元素，可以更深刻地理解线性微分方程解的结构</p><h2 id="函数空间-displaystylemathbbri">函数空间 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span></h2><p>给定一个定义域 <span class="math inline">\(\displaystyleI\)</span>（通常是一个区间），所有定义在 <spanclass="math inline">\(\displaystyle I\)</span> 上的实值函数 <spanclass="math inline">\(\displaystyle f: I \rightarrow \mathbb{R}\)</span>的集合，记作 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span></p><p>对于任意函数 <span class="math inline">\(\displaystyle f, g \in\mathbb{R}^{I}\)</span> 和任意实数 <spanclass="math inline">\(\displaystyle c \in\mathbb{R}\)</span>，我们定义“逐点”（point-wise）的加法和标量乘法：</p><ul><li><p><strong>函数加法</strong>： <spanclass="math inline">\(\displaystyle (f+g)(t) = f(t) +g(t)\)</span></p></li><li><p><strong>标量乘法</strong>： <spanclass="math inline">\(\displaystyle (cf)(t) = c \cdotf(t)\)</span></p></li></ul><p>在这些运算下，<spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>构成一个实数域 <spanclass="math inline">\(\displaystyle\mathbb{R}\)</span>上的<strong>向量空间（vectorspace）</strong>，该向量空间中的零向量是零函数，即对所有 <spanclass="math inline">\(\displaystyle t \in I\)</span>，满足 <spanclass="math inline">\(\displaystyle t \mapsto 0\)</span> 的函数</p><h3 id="子空间">子空间</h3><p><span class="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>的一个子集 <span class="math inline">\(\displaystyle S \subseteq\mathbb{R}^{I}\)</span>如果满足以下条件，则称其为一个<strong>子空间（subspace）</strong>：</p><ul><li><p><span class="math inline">\(\displaystyle S\)</span> 非空 (<spanclass="math inline">\(\displaystyle S \neq \emptyset\)</span>)</p></li><li><p><span class="math inline">\(\displaystyle S\)</span>对向量加法封闭：若 <span class="math inline">\(\displaystyle f, g \inS\)</span>，则 <span class="math inline">\(\displaystyle f+g \inS\)</span></p></li><li><p><span class="math inline">\(\displaystyle S\)</span>对标量乘法封闭：若 <span class="math inline">\(\displaystyle f \inS\)</span> 且 <span class="math inline">\(\displaystyle c \in\mathbb{R}\)</span>，则 <span class="math inline">\(\displaystyle cf \inS\)</span> for all <span class="math inline">\(\displaystyle c \in\mathbb{R}\)</span></p></li></ul><p>函数空间 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span> （其中 <spanclass="math inline">\(\displaystyle I\)</span>是一个包含正长度区间的无限集）与我们熟悉的 <spanclass="math inline">\(\displaystyle\mathbb{R}^{n}\)</span>的一个主要区别在于 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>是<strong>无限维的</strong></p><p>例如，对于 <span class="math inline">\(\displaystyle s \inI\)</span>，定义函数 <span class="math inline">\(\displaystyle\delta_s(t)\)</span> 如下： <span class="math display">\[\displaystyle\delta_s(t) = \begin{cases} 1 &amp; \text{if } t=s \\ 0&amp; \text{if } t \neq s \end{cases}\\\]</span> 集合 <span class="math inline">\(\displaystyle \{\delta_s : s\in I\}\)</span> 是线性无关的，但它并不能张成整个 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span></p><p>在微分方程中，我们更常关注 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>的某些特定子空间，例如 <span class="math inline">\(\displaystyleC^{\infty}(I)\)</span>，它由所有在 <spanclass="math inline">\(\displaystyle I\)</span>上具有任意阶导数的函数构成</p><h3 id="线性无关性基与维度">线性无关性、基与维度</h3><p>这些概念与在 <spanclass="math inline">\(\displaystyle\mathbb{R}^{n}\)</span>中的定义类似</p><ul><li><p><strong>线性无关</strong>： 函数集 <spanclass="math inline">\(\displaystyle \{f_1, f_2, \dots, f_k\} \subseteq\mathbb{R}^{I}\)</span> 是线性无关的，如果方程 <spanclass="math inline">\(\displaystyle c_1 f_1 + c_2 f_2 + \dots + c_k f_k= \mathbf{0}\)</span> (其中 <spanclass="math inline">\(\displaystyle\mathbf{0}\)</span> 是零函数) 仅当<span class="math inline">\(\displaystyle c_1 = c_2 = \dots = c_k =0\)</span> 时成立</p></li><li><p><strong>生成集（张成集）</strong>：一个函数集可以张成一个子空间</p></li><li><p><strong>基</strong>： 一个子空间的线性无关的生成集</p></li><li><p><strong>维度</strong>： 一个子空间的基中元素的个数</p></li></ul><p>一个重要的例子是函数族 <span class="math display">\[\displaystyle \{f_{\lambda}(t) = e^{\lambda t} : \lambda \in\mathbb{R}\}\]</span> 这个集合在 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>中是线性无关的，这意味着即使是 <span class="math inline">\(\displaystyleC^{\infty}(I)\)</span> 这样的子空间也是无限维的</p><p>尽管 <span class="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>及其许多子空间是无限维的，但线性常微分方程的解集通常会形成<strong>有限维</strong>的子空间，这使得线性代数的许多结论能够得以应用</p><h2 id="线性微分方程解集的代数结构">线性微分方程解集的代数结构</h2><h3 id="齐次线性一阶微分方程">齐次线性一阶微分方程</h3><p>考虑齐次线性一阶微分方程 <span class="math display">\[\displaystyle y&#39; = a(t)y\]</span> 其中 <span class="math inline">\(\displaystyle a(t)\)</span>在区间 <span class="math inline">\(\displaystyle I\)</span>上连续</p><p>那么，该方程的解集 <span class="math inline">\(\displaystyleS\)</span> 构成 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>的一个<strong>一维子空间</strong></p><p>并且对于任意选定的 <span class="math inline">\(\displaystyle t_0 \inI\)</span>，函数 <span class="math inline">\(\displaystyle y_h(t) =\exp\left(\int_{t_0}^{t} a(s)ds\right)\)</span> 是这个子空间的一个基</p><p>这意味着所有解都可以表示为 <span class="math inline">\(\displaystylec \cdot y_h(t)\)</span> 的形式，其中 <spanclass="math inline">\(\displaystyle c\)</span> 是一个常数</p><p>解集 <span class="math inline">\(\displaystyle S\)</span>是一维的这一事实可能有些令人惊讶，因为在 <spanclass="math inline">\(\displaystyle\mathbb{R}^n\)</span>中，一个（非平凡的）线性方程的解空间维度是 <spanclass="math inline">\(\displaystyle n-1\)</span></p><h3 id="非齐次线性一阶微分方程">非齐次线性一阶微分方程</h3><p>考虑非齐次线性一阶微分方程 <span class="math display">\[\displaystyle y&#39; = a(t)y + b(t)\]</span> 其中 <span class="math inline">\(\displaystyle a(t)\)</span>和 <span class="math inline">\(\displaystyle b(t)\)</span> 在区间 <spanclass="math inline">\(\displaystyle I\)</span> 上连续</p><p>那么，该方程的解集 <span class="math inline">\(\displaystyleS\)</span> 构成 <spanclass="math inline">\(\displaystyle\mathbb{R}^{I}\)</span>中的一条<strong>线</strong>（即一维仿射子空间 affine subspace）</p><p>这条线可以表示为 <span class="math inline">\(\displaystyle y_p(t) +\text{span}\{y_h(t)\}\)</span>，其中 <spanclass="math inline">\(\displaystyle y_p(t)\)</span>是非齐次方程的一个特解，而 <span class="math inline">\(\displaystyley_h(t)\)</span> 是对应的齐次方程 <spanclass="math inline">\(\displaystyle y&#39; = a(t)y\)</span> 的一个非零解具体而言，我们可以取 <span class="math display">\[\displaystyle A(t) = \int_{t_0}^{t} a(s)ds\]</span> 则方向向量为 <span class="math display">\[\displaystyle y_h(t) = e^{A(t)}\]</span> “起点”可以取为特解 <span class="math display">\[\displaystyle y_p(t) = e^{A(t)}\int_{t_0}^{t} b(s)e^{-A(s)}ds\]</span> 解集 <span class="math inline">\(\displaystyle S\)</span>也可以看作是对应的齐次方程解空间 <spanclass="math inline">\(\displaystyle D = \{y_1 - y_2 : y_1, y_2 \inS\}\)</span> 的一个<strong>陪集(translate/coset)</strong></p><blockquote><p>例：<span class="math inline">\(\displaystyle y&#39;&#39; + y =0\)</span></p><p>方程 <span class="math inline">\(\displaystyle y&#39;&#39; + y =0\)</span> 在 <spanclass="math inline">\(\displaystyle\mathbb{R}\)</span> 上的解集 <spanclass="math inline">\(\displaystyle S\)</span> 构成 <spanclass="math inline">\(\displaystyle\mathbb{R}^{\mathbb{R}}\)</span>的一个<strong>二维子空间</strong>，且函数集合 <spanclass="math inline">\(\displaystyle \{\cos t, \sin t\}\)</span>是这个子空间的一个基</p><p><strong>求值映射</strong> (evaluation map) <spanclass="math inline">\(\displaystyle E: S \rightarrow\mathbb{R}^2\)</span> 定义为 <span class="math inline">\(\displaystyleE(y) = (y(0), y&#39;(0))\)</span>，它将一个解映射到其在 <spanclass="math inline">\(\displaystyle t=0\)</span> 处的初始值(函数值和导数值)</p><p>这个映射是一个<strong>线性双射</strong> (linearbijection)，其逆映射为 <span class="math inline">\(\displaystyle (A,B)\mapsto A \cos t + B \sin t\)</span></p></blockquote><h2 id="函数线性无关性的判定">函数线性无关性的判定</h2><p>要判断一组函数 <span class="math inline">\(\displaystyle f_1, f_2,\dots, f_n \in \mathbb{R}^{I}\)</span>是否线性无关，可以使用以下定理：</p><p><strong>定理</strong>： 函数 <spanclass="math inline">\(\displaystyle f_1, \dots, f_n \in\mathbb{R}^{I}\)</span> 线性无关当且仅当存在点 <spanclass="math inline">\(\displaystyle t_1, \dots, t_n \in I\)</span>使得如下行列式非零：</p><p><span class="math display">\[\displaystyle\begin{vmatrix}f_1(t_1) &amp; f_1(t_2) &amp; \cdots &amp; f_1(t_n) \\f_2(t_1) &amp; f_2(t_2) &amp; \cdots &amp; f_2(t_n) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\f_n(t_1) &amp; f_n(t_2) &amp; \cdots &amp; f_n(t_n)\end{vmatrix}\neq 0\\\]</span> 证明思路：</p><ul><li><strong>充分性</strong>： 如果存在这样的 <spanclass="math inline">\(\displaystyle t_1, \dots, t_n\)</span>使得行列式非零假设 <span class="math inline">\(\displaystyle\sum_{i=1}^{n} \lambda_i f_i(t) = 0\)</span> 对所有 <spanclass="math inline">\(\displaystyle t \in I\)</span> 成立，那么它对<span class="math inline">\(\displaystyle t_1, \dots, t_n\)</span>也成立，这导出一个关于 <span class="math inline">\(\displaystyle\lambda_i\)</span>的齐次线性方程组，其系数矩阵就是上述行列式不为零的矩阵。因此，该方程组只有零解<span class="math inline">\(\displaystyle \lambda_1 = \dots = \lambda_n= 0\)</span></li><li><strong>必要性</strong>： 可以用数学归纳法证明。对于 <spanclass="math inline">\(\displaystyle n=1\)</span>，条件简化为存在 <spanclass="math inline">\(\displaystyle t_1\)</span> 使得 <spanclass="math inline">\(\displaystyle f_1(t_1) \neq 0\)</span>，这等价于<span class="math inline">\(\displaystyle f_1\)</span> 不是零函数，即<span class="math inline">\(\displaystyle f_1\)</span> 线性无关归纳步骤中，可以构造一个函数 <span class="math inline">\(\displaystyleg(t)\)</span>，它是将某个选定矩阵的最后一列替换为 <spanclass="math inline">\(\displaystyle (f_1(t), \dots, f_n(t))^T\)</span>后得到的行列式。通过拉普拉斯展开，<spanclass="math inline">\(\displaystyle g(t)\)</span> 可以表示为 <spanclass="math inline">\(\displaystyle f_i(t)\)</span> 的线性组合，其中<span class="math inline">\(\displaystyle f_n(t)\)</span>的系数非零。由于 <span class="math inline">\(\displaystyle f_1, \dots,f_n\)</span> 线性无关，所以 <span class="math inline">\(\displaystyleg(t)\)</span> 不是零函数，因此存在 <spanclass="math inline">\(\displaystyle t_n\)</span> 使得 <spanclass="math inline">\(\displaystyle g(t_n) \neq 0\)</span></li></ul><blockquote><p><strong>示例：判断 <span class="math inline">\(\displaystyle \langle1, \cos t, \sin t \rangle\)</span> 的维度</strong></p><p>我们要证明 <span class="math inline">\(\displaystyle f_1(t)=1,f_2(t)=\cos t, f_3(t)=\sin t\)</span> 是线性无关的</p><p>假设 <span class="math inline">\(\displaystyle \lambda_1 \cdot 1 +\lambda_2 \cos t + \lambda_3 \sin t = 0\)</span> 对所有 <spanclass="math inline">\(\displaystyle t \in \mathbb{R}\)</span> 成立</p><p>取 <span class="math inline">\(\displaystyle t_1=0, t_2=\pi/2,t_3=\pi\)</span>：</p><ul><li><p><span class="math inline">\(\displaystyle t=0 \implies \lambda_1+ \lambda_2 = 0\)</span></p></li><li><p><span class="math inline">\(\displaystyle t=\pi/2 \implies\lambda_1 + \lambda_3 = 0\)</span></p></li><li><p><span class="math inline">\(\displaystyle t=\pi \implies\lambda_1 - \lambda_2 = 0\)</span></p></li></ul><p>这个线性方程组只有唯一解 <span class="math inline">\(\displaystyle\lambda_1 = \lambda_2 = \lambda_3 = 0\)</span>因此，这三个函数线性无关，它们张成的子空间的维度是 3</p><p>或者，我们可以计算行列式：</p><p><span class="math display">\[\displaystyle\begin{vmatrix}f_1(0) &amp; f_1(\pi/2) &amp; f_1(\pi) \\f_2(0) &amp; f_2(\pi/2) &amp; f_2(\pi) \\f_3(0) &amp; f_3(\pi/2) &amp; f_3(\pi)\end{vmatrix}=\begin{vmatrix}1 &amp; 1 &amp; 1 \\\cos 0 &amp; \cos (\pi/2) &amp; \cos \pi \\\sin 0 &amp; \sin (\pi/2) &amp; \sin \pi\end{vmatrix}= 2 \neq 0\\\]</span> 这表明这些函数是线性无关的</p></blockquote>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>实系数与复系数的一阶线性常微分方程</title>
    <link href="/2025/06/02/%E5%AE%9E%E7%B3%BB%E6%95%B0%E4%B8%8E%E5%A4%8D%E7%B3%BB%E6%95%B0%E7%9A%84%E4%B8%80%E9%98%B6%E7%BA%BF%E6%80%A7%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    <url>/2025/06/02/%E5%AE%9E%E7%B3%BB%E6%95%B0%E4%B8%8E%E5%A4%8D%E7%B3%BB%E6%95%B0%E7%9A%84%E4%B8%80%E9%98%B6%E7%BA%BF%E6%80%A7%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1id="实系数与复系数的一阶线性常微分方程">实系数与复系数的一阶线性常微分方程</h1><h2id="一阶线性微分方程first-order-linear-ode">一阶线性微分方程（First-OrderLinear ODE）</h2><p>一阶线性常微分方程是形如：</p><p><span class="math display">\[\displaystyle y^{\prime}=a(t)y+b(t)\\\]</span> 的方程，其中 <span class="math inline">\(\displaystyley^{\prime}=\frac{dy}{dt}，\)</span>并且 <spanclass="math inline">\(\displaystyle a(t)\)</span> 和 <spanclass="math inline">\(\displaystyle b(t)\)</span> 是 <spanclass="math inline">\(\displaystyle t\)</span> 的函数</p><ul><li>如果 <span class="math inline">\(\displaystyle b(t)\equiv0\)</span>（意味着对于所有 <span class="math inline">\(\displaystyle t\)</span>，<span class="math inline">\(\displaystyle b(t)\)</span>均为零），该ODE称为<strong>齐次的（homogeneous）</strong></li><li>如果对于至少一个值， <span class="math inline">\(\displaystyleb(t)\ne0\)</span>，该ODE称为<strong>非齐次的（inhomogeneous）</strong></li></ul><p>这些ODE 的解通常在 <span class="math inline">\(\displaystylea(t)\)</span> 和 <span class="math inline">\(\displaystyle b(t)\)</span>有定义的任何地方都存在，并且具有显式形式（explicit form）</p><p>它们通常形成一个单参数的解族，并且与它们相关的初值问题（IVP）具有唯一的解</p><h2 id="实系数一阶线性ode">实系数一阶线性ODE</h2><h3 id="齐次情况">齐次情况</h3><p>对于齐次一阶线性 ODE：</p><p><span class="math display">\[\displaystyle y^{\prime}=a(t)y\\\]</span> 如果 <span class="math inline">\(\displaystyle a(t)\)</span>连续，其通解由下式给出：</p><p><span class="math display">\[\displaystyle y(t)=c\cdot e^{\int_{t_{0}}^{t}a(s)ds}\\\]</span> 其中 <span class="math inline">\(\displaystyle c\)</span>是一个任意常数（通常由初始条件 <span class="math inline">\(\displaystyley(t_{0})\)</span> 确定）</p><p><span class="math inline">\(\displaystyle y(t)\)</span> 的定义域与<span class="math inline">\(\displaystyle a(t)\)</span> 的定义域相同</p><h3 id="非齐次情况">非齐次情况</h3><p>对于非齐次一阶线性 ODE：</p><p><span class="math display">\[\displaystyle y^{\prime}=a(t)y+b(t)\\\]</span> 解是通过一种称为<strong>参数变易法（variation ofparameters）</strong>的技术找到的</p><p>其思想是采用齐次解 <span class="math inline">\(\displaystyley_{h}(t)=ce^{A(t)}\)</span> 的形式，并允许“常数”成为 <spanclass="math inline">\(\displaystyle t\)</span> 的函数，即 <spanclass="math inline">\(\displaystyle c(t)\)</span></p><p>令 <span class="math inline">\(\displaystyley_{p}(t)=c(t)e^{A(t)}\)</span> 为一个特解，其中 <spanclass="math inline">\(\displaystyleA(t)=\int_{t_{0}}^{t}a(s)ds\)</span></p><p>将其代入 ODE：</p><p><span class="math display">\[\displaystyle y_{p}^{\prime}(t)=c^{\prime}(t)e^{A(t)}+c(t)a(t)e^{A(t)}\\\]</span> 所以，</p><p><span class="math display">\[\displaystylec^{\prime}(t)e^{A(t)}+c(t)a(t)e^{A(t)}=a(t)c(t)e^{A(t)}+b(t)\\\]</span> 简化为 <span class="math inline">\(\displaystylec^{\prime}(t)e^{A(t)}=b(t)\)</span> 或 <spanclass="math inline">\(\displaystylec^{\prime}(t)=b(t)e^{-A(t)}\)</span></p><p>对 <span class="math inline">\(\displaystyle c^{\prime}(t)\)</span>积分得到</p><p><span class="math display">\[\displaystyle c(t)=\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 因此，一个特解是：</p><p><span class="math display">\[\displaystyle y_{p}(t)=e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 非齐次 ODE 的通解 <span class="math inline">\(\displaystyley(t)\)</span> 是齐次ODE的通解 <span class="math inline">\(\displaystyley_{h}(t)=Ce^{A(t)}\)</span> （其中<spanclass="math inline">\(C\)</span>是任意常数）与特解 <spanclass="math inline">\(\displaystyle y_{p}(t)\)</span> 的和：</p><p><span class="math display">\[\displaystyle y(t)=Ce^{A(t)}+e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 如果给定初始条件 <span class="math inline">\(\displaystyley(t_{0})=y_{0}\)</span>，则 <span class="math inline">\(\displaystyleCe^{A(t_{0})}=y_{0}\)</span> （因为根据积分限的选择， <spanclass="math inline">\(\displaystyle A(t_{0})=0\)</span> 且 <spanclass="math inline">\(\displaystyle y_{p}(t_{0})=0\)</span>）</p><p>所以 <span class="math inline">\(\displaystyleC=y_{0}\)</span>，得到：</p><p><span class="math display">\[\displaystyley(t)=y(t_{0})e^{A(t)}+e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 解的最大定义域是 <span class="math inline">\(\displaystylea(t)\)</span> 和 <span class="math inline">\(\displaystyle b(t)\)</span>定义域的交集</p><p>特解 <span class="math inline">\(\displaystyle y_{p}(t)\)</span>的另一种表示有时很有用： <span class="math display">\[\displaystyley_{p}(t)=\int_{t_{0}}^{t}b(s)e^{A(t)-A(s)}ds=\int_{t_{0}}^{t}G(s，t)b(s)ds\\\]</span> 其中<span class="math display">\[\displaystyleG(s，t)=\exp\left(\int_{s}^{t}a(\tau)d\tau\right)\\\]</span></p><h3 id="积分因子integrating-factors">积分因子（IntegratingFactors）</h3><p>解 <span class="math inline">\(\displaystyley^{\prime}=a(t)y+b(t)\)</span> 的另一种方法是使用积分因子</p><p>将ODE改写为</p><p><span class="math display">\[\displaystyle y^{\prime}-a(t)y=b(t)\\\]</span> 两边乘以一个函数 <span class="math inline">\(\displaystylem(t)\)</span>，即积分因子：</p><p><span class="math display">\[\displaystyle m(t)y^{\prime}-m(t)a(t)y=m(t)b(t)\\\]</span> 我们希望左边是乘积的导数，具体来说是 <spanclass="math inline">\(\displaystyle\frac{d}{dt}(m(t)y(t))=m(t)y^{\prime}+m^{\prime}(t)y\)</span></p><p>比较 <span class="math inline">\(\displaystylem(t)y^{\prime}-m(t)a(t)y\)</span> 和 <spanclass="math inline">\(\displaystylem(t)y^{\prime}+m^{\prime}(t)y，\)</span> 我们需要 <spanclass="math inline">\(\displaystyle m^{\prime}(t)=-a(t)m(t)\)</span></p><p>这是一个关于 <span class="math inline">\(\displaystyle m(t)\)</span>的可分离ODE：</p><p><span class="math display">\[\displaystyle \frac{dm}{m}=-a(t)dt\\\]</span> 积分得到</p><p><span class="math display">\[\displaystyle \ln|m(t)|=-\int a(t)dt=-A(t)\\\]</span> 所以 <span class="math inline">\(\displaystylem(t)=e^{-A(t)}\)</span> （我们可以选择积分常数为1）</p><p>有了这个 <span class="math inline">\(\displaystylem(t)\)</span>，方程变为：</p><p><span class="math display">\[\displaystyle \frac{d}{dt}(e^{-A(t)}y(t))=e^{-A(t)}b(t)\\\]</span> 两边积分：</p><p><span class="math display">\[\displaystyle e^{-A(t)}y(t)=\int e^{-A(t)}b(t)dt+C\\\]</span></p><p><span class="math display">\[\displaystyle y(t)=e^{A(t)}\left(\int e^{-A(t)}b(t)dt+C\right)\\\]</span></p><p>这与通过参数变易法得到的通解相同</p><h2 id="复系数一阶线性ode">复系数一阶线性ODE</h2><p>显式复系数一阶线性 ODE具有以下形式：</p><p><span class="math display">\[\displaystyle z^{\prime}(t)=a(t)z(t)+b(t)\\\]</span> <span class="math inline">\(\displaystyle z(t)\)</span>是实变量的复值函数，也就是说 <span class="math inline">\(\displaystylez(t)=x(t)+iy(t)\)</span></p><p><span class="math inline">\(\displaystyle a(t)\)</span>，<spanclass="math inline">\(b(t)\)</span> 是复值函数 <spanclass="math inline">\(\displaystyle D\rightarrow\mathbb{C}，\)</span> 令<span class="math inline">\(\displaystylea(t)=a_{1}(t)+ia_{2}(t)\)</span> 和 <spanclass="math inline">\(\displaystyle b(t)=b_{1}(t)+ib_{2}(t)\)</span></p><p>将这些代入 ODE 得到：</p><p><span class="math display">\[\displaystylex^{\prime}(t)+iy^{\prime}(t)=(a_{1}(t)+ia_{2}(t))(x(t)+iy(t))+(b_{1}(t)+ib_{2}(t))\\\]</span></p><p><span class="math display">\[\displaystylex^{\prime}(t)+iy^{\prime}(t)=(a_{1}x-a_{2}y+b_{1})+i(a_{2}x+a_{1}y+b_{2})\\\]</span></p><p>令实部和虚部分别相等，得到一个由两个实系数一阶线性ODE组成的系统：</p><p><span class="math display">\[\displaystyle x^{\prime}(t)=a_{1}(t)x(t)-a_{2}(t)y(t)+b_{1}(t)\\\]</span></p><p><span class="math display">\[\displaystyle y^{\prime}(t)=a_{2}(t)x(t)+a_{1}(t)y(t)+b_{2}(t)\\\]</span></p><p>矩阵形式为：</p><p><span class="math display">\[\displaystyle \begin{pmatrix}x^{\prime}(t)\\y^{\prime}(t)\end{pmatrix}=\begin{pmatrix}a_{1}(t)&amp;-a_{2}(t)\\a_{2}(t)&amp;a_{1}(t)\end{pmatrix}\begin{pmatrix}x(t)\\y(t)\end{pmatrix}+\begin{pmatrix}b_{1}(t)\\ b_{2}(t)\end{pmatrix}\\\]</span></p><h3 id="通解">通解</h3><p>求解方法与实数情况类似</p><p>通解为：</p><p><span class="math display">\[\displaystyle z(t)=Z_{0}e^{A(t)}+z_{p}(t)\\\]</span> 其中 <span class="math inline">\(\displaystyleZ_{0}\in\mathbb{C}\)</span> 是任意复常数，并且 <spanclass="math inline">\(\displaystyle A(t)=\int_{t_{0}}^{t}a(s)ds\)</span>（这现在是一个复值积分），此外：</p><p><span class="math display">\[\displaystyle z_{p}(t)=e^{A(t)}\int_{t_{0}}^{t}b(s)e^{-A(s)}ds\\\]</span> 证明依赖于实变量复值函数的微分和积分是逐分量进行的，并且 <spanclass="math inline">\(\displaystyle\frac{d}{dt}e^{A(t)}=A^{\prime}(t)e^{A(t)}\)</span> 对于复值 <spanclass="math inline">\(\displaystyle A(t)\)</span> 也成立</p><p>对于 IVP <span class="math inline">\(\displaystylez(t_{0})=z_{0}，\)</span> 唯一解是 <spanclass="math inline">\(\displaystyle z(t)=z_{0}e^{A(t)}+z_{p}(t)\)</span>（由于选择 <span class="math inline">\(\displaystyle t_{0}\)</span>作为积分下限， <span class="math inline">\(\displaystyle A(t_{0})=0，z_{p}(t_{0})=0\)</span>）</p><h3 id="实系数-ode-的复化complexification">实系数 ODE的复化（Complexification）</h3><p>为了解实系数 ODE</p><p><span class="math display">\[\displaystyle y^{\prime}(t)=a(t)y(t)+b(t)\\\]</span> 其中 <span class="math inline">\(\displaystyle a(t)\)</span>是实数，如果 <span class="math inline">\(\displaystyle b(t)\)</span>例如是 <span class="math inline">\(\displaystyle \sin(\omegat)\)</span>，我们可以使用<strong>复化（Complexification）</strong>方法</p><p>令 <span class="math inline">\(\displaystyleb(t)=\text{Im}(B(t))\)</span>，其中 <spanclass="math inline">\(\displaystyle B(t)\)</span>是一个复函数(例如，如果 <span class="math inline">\(\displaystyleb(t)=\sin(\omega t)\)</span> 可以选择 <spanclass="math inline">\(\displaystyle B(t)=e^{i\omega t}\)</span>，因为<span class="math inline">\(\displaystyle \text{Im}(e^{i\omegat})=\sin(\omega t))\)</span></p><p>解复系数 ODE</p><p><span class="math display">\[\displaystyle z^{\prime}(t)=a(t)z(t)+B(t)\\\]</span> 那么 <span class="math inline">\(\displaystyley(t)=\text{Im}(z(t))\)</span> 将是原始实系数 ODE 的一个解</p><p>类似地，如果 <span class="math inline">\(\displaystyleb(t)=\text{Re}(B(t))\)</span>，那么 <spanclass="math inline">\(\displaystyle y(t)=\text{Re}(z(t))\)</span>是解</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>常微分方程、初值问题与方向场</title>
    <link href="/2025/05/16/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E3%80%81%E5%88%9D%E5%80%BC%E9%97%AE%E9%A2%98%E4%B8%8E%E6%96%B9%E5%90%91%E5%9C%BA/"/>
    <url>/2025/05/16/%E5%B8%B8%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B%E3%80%81%E5%88%9D%E5%80%BC%E9%97%AE%E9%A2%98%E4%B8%8E%E6%96%B9%E5%90%91%E5%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="常微分方程初值问题与方向场">常微分方程、初值问题与方向场</h1><h2 id="常微分方程-ordinary-differential-equation-ode">常微分方程(Ordinary Differential Equation, ODE)</h2><p><strong>常微分方程 (Ordinary Differential Equation, ODE)</strong>是一个包含单变量函数 <span class="math inline">\(f(t)\)</span> 及其导数<span class="math inline">\(f&#39;(t)\)</span>、<spanclass="math inline">\(f&#39;&#39;(t)\)</span> 等的方程。这与<strong>偏微分方程 (Partial Differential Equation, PDE)</strong>不同，后者涉及多变量函数及其偏导数</p><p>一个<span class="math inline">\(n\)</span>阶ODE具有以下形式： <spanclass="math display">\[F(t,y,y&#39;,y&#39;&#39;,...,y^{(n)})=0\]</span> 其中 <span class="math inline">\(F\)</span> 的定义域为 <spanclass="math display">\[D\subseteq\mathbb{R}\times\underbrace{\mathbb{R}^{m}\times\cdot\cdot\cdot\times\mathbb{R}^{m}}_{n+1}\]</span> 并且依赖于最后一个变量<spanclass="math inline">\(y^{(n)}\)</span>（否则阶数会小于n）</p><p>该方程的解是一个函数<spanclass="math inline">\(f:I\rightarrow\mathbb{R}^{m}\)</span>，定义在一个长度大于<spanclass="math inline">\(0\)</span>的区间 <spanclass="math inline">\(I\subseteq\mathbb{R}\)</span>上，该函数n次可微，并且对于所有 <span class="math inline">\(t\inI\)</span> 满足 <spanclass="math inline">\(F(t,f(t),f&#39;(t),f&#39;&#39;(t),...,f^{(n)}(t))=0\)</span></p><h2 id="初值问题-initial-value-problem-ivp">初值问题 (Initial ValueProblem, IVP)</h2><p>给定一个如上所述的ODE，并且 <span class="math display">\[t_{0}\in\mathbb{R},y_{0},...,y_{n}\in\mathbb{R}^{m}\]</span> 使得 <span class="math display">\[F(t_{0},y_{0},...,y_{n})=0\]</span> <strong>初值问题 (Initial Value Problem, IVP)</strong> 的解：<span class="math display">\[F(t,y,y&#39;,y&#39;&#39;,...,y^{(n)})=0, y^{(i)}(t_{0})=y_{i} \text{ for} 0\le i\le n\]</span> 是指定义域为<spanclass="math inline">\(I\)</span>的任何函数<spanclass="math inline">\(f:I\rightarrow\mathbb{R}^{m}\)</span>，它解决了上述ODE问题，并满足<span class="math display">\[f(t_{0})=y_{0},f&#39;(t_{0})=y_{1},...,f^{(n)}(t_{0})=y_{n}\]</span></p><h3 id="显式形式explicit-form">显式形式（Explicit Form）</h3><p>虽然以上ODE和IVP的定义是最通用的，但以下<spanclass="math inline">\(n\)</span>阶ODE的显式形式 (explicit form)最为常见： <span class="math display">\[y^{(n)}=G(t,y,y&#39;,...,y^{(n-1)})\]</span> 显式形式的IVP仅需指定 <span class="math display">\[y^{(i)}(t_{0})=y_{i}\text{ for } 0\le i\le n-1\]</span></p><h3 id="最大解maximal-solution">最大解（Maximal Solution）</h3><p>有时很容易找到给定ODE的解（或解族），问题是是否存在其他解</p><p>由于将解 <spanclass="math inline">\(y:I\rightarrow\mathbb{R}\)</span>限制到子区间<span class="math inline">\(J\subset I\)</span>会产生另一个解，因此我们只对<strong>最大解 (maximalsolutions)</strong>感兴趣，即那些不是通过从另一个解进行限制而产生的解</p><h2 id="方向场-direction-fields">方向场 (Direction fields)</h2><p>对于一阶ODE： <span class="math display">\[y&#39;=f(t,y)\]</span> 求解它相当于找到一个定义在某个区间<spanclass="math inline">\(I\subseteq\mathbb{R}\)</span> 上的函数<spanclass="math inline">\(y=y(t)\)</span>，并且对于所有的 <spanclass="math inline">\(t\in I\)</span>，满足以下条件：</p><ol type="1"><li>点<span class="math inline">\((t,y(t))\)</span>在 <spanclass="math inline">\(f\)</span> 的定义域内</li><li>在点 <span class="math inline">\((t,y(t))\)</span> 处， <spanclass="math inline">\(y\)</span> 的斜率等于 <spanclass="math inline">\(f(t,y(t))\)</span></li></ol><p><span class="math inline">\(y\)</span>的斜率我们可以用切线方向代替，而点 <spanclass="math inline">\((t,y(t))\)</span> 处图形的切线方向又可以用向量<span class="math inline">\((1,f(t,y(t)))\)</span> 图形化表示</p><p>因此，我们可以在采样点 <span class="math inline">\((t,y)\inD\)</span> 处绘制一个小向量<spanclass="math inline">\((1,f(t,y))\)</span> (或此向量的正倍数)</p><p>这些向量被称为 <span class="math inline">\(y&#39;=f(t,y)\)</span>的<strong>斜率场 (slope field)</strong> 或<strong>方向场 (directionfield)</strong></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>败家日记</title>
    <link href="/2025/03/17/%E8%B4%A5%E5%AE%B6%E6%97%A5%E8%AE%B0/"/>
    <url>/2025/03/17/%E8%B4%A5%E5%AE%B6%E6%97%A5%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<div id="main" style="width: 100%; height: 400px;"></div><script>  // 读取 total_position.json 数据  fetch('/total_position.json')  // Hexo 会把 `source/total_position.json` 复制到 `public/`    .then(response => response.json())    .then(data => {      // 解析 JSON 数据      let xAxisData = []; // 存放时间      let yAxisData = []; // 存放资产值      data.forEach(entry => {        entry.data.forEach(point => {          let timestamp = point[0];          let assetValue = parseFloat(point[1]); // 转换为数值              xAxisData.push(new Date(timestamp).toLocaleString()); // 转换为可读时间          yAxisData.push(assetValue);        });      });          // 初始化 ECharts      var myChart = echarts.init(document.getElementById('main'));          // ECharts 配置      var option = {        title: { text: '总资产折线图' },        tooltip: { trigger: 'axis' },        xAxis: {          type: 'category',          data: xAxisData,          axisLabel: { rotate: 30 }  // 避免标签重叠        },        yAxis: { type: 'value' },        series: [{          name: '总资产',          type: 'line',          data: yAxisData        }]      };          myChart.setOption(option);    })    .catch(error => console.error('加载 JSON 数据失败:', error));</script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>最大似然学习 Maximum Likelihood Learning</title>
    <link href="/2024/11/22/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%AD%A6%E4%B9%A0/"/>
    <url>/2024/11/22/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="learning的目标">learning的目标</h2><p>假设我们已知数据集<spanclass="math inline">\(\mathcal{D}\)</span>为<spanclass="math inline">\(m\)</span>个从<spanclass="math inline">\(P_{\mathrm{data}}\)</span>中获得的采样构成的集合，同时还已知一些模型<spanclass="math inline">\(P_\theta\)</span>构成的集合<spanclass="math inline">\(\mathcal{M}\)</span></p><p>那么learning的目标就是利用<span class="math inline">\(\mathcalD\)</span>中的元素，在<span class="math inline">\(\mathcalM\)</span>中找到与<spanclass="math inline">\(P_{\mathrm{data}}\)</span>最为相似的<spanclass="math inline">\(P_\theta\)</span></p><h2 id="最相似">”最相似“</h2><p>我们需要一些指标来表征<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>的相似程度</p><p>一旦明确了该指标，求得该指标取极值时的<spanclass="math inline">\(P_\theta\)</span>即可</p><h3 id="相对熵-kullback-leibler-divergence">相对熵 Kullback-Leiblerdivergence</h3><p><span class="math display">\[D(p||q)=\sum_\mathbf x p(\mathbf x)\log\frac{p(\mathbf x)}{q(\mathbf x)}\]</span></p><p>相对熵的含义是，<spanclass="math inline">\(q\)</span>这种方案相对于<spanclass="math inline">\(p\)</span>需要的额外的数据的平均值</p><p>相对熵始终非负，当且仅当<spanclass="math inline">\(p=q\)</span>时相对熵为<spanclass="math inline">\(0\)</span>，因为： <span class="math display">\[D(p||q)=\mathbf E_{\mathbf x\sim p}\left[-\log\frac{q(\mathbfx)}{p(\mathbf x)}\right]\geq-\log\mathbf E_{\mathbf x\sim p}\left[\frac{q(\mathbf x)}{p(\mathbfx)}\right]=-\log\left(\sum_\mathbf x p(\mathbf x)\frac{q(\mathbf x)}{p(\mathbfx)}\right)=0\]</span> 我们可以选择用相对熵来表示<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>的相似程度： <spanclass="math display">\[D(P_{\mathrm{data}}||P_\theta)=\mathbf E_{\mathbf x\simP_{\mathrm{data}}}\left[\log\frac{P_{\mathrm{data}}}{P_\theta(\mathbfx)}\right]=\sum_\mathbf x P_{\mathrm{data}}(\mathbfx)\log\frac{P_{\mathrm{data}}(\mathbf x)}{P_\theta(\mathbf x)}\]</span> 当且仅当<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>完全相同时，相对熵为0</p><h3 id="最大似然-maximum-likelihood">最大似然 Maximum likelihood</h3><p>我们的目标是得到当相对熵取最小值时的<spanclass="math inline">\(P_\theta\)</span>，但是问题是我们并不知道<spanclass="math inline">\(P_{\mathrm{data}}\)</span></p><p>我们可以将相对熵的表达式简单变形： <span class="math display">\[\begin{align}D(P_{\mathrm{data}}||P_\theta)&amp;=\mathbf E_{\mathbf x\simP_{\mathrm{data}}}\left[\log\frac{P_{\mathrm{data}}}{P_\theta(\mathbfx)}\right]\\&amp;=\mathbf E_{\mathbf x\sim P_{\mathrm{data}}}[\logP_{\mathrm{data}}(\mathbf x)]-\mathbf E_{\mathbf x\simP_{\mathrm{data}}}[\log P_{\theta}(\mathbf x)]\end{align}\]</span> 显然<span class="math inline">\(E_{\mathbf x\simP_{\mathrm{data}}}[\logP_{\mathrm{data}}]\)</span>是个常数，那么相对熵的大小只取决于后面一项，即：<span class="math display">\[\arg\min_{P_\theta}D(P_{\mathrm{data}}||P_\theta) = \arg\min_{P_\theta}-\mathbf E_{\mathbf x\sim P_{\mathrm{data}}}[\logP_{\theta}(\mathbf x)]=\arg \max_{P_\theta}\mathbf E_{\mathbf x\simP_{\mathrm{data}}}[\log P_{\theta}(\mathbf x)]\]</span> 此时用empirical log-likelihood作近似： <spanclass="math display">\[\mathbf E_{\mathcal D}[\log P_\theta(\mathbf x)] = \frac1{|\mathcalD|}\sum_{\mathbf x\in \mathcal D}\log P_\theta(\mathbf x)\]</span> 所以，求最大似然时得到的<spanclass="math inline">\(P_\theta\)</span>与求最小相对熵得到的<spanclass="math inline">\(P_\theta\)</span>是相同的，因此我们可以使用似然来表征<spanclass="math inline">\(P_{\mathrm{data}}\)</span>和<spanclass="math inline">\(P_\theta\)</span>的相似程度</p><p>最大似然学习就是指： <span class="math display">\[\max_{P_\theta}\frac1{|\mathcal D|}\sum_{\mathbf x\in \mathcal D}\logP_\theta(\mathbf x)\]</span></p><h2 id="例子自回归模型">例子：自回归模型</h2><p>已知一个有<span class="math inline">\(n\)</span>个变量的自回归模型：<span class="math display">\[P_\theta(\mathbf x) = \prod_{i=1}^np_{\mathrm{neural}}(x_i|\mathbfx_{&lt;i};\mathbf\theta_i)\]</span> 其中<spanclass="math inline">\(\mathbf\theta=(\theta_1,...,\theta_n)\)</span>为所有条件下的参数，训练数据集为<spanclass="math inline">\(\mathcal D=\{\mathbf x^{(1)},...,\mathbfx^{(m)}\}\)</span></p><p>此时，它的似然函数就是： <span class="math display">\[L(\theta,\mathcal D) = \prod_{j=1}^mP_\theta(\mathbf x^{(j)}) =\prod_{j=1}^m\prod_{i=1}^np_{\mathrm{neural}}(x_i^{(j)}|\mathbfx^{(j)}_{&lt;i};\mathbf\theta_i)\]</span> 我们的目的就是求出<span class="math inline">\(\arg \max_\thetaL(\theta, \mathcal D)=\arg \max_\theta \log L(\theta, \mathcalD)\)</span></p><p>为方便计算，一般使用对数似然： <span class="math display">\[\ell(\theta,\mathcal D) = \log L(\theta, \mathcal D)=\sum_{j=1}^m\sum_{i=1}^n\log p_{\mathrm{neural}}(x_i^{(j)}|\mathbfx^{(j)}_{&lt;i};\mathbf\theta_i)\]</span> 然后使用梯度下降等优化方法即可</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>自回归模型 Autoregressive Model</title>
    <link href="/2024/11/13/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/11/13/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="基于分类模型的神经网络">基于分类模型的神经网络</h2><p>对于一个带标签的二元数据集<span class="math inline">\((X,Y)\)</span>，<span class="math inline">\(X\in\{\mathbf{x}\}^n\)</span>，<span class="math inline">\(Y\in\{0,1\}^n\)</span>，分类模型关心的是<spanclass="math inline">\(p(Y|\mathbf{x})\)</span>的分布</p><p>虽然这个概率分布是任意的，但是我们假设该分布是由参数<spanclass="math inline">\(\mathbf{\alpha}\)</span>控制的，即 <spanclass="math display">\[p(Y=1\,|\,\mathbf{x};\mathbf{\alpha})=f(\mathbf{x},\mathbf{\alpha})\]</span></p><h3 id="逻辑回归-logistic-regression">逻辑回归 Logistic Regression</h3><p>对于二元分类，我们可以利用sigmoid函数<spanclass="math inline">\(\sigma(z) =\frac{1}{1+e^{-z}}\)</span>进行回归分析</p><p>由于sigmoid函数是单变量函数，我们先利用参数<spanclass="math inline">\(\mathbf{\alpha}\)</span>将数据的特征向量<spanclass="math inline">\(\mathbf{x}\)</span>映射<spanclass="math inline">\(\mathbb{R}\)</span>中： <spanclass="math display">\[z(\mathbf{\alpha},\mathbf{x})=\alpha_0+\sum_{i=1}^n\alpha_ix_i\]</span> 此时再进行回归分析： <span class="math display">\[p_{\mathrm{logit}}(Y=1\,|\,\mathbf{x};\mathbf\alpha)=\sigma(z(\mathbf\alpha,\mathbfx))\]</span></p><h3 id="非线性相关性">非线性相关性</h3><p>为了使分析更加灵活，我们可以先将输入的特征向量分解成中间向量，再对中间向量进行回归分析</p><p>显然分解输入的特征向量应为非线性的变换，将该变换过程记为<spanclass="math inline">\(f:\mathbb{R}^n \to\mathbb{R}^m, \mathbf x\mapsto\mathbf h(A, \mathbf b, \mathbf x)\)</span></p><p>那么神经网络的基本公式就是 <span class="math display">\[p_{\mathrm{Neural}}(Y=1\,|\,\mathbf x; \mathbf \alpha, A, \mathbf b) =f(\alpha_0+\sum_{i=1}^m\alpha_ih_i)\]</span></p><h2 id="自回归模型-autoregressive-model">自回归模型 AutoregressiveModel</h2><p>为了得到<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布，首先人为选定一个顺序使用链式法则将它展开<span class="math display">\[p(x_1,...,x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1, x_2)...p(x_n|x_1,..,x_{n-1})\]</span> 第一个概率我们直接从条件概率表（CPT）查得</p><p>剩余的条件概率我们假设是一个与参数<spanclass="math inline">\(\alpha\)</span>有关的函数： <spanclass="math display">\[p(x_k|x_1,...,x_{k-1}) = p(x_k|x_1,...,x_{k-1};\mathbf\alpha^k)\]</span> 那么我们就得到了<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布： <spanclass="math display">\[p(x_1,...,x_n)=p_{\mathrm{CPT}}(x_1;\alpha^1)p(x_2|x_1;\mathbf\alpha^2)p(x_3|x_1,x_2;\mathbf\alpha^3)...p(x_n|x_1,..,x_{n-1};\mathbf\alpha^n)\]</span>我们对于剩余的条件概率进行的模型假设，被称为<strong>自回归模型（AutoregressiveModel）</strong></p><h3id="完全可见的sigmoid置信网络-fully-visible-sigmoid-belief-network-fvsbn">完全可见的sigmoid置信网络Fully Visible Sigmoid Belief Network (FVSBN)</h3><p>在自回归模型中，当每个随机变量都是二元变量时，我们使用sigmoid函数进行逻辑回归来得到这个与参数<spanclass="math inline">\(\alpha\)</span>有关的函数，这种模型就被称为<strong>完全可见的sigmoid置信网络（FVSBN）</strong>，即：<span class="math display">\[p(x_k|x_1,...,x_{k-1}) =p_{\mathrm{logit}}(x_k|x_1,...,x_{k-1};\mathbf\alpha^k) =\sigma(\alpha^k_0+\sum_{i=1}^k\alpha_i^kx_i)\]</span> 那么我们就得到了<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布： <spanclass="math display">\[p(x_1,...,x_n)=p_{\mathrm{CPT}}(x_1;\alpha^1)p_{\mathrm{logit}}(x_2|x_1;\mathbf\alpha^2)p_{\mathrm{logit}}(x_3|x_1,x_2;\mathbf\alpha^3)...p_{\mathrm{logit}}(x_n|x_1,..,x_{n-1};\mathbf\alpha^n)\]</span></p><h3id="神经自回归密度估计-neural-autoregressive-density-estimation-nade">神经自回归密度估计Neural Autoregressive Density Estimation (NADE)</h3><p>为了提升模型的效果，我们使用单层的神经网络来替代FVSBN中的逻辑回归，这种模型就是<strong>神经自回归密度估计（NADE）</strong>，即：<span class="math display">\[\mathbf h_k=\sigma(A_k\mathbf x_{&lt;k}+\mathbf c_k)\]</span> <span class="math display">\[p(x_k|x_1,...,x_{k-1})=p_{\mathrm{Neural}}(x_k|x_1,...,x_{k-1};A_k,\mathbf c_k,\alpha_k,\mathbf b_k)=\sigma(\alpha_k\mathbf h_k+\mathbf b_k)\]</span></p><p>为了减少参数数量并且加速计算，在计算当前时间步的权重时，可以加入先前的权重</p><p>也就是说，假设最后一个权重<spanclass="math inline">\(A_n\)</span>为<spanclass="math inline">\(W\)</span>，那么<spanclass="math inline">\(A_k\)</span>就变为<spanclass="math inline">\(W_{.,&lt;k}\)</span></p><h3 id="rnade">RNADE</h3><p>对于非二元的离散型随机变量<span class="math inline">\(X_i\in\{1,...,K\}\)</span>，条件概率可以用categoricaldistribution来表示，并用softmax函数替代sigmoid，即： <spanclass="math display">\[\mathbf h_i = \sigma(W_{.,&lt;i}\mathbf x_{&lt;i}+\mathbf c)\]</span> <span class="math display">\[p(x_i|x_1,...,x_{i-1}) =\mathrm{Cat}(p_i^1,...,p_i^K)=\mathrm{Cat}(\mathrm{softmax}(A_i\mathbfh_i+\mathbf b_i))\]</span></p><p>对于连续型随机变量<spanclass="math inline">\(X_i\in\mathbb{R}\)</span>，条件概率用参数化的连续型分布来表示即可，例如我们可以用<spanclass="math inline">\(K\)</span>个混合高斯模型： <spanclass="math display">\[p(x_i|x_1,...,x_{i-1}) =\sum_{j=1}^K\frac1K\mathcalN(x_i;\mu_i^j,\sigma_i^j)\]</span></p><p>其中获取参数<spanclass="math inline">\(\mu_i^j,\sigma_i^j\)</span>与NADE的方法相同：<span class="math display">\[h_i=\sigma(W_{.,&lt;i}\mathbf x_{&lt;i}+\mathbf c)\]</span></p><p><span class="math display">\[(\mu_i^1,...,\mu_i^K,\sigma_i^1,...,\sigma_i^K)=f(\mathbf h_i)\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>统计生成模型 Statistical Generative Model</title>
    <link href="/2024/11/10/%E7%BB%9F%E8%AE%A1%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/11/10/%E7%BB%9F%E8%AE%A1%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h2 id="统计生成模型-statistical-generative-model">统计生成模型Statistical Generative Model</h2><p><strong>统计生成模型（Statistical GenerativeModel）</strong>就是将生成模型视为一个概率分布<spanclass="math inline">\(p(x)\)</span></p><p>相应的，数据就是对<span class="math inline">\(p(x)\)</span>采样的结果，</p><p>先验知识就是参数形式、损失函数、优化算法等等</p><p>如果我们用<span class="math inline">\(X\)</span>表示图像，<spanclass="math inline">\(Y\)</span>表示标签，discriminative model的目标是得到<span class="math inline">\(P(Y|X)\)</span>，而generativemodel则是为了得到<span class="math inline">\(P(X,Y)\)</span></p><h2 id="概率分布的表示形式">概率分布的表示形式</h2><p>我们希望得到<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布，直接的方法是列举所有的情形，然而代价过于昂贵</p><p>例如，假设<span class="math inline">\(x_i\sim Ber(p_i)\)</span>，<span class="math inline">\(1\leq i \leqn\)</span>，那么我们就需要<spanclass="math inline">\((2^n-1)\)</span>个参数来表示<spanclass="math inline">\(p(x_1,...,x_n)\)</span>的分布</p><p>所以我们可以采用其他方法</p><h3 id="链式法则-chain-rule">链式法则 Chain Rule</h3><p><span class="math display">\[p(x_1,...,x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1, x_2)...p(x_n|x_1,..,x_{n-1})\]</span></p><p>但是使用了链式法则不能减少参数数量，我们依然需要<spanclass="math inline">\((2^n-1)\)</span>个参数</p><h3 id="贝叶斯网络-bayesian-network">贝叶斯网络 Bayesian Network</h3><p>我们可以尝试进行一些独立性假设，例如<spanclass="math inline">\(X_i\)</span>与<span class="math inline">\(X_{&lt;i-1}\)</span>均两两独立，那么 <span class="math display">\[\begin{align}p(x_1,...,x_n) &amp;= p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)...p(x_n|x_1,..,x_{n-1})\\&amp;=p(x_1)p(x_2|x_1)p(x_3|x_2)...p(x_n|x_{n-1})\end{align}\]</span> 此时我们仅需(2n-1)个参数</p><p>由此可见，独立性假设可以帮助我们减少参数：对于每个随机变量<spanclass="math inline">\(X_i\)</span>，假设其与集合<spanclass="math inline">\(\mathbf{X_{A_i}}\)</span>中的随机变量有关，那么，<span class="math display">\[\begin{align}p(x_1,...,x_n) &amp;= p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)...p(x_n|x_1,..,x_{n-1})\\&amp;=\prod_i p(x_i|\mathbf{x_{A_i}})\end{align}\]</span> 变量之间复杂的独立性关系可以用一个<strong>有向无环图（DirectedAcyclic Graph, DAG）</strong>表示，</p><p>对于一个有向无环图<spanclass="math inline">\(G=(V,E)\)</span>，如果：</p><ol type="1"><li><p>节点<span class="math inline">\(i\in V\)</span>表示随机变量<spanclass="math inline">\(X_i\)</span></p></li><li><p>每个随机变量只与其父节点的随机变量有关，即<spanclass="math inline">\(p(x_i|\mathbf{x_{A_i}})=p(x_i|\mathbf{x_{Pa(i)}})\)</span></p></li></ol><p>这样的有向无环图<spanclass="math inline">\(G\)</span>就被称为<strong>贝叶斯网络（BayesianNetwork）</strong></p><p>在贝叶斯网络的基础上， <span class="math display">\[p(x_1,...,x_n)=\prod_{i\in V}p(x_i|\mathbf{x_{Pa(i)}})\]</span></p><h3 id="神经网络模型-neural-models">神经网络模型 Neural Models</h3><p>因为足够深的神经网络可以拟合任意的函数，所以我们没有必要研究变量之间的独立性关系，我们可以在链式法则展开后直接利用神经网络进行函数的拟合，即：<span class="math display">\[p(x_1,...,x_n) \approx p(x_1)p(x_2|x_1)p_{\mathrm{Neural}}(x_3|x_1,x_2)...p_{\mathrm{Neural}}(x_n|x_1,..,x_{n-1})\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>IPv4与IPv6环境下Shadowsocks的搭建与使用</title>
    <link href="/2024/09/07/IPv4%E4%B8%8EIPv6%E7%8E%AF%E5%A2%83%E4%B8%8BShadowsocks%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2024/09/07/IPv4%E4%B8%8EIPv6%E7%8E%AF%E5%A2%83%E4%B8%8BShadowsocks%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<blockquote><p>Ubuntu版本 22.04</p></blockquote><h2id="ipv4环境下安装并配置shadowsocks">IPv4环境下安装并配置Shadowsocks</h2><h3 id="客户端的安装与配置">客户端的安装与配置</h3><p>安装Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get update<br>sudo pip install shadowsocks<br></code></pre></td></tr></table></figure><p>创建并编辑配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">touch</span> /etc/shadowsocks.json<br>sudo vim /etc/shadowsocks.json<br></code></pre></td></tr></table></figure><p>配置文件的格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;server&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;服务器的IPv4地址&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;server_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">8388</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_address&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;127.0.0.1&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1080</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;password&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;密码&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;timeout&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">300</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;method&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;aes-256-gcm&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;fast_open&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>启动Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo sslocal -c /etc/shadowsocks.json -d start<br></code></pre></td></tr></table></figure><p>关闭Shadowsocks时，在终端内输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo lsof -i:1080<br></code></pre></td></tr></table></figure><p>kill相应的pid即可</p><p>如果在启动Shadowsocks时出现如下报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">Traceback (most recent call last):<br>  File <span class="hljs-string">&quot;/usr/local/bin/sslocal&quot;</span>, line 5, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    from shadowsocks.local import main<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.10/dist-packages/shadowsocks/local.py&quot;</span>, line 27, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    from shadowsocks import shell, daemon, eventloop, tcprelay, udprelay, asyncdns<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.10/dist-packages/shadowsocks/udprelay.py&quot;</span>, line 71, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    from shadowsocks import encrypt, eventloop, lru_cache, common, shell<br>  File <span class="hljs-string">&quot;/usr/local/lib/python3.10/dist-packages/shadowsocks/lru_cache.py&quot;</span>, line 34, <span class="hljs-keyword">in</span> &lt;module&gt;<br>    class LRUCache(collections.MutableMapping):<br>AttributeError: module <span class="hljs-string">&#x27;collections&#x27;</span> has no attribute <span class="hljs-string">&#x27;MutableMapping&#x27;</span><br></code></pre></td></tr></table></figure><p>打开文件<code>/usr/local/lib/python3.10/dist-packages/shadowsocks/lru_cache.py</code>，将第34行的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LRUCache</span>(collections.MutableMapping):<br></code></pre></td></tr></table></figure><p>修改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LRUCache</span>(collections.abc.MutableMapping):<br></code></pre></td></tr></table></figure><p>即可</p><h3 id="服务器的安装与配置">服务器的安装与配置</h3><p>安装Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get update<br>sudo pip install shadowsocks<br></code></pre></td></tr></table></figure><p>创建并编辑配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">touch</span> /etc/shadowsocks.json<br>sudo vim /etc/shadowsocks.json<br></code></pre></td></tr></table></figure><p>配置文件的格式如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;server&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;0.0.0.0&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;server_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">8388</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_address&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;127.0.0.1&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;local_port&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">1080</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;password&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;密码&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;timeout&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">300</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;method&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;aes-256-gcm&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;fast_open&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>启动Shadowsocks：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo ssserver -c /etc/shadowsocks.json -d start<br></code></pre></td></tr></table></figure><p>关闭Shadowsocks时，在终端内输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo lsof -i:8388<br></code></pre></td></tr></table></figure><p>kill相应的pid即可</p><h2id="在终端中利用privoxy使用shadowsocks">在终端中利用privoxy使用Shadowsocks</h2><p>在客户端的终端使用Shadowsocks时，需要安装privoxy</p><p>安装privoxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt-get update<br>sudo apt-get install privoxy<br></code></pre></td></tr></table></figure><p>编辑配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo vim /etc/privoxy/config<br></code></pre></td></tr></table></figure><p>确保文件中第794行有<code>listen-address  127.0.0.1:8118</code></p><p>在文件中第1483行添加：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">forward-socks5 / 127.0.0.1:1080 .<br></code></pre></td></tr></table></figure><p>注意后面有<code>.</code></p><p>添加环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>在文件的末尾添加如下两行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> http_proxy=<span class="hljs-string">&quot;http://127.0.0.1:8118&quot;</span><br><span class="hljs-built_in">export</span> https_proxy=<span class="hljs-string">&quot;http://127.0.0.1:8118&quot;</span><br></code></pre></td></tr></table></figure><p>重新加载该文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><p>启动privoxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo service privoxy start<br>sudo service privoxy status<br></code></pre></td></tr></table></figure><p>此时可以<code>curl cip.cc</code>检查是否成功，成功后就可以使用Shadowsocks了</p><p>关闭privoxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo service privoxy stop<br></code></pre></td></tr></table></figure><h2 id="ipv6环境中使用">IPv6环境中使用</h2><h3 id="客户端修改为ipv6">客户端修改为IPv6</h3><h4 id="修改环境变量">修改环境变量</h4><p>把<code>~/.bashrc</code>最后两行修改为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">export</span> http_proxy=<span class="hljs-string">&quot;http://[::1]:8118&quot;</span><br><span class="hljs-built_in">export</span> https_proxy=<span class="hljs-string">&quot;http://[::1]:8118&quot;</span><br></code></pre></td></tr></table></figure><p>重新加载<code>~/.bashrc</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><h4 id="修改shadowsocks">修改Shadowsocks</h4><p>修改Shadowsocks配置文件<code>/etc/shadowsocks.json</code>的<code>"server"</code>和<code>"local_address"</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>    <span class="hljs-string">&quot;server&quot;</span>:<span class="hljs-string">&quot;服务器的IPv6地址&quot;</span>,<br>    <span class="hljs-string">&quot;server_port&quot;</span>:8388,<br>    <span class="hljs-string">&quot;local_address&quot;</span>: <span class="hljs-string">&quot;::1&quot;</span>,<br>    <span class="hljs-string">&quot;local_port&quot;</span>:1080,<br>    <span class="hljs-string">&quot;password&quot;</span>:<span class="hljs-string">&quot;密码&quot;</span>,<br>    <span class="hljs-string">&quot;timeout&quot;</span>:300,<br>    <span class="hljs-string">&quot;method&quot;</span>:<span class="hljs-string">&quot;aes-256-gcm&quot;</span>,<br>    <span class="hljs-string">&quot;fast_open&quot;</span>: <span class="hljs-literal">false</span><br>&#125;<br></code></pre></td></tr></table></figure><p>修改后重启Shadowsocks，此时终端中输入<code>sudo lsof -i:1080</code>，可以看到TYPE被改成了ipv6</p><h4 id="修改privoxy">修改privoxy</h4><p>修改<code>/etc/privoxy/config</code>：</p><p>确保795行有<code>listen-address [::1]:8118</code></p><p>将1372行修改为<code>forward-socks5 / [::1]:1080     .</code></p><p>重启privoxy：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo service privoxy restart<br>sudo service privoxy status<br></code></pre></td></tr></table></figure><h3 id="服务器修改为ipv6">服务器修改为IPv6</h3><p>修改Shadowsocks配置文件<code>/etc/shadowsocks.json</code>的<code>"server"</code>和<code>"local_address"</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">&#123;<br>    <span class="hljs-string">&quot;server&quot;</span>:<span class="hljs-string">&quot;::&quot;</span>,<br>    <span class="hljs-string">&quot;server_port&quot;</span>:8388,<br>    <span class="hljs-string">&quot;local_address&quot;</span>: <span class="hljs-string">&quot;::1&quot;</span>,<br>    <span class="hljs-string">&quot;local_port&quot;</span>:1080,<br>    <span class="hljs-string">&quot;password&quot;</span>:<span class="hljs-string">&quot;密码&quot;</span>,<br>    <span class="hljs-string">&quot;timeout&quot;</span>:300,<br>    <span class="hljs-string">&quot;method&quot;</span>:<span class="hljs-string">&quot;aes-256-gcm&quot;</span>,<br>    <span class="hljs-string">&quot;fast_open&quot;</span>: <span class="hljs-literal">false</span><br>&#125;<br></code></pre></td></tr></table></figure><p>修改后重启Shadowsocks，此时终端中输入<code>sudo lsof -i:1080</code>，可以看到TYPE被改成了ipv6</p><h3 id="测试">测试</h3><p>客户端终端输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl 6.ipw.cn<br></code></pre></td></tr></table></figure><p>若返回IPv6地址则成功</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>反向传播 Backpropagation</title>
    <link href="/2024/07/23/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <url>/2024/07/23/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    
    <content type="html"><![CDATA[<h1 id="反向传播-backpropagation">5. 反向传播 Backpropagation</h1><h2 id="反向传播-backpropagation-1">5.1 反向传播 Backpropagation</h2><p>优化算法中，无一例外需要计算梯度，由于直接推导梯度的解析式过于繁杂，数值计算梯度又不够精确，所以我们选择<strong>反向传播（backpropagation）</strong>来计算梯度</p><p>将复杂的表达式表示为计算图（computationalgraph），不妨设其中一个节点为<spanclass="math inline">\(z=f(x)\)</span></p><p>Upstream gradient: <spanclass="math inline">\(\displaystyle\frac{\partial L}{\partialz}\)</span></p><p>Local gradient: <span class="math inline">\(\displaystyle\frac{\partial z}{\partial x}\)</span></p><p>那么由链式法则，downstream gradient 就是 <spanclass="math display">\[\frac{\partial L}{\partial x} = \frac{\partial z}{\partialx}\frac{\partial L}{\partial z}\]</span> 如此反复便能得到每一个参数的梯度</p><h2 id="向量求导-vector-derivatives">5.2 向量求导 VectorDerivatives</h2><p><span class="math display">\[x \in \mathbb R,y\in \mathbb R, \frac{\partial y}{\partial x}\in \mathbbR\]</span></p><p><span class="math display">\[x \in \mathbb R^N,y\in \mathbb R, \frac{\partial y}{\partial x}\in\mathbb R^N,\left(\frac{\partial y}{\partial x}\right)_n = \frac{\partialy}{\partial x_n}\]</span></p><p><span class="math display">\[x \in \mathbb R^N,y\in \mathbb R^M, \frac{\partial y}{\partial x}\in\mathbb R^{N\times M},\left(\frac{\partial y}{\partial x}\right)_{n,m} = \frac{\partialy_m}{\partial x_n}\]</span></p><h2 id="矩阵相乘的反向传播">5.3 矩阵相乘的反向传播</h2><p>在computational graph中，对于矩阵相乘的节点： <spanclass="math display">\[y=xw,x \in \mathbb R^{N \times D}, y \in \mathbb R^{N\times M}, w \in\mathbb R^{M\times D}\]</span>若直接进行反向传播计算，Jacobians过于庞大且稀疏，产生一些不必要的内存开销</p><p>事实上，如下公式会简化计算： <span class="math display">\[\frac{\partial L}{\partial x} = \left(\frac{\partial L}{\partialy}\right)w^{\mathsf T}\]</span> 以下是该公式的一些简单应用：</p><h3 id="线性分类器">线性分类器</h3><p><span class="math display">\[z=Wx+b\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Backpropagation of linear classifier</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: Upstream derivative, of shape (N, M)</span><br><span class="hljs-string">    - cache: Tuple of:</span><br><span class="hljs-string">      - x: Input data, of shape (N, D)</span><br><span class="hljs-string">      - w: Weights, of shape (D, M)</span><br><span class="hljs-string">      - b: Biases, of shape (M,)</span><br><span class="hljs-string">    Returns a tuple of:</span><br><span class="hljs-string">    - dx: Gradient with respect to x, of shape (N, D)</span><br><span class="hljs-string">    - dw: Gradient with respect to w, of shape (D, M)</span><br><span class="hljs-string">    - db: Gradient with respect to b, of shape (M,)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x, w, b = cache<br>    <br>    dx = dout.mm(w.t())<br>    dw = x.t().mm(dout)<br>    db = dout.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-keyword">return</span> dx, dw, db<br></code></pre></td></tr></table></figure><h3 id="relu">ReLU</h3><p><span class="math display">\[\mathrm{ReLU}(z) = \max (0,z)\]</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Backpropagation of ReLU function</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ReLU</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">    - dout: Upstream derivatives, of any shape</span><br><span class="hljs-string">    - cache: Input x, of same shape as dout</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: Gradient with respect to x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x = cache    <br>    dx = dout * (x &gt; <span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> dx<br></code></pre></td></tr></table></figure><h3 id="dropout">Dropout</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Backpropagation of dropout</span><br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">dropout</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">dout, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Inputs:</span><br><span class="hljs-string">    - dout: Upstream derivatives, of any shape</span><br><span class="hljs-string">    - cache: (dropout_param, mask) from Dropout.forward.</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - dx: Gradient with respect to x</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    mask = cache  <br>    dx = dout * mask<br>    <span class="hljs-keyword">return</span> dx<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>神经网络 Neural Networks</title>
    <link href="/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/07/22/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络-neural-networks">4. 神经网络 Neural Networks</h1><h2 id="神经网络-neural-network">4.1 神经网络 Neural Network</h2><p>由于两个线性分类器的直接叠加等效于一个线性分类器（忽略bias）： <spanclass="math display">\[f=W_2W_1x=W^*x\]</span>我们在两个线性分类器中间添加一个非线性函数，就构成了一个两层的<strong>神经网络（NeuralNetwork）</strong>： <span class="math display">\[f=W_2\max(0, W_1x)\]</span></p><h2 id="激活函数-activation-function">4.2 激活函数 Activationfunction</h2><p>神经网络中的非线性函数被称为<strong>激活函数（Activationfunction）</strong></p><p>以下是一些常见的激活函数</p><p>ReLU(Rectified Linear Unit): <span class="math display">\[\mathrm{ReLU}(z) = \max (0,z)\]</span> Leaky ReLU: <span class="math display">\[\max(0.1x,x)\]</span></p><p>Sigmoid: <span class="math display">\[\sigma(x) = \frac{1}{1+e^{-x}}\]</span> tanh: <span class="math display">\[\tanh(x)\]</span> Maxout: <span class="math display">\[max(w_1^{\mathsf T}x+b_1, w_2^{\mathsf T}x+b_2)\]</span> ELU: <span class="math display">\[\begin{cases}x&amp; x\geq0\\\alpha (e^x-1)&amp; x&lt;0\end{cases}\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>优化理论 Optimization</title>
    <link href="/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    <url>/2024/07/20/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="优化理论-optimization">3. 优化理论 Optimization</h1><h2 id="优化理论-optimization-1">3.1 优化理论 Optimization</h2><p>一个线性分类器的loss可以表示为 <span class="math display">\[L(W) = \frac{1}{N}\sum_{i=1}^N L_i(x_i, y_i,W) + \lambda R(W)\]</span> <strong>优化理论（optimization）</strong>就是求使<spanclass="math inline">\(L\)</span>最小时<spanclass="math inline">\(W\)</span>的值，即求 <span class="math display">\[w^*=\arg \min_w L(w)\]</span></p><h2 id="梯度下降法-gradient-descent">3.2 梯度下降法 GradientDescent</h2><p>通过迭代的方式，沿函数梯度的负方向下降以寻找函数最小值的方法为<strong>梯度下降法（gradientdescent）</strong>： <span class="math display">\[x_{t+1} = x_t - \alpha \nabla f(x_t)\]</span> 代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Vanilla gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(loss_fn, data, w)<br>    w -= lenrning_rate * dw<br></code></pre></td></tr></table></figure><p>其中hyperparameters有<code>initailize_weights()</code>、<code>num_steps</code>和<code>learning_rate</code></p><p>在计算梯度时，通常使用<spanclass="math inline">\(\nabla_WL\)</span>的解析式计算，并用数值计算的方式检验</p><h2 id="随机梯度下降法-stochastic-gradient-descent-sgd">3.3随机梯度下降法 Stochastic Gradient Descent (SGD)</h2><p>loss的梯度计算表达式为 <span class="math display">\[\nabla_WL(W) = \frac{1}{N}\sum_{i=1}^N \nabla _W L_i(x_i, y_i,W) +\lambda \nabla_WR(W)\]</span> 当<spanclass="math inline">\(N\)</span>的数值很大时，计算梯度的开销会很大</p><p>为了避免巨大的开销，我们可以从概率的角度考虑loss function</p><p>对于一个数据集： <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> 式中<span class="math inline">\(x_i\)</span>是图像，<spanclass="math inline">\(y_i\)</span>是该图像对应的正确标签，我们将<spanclass="math inline">\(L\)</span>视作关于<spanclass="math inline">\(x\)</span>、<spanclass="math inline">\(y\)</span>的joint probability distribution</p><p>那么loss就可以看做该分布的期望，即 <span class="math display">\[L(W) = \mathbb E(L) +\lambda R(W)\]</span> <span class="math display">\[\nabla _W L(W) = \nabla_W\mathbb E(L) + \lambda \nabla_WR(W)\]</span></p><p>为了方便计算<span class="math inline">\(\mathbbE(L)\)</span>，可以采用蒙特卡洛方法进行采样估计： <spanclass="math display">\[L(W) \approx \frac1n \sum_{i=1}^n L_i(x_i, y_i, W) + \lambda R(W)\]</span> <span class="math display">\[\nabla_WL(W) \approx \frac1n \sum _{i=1}^n \nabla_WL_i(x_i, y_i, W) +\lambda \nabla _WR(W)\]</span></p><p>所以我们会从整个数据集中采样出minibatch来估计梯度，称为<strong>随机梯度下降法（stochasticgradient descent）</strong>，minibatch的大小通常为32、64或128</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent</span><br>w = initialize_weights()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    minibatch = sample_data(data, batch_size)<br>    dw = compute_gradient(loss_fn, minibatch, w)<br>    w -= learning_rate * dw<br></code></pre></td></tr></table></figure><p>其中hyperparameters有<code>initialize_weights()</code>，<code>num_steps</code>、<code>learning_rate</code>、<code>batch_size</code>和<code>sample_data()</code></p><h2 id="sgd-momentum-sgdm">3.4 SGD + Momentum (SGDM)</h2><p>当loss function有局部最小值（local minimum）或鞍点（saddlepoint）时，SGD方法会立即停止，我们引入”速度向量“来模拟一个小球沿斜坡下滚的过程：<span class="math display">\[v_{t+1} = \rho v_t + \nabla f(x_t)\]</span> <span class="math display">\[x_{t+1} = x_t - \alpha v_{t+1}\]</span></p><p>速度向量的实质就是梯度的累计，函数将沿累计的梯度方向下降</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Stochastic gradient descent with momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    v = rho * v + dw<br>    w -= learning_rate * v<br></code></pre></td></tr></table></figure><p>同时由于速度向量的引入，我们引入了一个新的hyperparameter<code>rho</code>模拟摩擦系数以保证小球最终会停止，通常<code>rho</code>的值为0.9或0.99</p><h2 id="nesterov-momentum">3.5 Nesterov Momentum</h2><p>在累计梯度时，我们选择不累计当前的梯度，而是假设小球以当前的速度运动一小段距离后，累计运动后小球处的梯度，这种方法被称为<strong>Nesterovmomentum</strong>： <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(x_t + \rho v_t)\]</span> <span class="math display">\[x_{t+1} = x_t + v_{t+1}\]</span></p><p>通常为了计算方便，我们令<span class="math inline">\(\tilde x = x_t +\rho v_t\)</span>，于是有： <span class="math display">\[v_{t+1} = \rho v_t - \alpha \nabla f(\tilde x_t)\]</span> <span class="math display">\[\tilde x_{t+1} = \tilde x_t + v_{t+1} +\rho ( v_{t+1} - v_t)\]</span></p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Nesterov momentum</span><br>v = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    old_v = v<br>    v = rho * v - learning_rate * dw<br>    w -= rho * old_v - (<span class="hljs-number">1</span> + rho) * v<br></code></pre></td></tr></table></figure><h2 id="adagrad">3.6 AdaGrad</h2><p>在梯度下降法中，由于learningrate是固定的，梯度大小不同的方向下降的速度是相同的</p><p>但是我们希望在梯度较大的方向减缓下降速度以防震荡，梯度较小的方向加快下降速度，于是我们用<strong>AdaGrad</strong>使learningrate自适应大小： <span class="math display">\[S_{t+1} = S_t + \nabla_W^2\]</span> <span class="math display">\[x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># AdaGrad</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared += dw * dw<br>    w -= learning_rate * dw / (grad_sqruared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><h2 id="rmsprop">3.7 RMSProp</h2><p>在AdaGrad中，若迭代的次数过多，可能会出现梯度平方累积过大的情况，导致learningrate过小而不能产生有效迭代</p><p><strong>RMSProp</strong>效仿SGD+Momentum引入摩擦系数<spanclass="math inline">\(\rho\)</span>保证小球最终停止的想法，引入“摩擦“以防止梯度平方的累积过大：<span class="math display">\[S_{t+1} = \rho S_t + (1-\rho)\nabla_W^2\]</span> <span class="math display">\[x_{t+1} = x_t - \alpha \frac{\nabla_W}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># RMSProp</span><br>grad_squared = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    grad_squared = decay_rate * grad_squared + (<span class="hljs-number">1</span> - decay_rate) * dw * dw<br>    w -= learning_rate * dw / (grad_squared.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>decay_rate</code>为新的hyperparameter</p><h2 id="adam">3.8 Adam</h2><p>将RMSProp和momentum的思想结合起来，我们就得到了<strong>Adam</strong>：<span class="math display">\[v_{t+1} = \rho_1 v_t + (1 - \rho_1)\nabla _W\]</span> <span class="math display">\[S_{t+1} = \rho_2 S_t + (1-\rho_2)\nabla_W^2\]</span></p><p><span class="math display">\[x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>式中平方和除法均指矩阵按元素操作，<spanclass="math inline">\(\varepsilon\)</span>是一个极小的数字以增强数值稳定性，防止出现分母为0的情况</p><p>代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>和<code>beta2</code>为新的hyperparameters</p><p>但是在第一次迭代时，由于<spanclass="math inline">\(\rho_2\)</span>通常趋近于1，<spanclass="math inline">\(S_1\)</span>和<spanclass="math inline">\(S_2\)</span>通常会趋近于0，导致第一次迭代的跨度会非常大，容易产生不好的结果，所以我们在Adam中会加入<strong>biascorrection</strong>来修正： <span class="math display">\[v_{t+1} = \frac{\rho_1 v_t + (1 - \rho_1)\nabla_W}{\sqrt{1-\rho_1^{t+1}}}\]</span> <span class="math display">\[S_{t+1} = \frac{\rho_2 S_t +(1-\rho_2)\nabla_W^2}{\sqrt{1-\rho_2^{t+1}}}\]</span></p><p><span class="math display">\[x_{t+1} = x_t - \alpha \frac{v_{t+1}}{\sqrt{S_{t+1}+\varepsilon}}\]</span></p><p>修正后的代码思路如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Adam with bias correction</span><br>moment1 = <span class="hljs-number">0</span><br>moment2 = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_steps):<br>    dw = compute_gradient(w)<br>    moment1 = beta1 * moment1 + (<span class="hljs-number">1</span> - beta1) * dw<br>    moment2 = beta2 * moment2 + (<span class="hljs-number">1</span> - beta2) * dw * dw<br>    moment1_unbias = moment1 / (<span class="hljs-number">1</span> - beta1 ** t)<br>    moment2_unbias = moment2 / (<span class="hljs-number">1</span> - beta2 ** t)<br>    w -= learning_rate * moment1 / (moment2.sqrt() + <span class="hljs-number">1e-7</span>)<br></code></pre></td></tr></table></figure><p><code>beta1</code>通常取0.9，<code>beta2</code>通常取0.999，<code>learning_rate</code>通常取1e-3、5e-4、1e-4</p><h2 id="二阶优化-second-order-optimization">3.9 二阶优化 Second-OrderOptimization</h2><p>以上我们提到的优化方式均为<strong>一阶优化</strong>，它们通过梯度来线性拟合函数并迭代以得到函数最小值</p><p>那么，我们可以考虑通过梯度和黑塞矩阵来二次拟合函数，不妨设二次拟合的迭代式为<span class="math display">\[x_{t+1} = x_t+d\]</span> 我们希望<spanclass="math inline">\(x_{t+1}\)</span>尽可能小，即求 <spanclass="math display">\[d = \arg \min_d f(x_t+d)\]</span> <span class="math inline">\(f(x)\)</span>在<spanclass="math inline">\(x_t\)</span>处的二阶泰勒展开式为 <spanclass="math display">\[f(x) = f(x_t) + (x - x_t)^{\mathsf{T}}\nabla f(x_t) +\frac12(x-x_t)^{\mathsf{T}}\mathbf{H}f(x_t)(x-x_t)\]</span> 代入<span class="math inline">\(x_{t+1}\)</span>的值得 <spanclass="math display">\[f(x_{t+1}) = f(x_t+d)=f(x_t)+d^{\mathsf{T}}\nablaf(x_t)+\frac12d^{\mathsf{T}}\mathbf{H}f(x_t)d\]</span> 当<span class="math inline">\(d=-[\mathbf{H}f(x_t)]^{-1}\nablaf(x_t)\)</span>时取最小值</p><p>则二阶优化的迭代式为： <span class="math display">\[x_{t+1} = x_t-\left[\mathbf{H}f(x_t)\right]^{-1}\nabla f(x_t)\]</span>然而由于黑塞矩阵元素数量过多且矩阵求逆复杂度过高，实践中很少使用二阶优化</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>线性分类器 Linear Classifier</title>
    <link href="/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/"/>
    <url>/2024/07/02/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="线性分类器-linear-classifier">2. 线性分类器 LinearClassifier</h1><h2 id="评分函数-score-function">2.1 评分函数 Score Function</h2><p><strong>评分函数（score function）</strong> 是<spanclass="math inline">\(\mathbb{R}^D \to\mathbb{R}^C\)</span>的线性映射，即一张图片在每个标签上所得到的评分：<span class="math display">\[f(x, W) = Wx+b,x\in \mathbb{R}^D, W\in \mathbb{R}^{C\times D}, b\in \mathbb{R}^D\]</span></p><p>式中<spanclass="math inline">\(x\)</span>是将图像数据拉长（flatten）得到的<spanclass="math inline">\(D\)</span>维列向量，<spanclass="math inline">\(W\)</span>是<strong>参数（parameter）</strong>或称<strong>权重（weight）</strong> ，<spanclass="math inline">\(b\)</span>为<strong>偏差向量（biasvector）</strong></p><p><span class="math inline">\(C\)</span>为待分类的标签个数，<spanclass="math inline">\(f(x,W)\)</span>即为该图像对于每个标签的评分</p><h2 id="损失函数-loss-function">2.2 损失函数 Loss Function</h2><p><strong>损失函数（loss funtion）</strong>量化了线性分类器的效果，其值越高，则线性分类器效果越差，又被称为目标函数（objectivefunction）、代价函数（cost function）</p><p>对于一个数据集： <span class="math display">\[\{(x_i, y_i)\}_{i=1}^N\]</span> 式中<span class="math inline">\(x_i\)</span>是图像，<spanclass="math inline">\(y_i\)</span>是该图像对应的正确标签</p><p>则对于单个的一张图像的损失为： <span class="math display">\[L_i(f(x_i, W), y_i)\]</span> 对于数据集来说，损失是每张图像损失的平均值： <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)\]</span> <span class="math inline">\(L_i\)</span>即为损失函数</p><h2 id="多类支持向量机损失-multiclass-svm-loss">2.3 多类支持向量机损失Multiclass SVM Loss</h2><p>朴素的想法是，正确的标签的评分应当比其他标签的评分要高</p><p>所以，对于给定的一张图像<span class="math inline">\(x_i,y_i\)</span>，其评分<span class="math inline">\(s=f(x_i,W)\)</span>，则SVM损失有如下形式： <span class="math display">\[L_i = \sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)\]</span> 当评分均为很小的随机值时，损失应当接近<spanclass="math inline">\(C-1\)</span>，<spanclass="math inline">\(C\)</span>为待分类的总标签数，此性质可作为debug的依据</p><h2 id="正则化-regularization">2.4 正则化 Regularization</h2><p>使上述的损失<span class="math inline">\(L\)</span>最小的<spanclass="math inline">\(W\)</span>并不唯一</p><p>若<span class="math inline">\(W\)</span>可使损失<spanclass="math inline">\(L\)</span>最小，则<spanclass="math inline">\(\lambda W\)</span>也可使<spanclass="math inline">\(L\)</span>最小</p><p>于是，我们在损失函数的表达式中引入一项<spanclass="math inline">\(\lambda R(W)\)</span> : <spanclass="math display">\[L=\frac1N\sum_iL_i(f(x_i,W),y_i)+\lambda R(W)\]</span> 式中<spanclass="math inline">\(\lambda\)</span>为<strong>正则化强度（regularizationstrength）</strong> ，为超参数</p><p>正则化的好处：</p><ol type="1"><li><p>进一步地筛选<spanclass="math inline">\(W\)</span>，使所选定的<spanclass="math inline">\(W\)</span>拥有除最小化损失以外的其他功能</p></li><li><p>避免<strong>过拟合（overfitting）</strong></p></li><li><p>通过增加曲率以提高<strong>优化（optimization）</strong>效果</p></li></ol><p>常用的正则化有：</p><p>L2正则化 <span class="math display">\[R(W) = \sum_k \sum_l W_{k,l}^2\]</span> L1正则化 <span class="math display">\[R(W)= \sum_k \sum_l \left| W_{k,l}^2 \right |\]</span> 或者将二者联系起来： <span class="math display">\[R(W) = \sum_k \sum_l \beta W_{k,l}^2 + \left |W_{k,l}^2 \right |\]</span> 其他还有dropout、Batchnormalization、Cutout、Mixup、Stochastic depth等等</p><h2 id="交叉熵损失-cross-entropy-loss">2.5 交叉熵损失 Cross-EntropyLoss</h2><p>另一种损失函函数使用<strong>归一化指数函数（softmaxfunction）</strong> 将评分用概率来描述，被称为<strong>交叉熵损失（Cross- Entropy Loss）</strong>或者<strong>多元逻辑回归（Multinomial Logistic Regression）</strong></p><p>对于一个评分函数<spanclass="math inline">\(s=f(x_i,W)\)</span>，其softmax function的形式为：<span class="math display">\[P(Y = k|X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}\]</span> 我们假设真实的概率分布为<spanclass="math inline">\(P\)</span>，训练得到的概率分布为<spanclass="math inline">\(Q\)</span>，我们使用<spanclass="math inline">\(Q\)</span>来拟合<spanclass="math inline">\(P\)</span>，则交叉熵为： <spanclass="math display">\[H(P, Q) = H(P) + D_{KL}(P||Q)\]</span> 式中<spanclass="math inline">\(D_{KL}(P||Q)\)</span>为<strong>相对熵（Kullback-LeiblerDivergence）</strong> ： <span class="math display">\[D_{KL}(P||Q) = \sum_{i=1}^n P(x_i)\log \frac{P(x_i)}{Q(x_i)}\]</span> 由于真实的概率分布不变，即<spanclass="math inline">\(H(P)\)</span>不变，则若交叉熵<spanclass="math inline">\(H(P,Q)\)</span>最小，只需相对熵<spanclass="math inline">\(D_{KL}(P||Q)\)</span>最小即可</p><p>当单张图片的损失具有如下形式时，交叉熵最小： <spanclass="math display">\[L_i = - \log P(Y=y_i|X = x_i)\]</span> 所以交叉熵损失的具体形式为： <span class="math display">\[L = \frac1N \sum_i\left(-\log\left(\frac{e^{s_{y_i}}}{\sum_je^{s_j}}\right)\right)+\lambda R(W)\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>图像分类 Image Classification</title>
    <link href="/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    <url>/2024/06/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="图像分类-image-classification">1. 图像分类 ImageClassification</h1><h2 id="图像分类器">1.1 图像分类器</h2><p>图像分类的算法难以用如下的函数进行硬编码(hard-code)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">classify_image</span>(<span class="hljs-params">image</span>):<br><span class="hljs-comment"># Some magic here?</span><br><span class="hljs-keyword">return</span> class_label<br></code></pre></td></tr></table></figure><p>所以通常采用机器学习，即Data-Driven的方式</p><p>先用包含图像与标签的数据集训练分类器，再评估分类器在分类新图片的表现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">images, labels</span>):<br><span class="hljs-comment"># Machine learning!</span><br><span class="hljs-keyword">return</span> model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">model, test_images</span>):<br><span class="hljs-comment"># Use model to predict labels</span><br><span class="hljs-keyword">return</span> test_labels<br></code></pre></td></tr></table></figure><h2 id="常见数据集">1.2 常见数据集</h2><p>MNIST</p><p>CIFAR10</p><p>CIFAR100</p><p>ImageNet</p><p>MIT Places</p><h2 id="邻近算法-nearest-neighbor">1.3 邻近算法 Nearest Neighbor</h2><ol type="1"><li>记忆数据集中所有的图像和对应的标签</li><li>与数据集中最相似的图像的标签即为新图像的标签</li></ol><p>使用<strong>距离度量</strong>比较图像，以下是常见的两种距离度量：</p><p>L1 距离（曼哈顿距离 Manhattan distance）: <spanclass="math display">\[d_1(I_1, I_2) = \sum_p |I_1^p - I_2^p|\]</span> L2 距离（欧拉距离 Euclidean distance）： <spanclass="math display">\[d_2(I_1, I_2) = \sqrt{\sum_p (I_1^p - I_2^p)^2}\]</span></p><h2 id="k近邻算法-k-nearest-neighbors">1.4 K近邻算法 K-NearestNeighbors</h2><p>若 𝑘 个最相似的图像中的大多数为某一个标签，则该图像也属于这个标签</p><p>当训练的样本足够多时，K近邻算法可以表示任何函数</p><h2 id="超参数-hyperparameter">1.5 超参数 Hyperparameter</h2><p>从数据集中无法通过训练得到的参数为<strong>超参数</strong>，超参数需要在学习之前设定</p><p>例如K邻近算法中的K就是超参数</p><p>超参数的设定方法：</p><p>将数据集分为train、validate和test三部分，选择超参数的值在train上训练，并在validate中验证，只在最后使用test上查看效果</p><p>如果条件允许，可以将数据集分割成许多部分，每次选择不同的部分作为validate，剩下的部分作为train，同样的只在最后在test上检验</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度学习与计算机视觉</title>
    <link href="/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    <url>/2024/06/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h2 id="写在前面">写在前面</h2><p>这是一篇Umich的EECS498-007课程的学习笔记，开始阅读之前您需要学习过这门课程或已经掌握了深度学习与计算机视觉的基本知识。本篇旨在复习课程要点，不适合初学者作为tutorial学习。</p><h2 id="关于umich-eecs-498-007">关于Umich EECS 498-007</h2><p>来自<ahref="https://csdiy.wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/EECS498-007/">csdiy</a>的课程简介</p><blockquote><p>UMich 的 Computer Vision课，课程视频和作业质量极高，涵盖的主题非常全，同时 Assignments的难度由浅及深，覆盖了 CV 主流模型发展的全阶段，是一门非常好的 ComputerVision 入门课。</p></blockquote><p>本篇笔记基于Fall2019的课程，课程链接：https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r</p><p>与此同时，该课程的assignments（含答案）已上传至<ahref="https://github.com/hmnkapa/eecs598">github</a>，答案是我自己做的，仅供参考</p><p>Assignments来自Fall 2020的<ahref="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/">课程主页</a></p><h2 id="目录">目录</h2><ol type="1"><li>图像分类 Image Classification</li><li>线性分类器 Linear Classifier</li><li>优化理论 Optimization</li><li>神经网络 Neural Networks</li><li>反向传播算法 Backpropagation</li><li>卷积网络 Convolutional Networks</li><li>CNN架构 CNN Architectures</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>柯西-施瓦茨（Cauchy-Schwarz）不等式的积分形式</title>
    <link href="/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/"/>
    <url>/2023/11/04/%E6%9F%AF%E8%A5%BF-%E6%96%BD%E7%93%A6%E8%8C%A8%EF%BC%88Cauchy-Schwarz%EF%BC%89%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%9A%84%E7%A7%AF%E5%88%86%E5%BD%A2%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h2 id="柯西-施瓦茨不等式的积分形式">柯西-施瓦茨不等式的积分形式</h2><p>假设 <span class="math inline">\(𝑓(𝑥)\)</span> 和 <spanclass="math inline">\(𝑔(𝑥)\)</span> 在区间 <spanclass="math inline">\([𝑎,𝑏]\)</span> 上黎曼可积，那么</p><p><span class="math display">\[\int_𝑎^𝑏𝑓^2(𝑥)\mathrm{d}𝑥⋅\int_𝑎^𝑏𝑔^2(𝑥)\mathrm{d}𝑥≥(\int_𝑎^𝑏𝑓(𝑥)𝑔(𝑥)\mathrm{d}𝑥)^2\]</span></p><h2 id="前置知识">前置知识</h2><p>柯西不等式 <span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 及其一般形式 <span class="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 极限、微积分基本知识</p><h2id="柯西-施瓦茨不等式的积分形式的证明">柯西-施瓦茨不等式的积分形式的证明</h2><p>事实上，说起柯西，在高中课本（习题）上我们已经学习过柯西不等式的二维形式</p><p><span class="math display">\[(𝑎^2+𝑏^2)(𝑐^2+𝑑^2)≥(𝑎𝑐+𝑏𝑑)^2\]</span> 进一步地，柯西不等式的一般形式如下 <spanclass="math display">\[\displaystyle\sum_{𝑖=1}^𝑛𝑎_𝑖^2\sum_{𝑖=1}^𝑛𝑏_𝑖^2≥(\sum_{𝑖=1}^𝑛𝑎_𝑖𝑏_𝑖)^2\]</span> 既然都叫柯西（</p><p>我们考虑使用柯西不等式的一般形式来证明柯西-施瓦茨不等式的积分形式</p><p>由定积分的定义，柯西-施瓦茨不等式的积分形式等价于 <spanclass="math display">\[\lim_{n\to\infty}\sum_{i=1}^nf^2(x_i)\Delta{x} \cdot\lim_{n\to\infty}\sum_{i=1}^ng^2(x_i)\Delta{x} \geq \left(\lim_{n\to\infty}\sum_{i=1}^nf(x_i)g(x_i)\Delta{x} \right)^2\\\]</span> 此处<spanclass="math inline">\(\Delta{x}=\frac{b-a}{n}\)</span>，<spanclass="math inline">\(x_i=a+i\Delta{x}\)</span></p><p>由极限的运算法则，上式等价于 <span class="math display">\[\lim_{n\to\infty} \left[ \sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i)- \left( \sum_{i=1}^nf(x_i)g(x_i) \right)^2 \right] \Delta^2{x} \geq0\\\]</span> 由极限的保号性并约去<spanclass="math inline">\(\Delta^2{x}\)</span>，有 <spanclass="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) - \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \geq0\\\]</span> 即 <span class="math display">\[\sum_{i=1}^nf^2(x_i) \cdot \sum_{i=1}^ng^2(x_i) \geq \left(\sum_{i=1}^nf(x_i)g(x_i) \right)^2 \\\]</span> 此即柯西不等式的一般形式.</p><p>柯西不等式的一般形式的取等条件为<spanclass="math inline">\(\frac{a_1}{b_1}=\frac{a_2}{b_2}=...=\frac{a_n}{b_n}\)</span>，因此对于柯西-施瓦茨不等式的一般形式，</p><p>当且仅当 <span class="math display">\[f(x)=\lambda g(x)\]</span> 时，等号成立，式中<spanclass="math inline">\(\lambda\)</span>为一实数.</p><h2 id="一道例题">一道例题</h2><p>这个不等式是最近在准备微积分的第一次期中考试时遇到的</p><p>（其实我开始写文章的时候距离考试还有24分钟</p><p>想找一些卷子做一做</p><p>于是发现了一份浙江某211大学号称<ahref="https://www.zhihu.com/question/265940112?utm_medium=social&amp;utm_oi=1650268599276343296&amp;utm_psn=1703111356650299392&amp;utm_source=qq">120年来最难的微积分试卷</a></p><p>其最后一题原题如下：</p><blockquote><ol start="12" type="1"><li>设 <span class="math inline">\(𝑓(𝑥)\)</span> 在 <spanclass="math inline">\([0,1]\)</span> 上连续且可导，当 <spanclass="math inline">\(𝑥∈[0,1]\)</span> 时， <spanclass="math inline">\(\displaystyle\int_𝑥^1𝑓(𝑡)\mathrm{d}𝑡≥\frac{1-x^3}{2}\)</span> ，证明： <spanclass="math display">\[\displaystyle \int_0^1\,[f(x)]^2\mathrm{d}x&gt;\frac{5}{12}\\\]</span></li></ol></blockquote><p>令<span class="math inline">\(\displaystyleF(x)=\int_x^1\,f(t)\mathrm{d}t=-\int_1^x\,f(t)\mathrm{d}t\)</span> ，则<span class="math inline">\(\displaystyle F^\prime(x)=-f(x)\)</span></p><p>由 <span class="math inline">\(\displaystyleF(x)\geq\frac{1-x^3}{2}\)</span> 可得 <span class="math display">\[\int_0^1\,F(x)\mathrm{d}x \geq\int_0^1\frac{1-x^3}{2}\mathrm{d}x=\frac{3}{8}\\\]</span> 下面运用分部积分法来计算 <spanclass="math inline">\(\displaystyle \int_0^1\,F(x)\mathrm{d}x\)</span><span class="math display">\[\begin{align} \int_0^1\,F(x)\mathrm{d}x&amp;=xF(x)\Big]_0^1-\int_0^1x\mathrm{d}F(x)\\&amp;=F(1)+\int_0^1xf(x)\mathrm{d}x\\ &amp;=\int_0^1xf(x)\mathrm{d}x\\\end{align} \\\]</span></p><p>于是，我们有 <span class="math display">\[\int_0^1xf(x)\mathrm{d}x \geq \frac{3}{8} \\\]</span> 由柯西-施瓦茨不等式 <span class="math display">\[\int_0^1\,f^2(x)\mathrm{d}x\cdot\int_0^1\,x^2\mathrm{d}x \geq \left(\int_0^1\,xf(x)\mathrm{d}x \right)^2 \geq \frac{9}{64} \\\]</span> <span class="math inline">\(\displaystyle\int_0^1\,x^2\mathrm{d}x\)</span>是好积的，其结果为 <spanclass="math inline">\(\frac{1}{3}\)</span> ，故 <spanclass="math display">\[\int_0^1\,f^2(x)\mathrm{d}x \geq \frac{27}{64}&gt;\frac{5}{12}\\\]</span></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
